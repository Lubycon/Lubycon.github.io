{"data":{"allFeedEvanBlog":{"edges":[{"node":{"contentSnippet":"올해 2020년은 개인적으로든 전반적으로든 정말 다사다난했던 해였다. 특히 전 세계를 덮친 COVID-19로 인해 정말 많은 부분들이 바뀌었는데, 개인적으로는 사람을 많이 만나지 못 했던 게 조금 힘들었던 것 같다. (살면서 노래방을 이렇게 안 가본 게 처음…)","link":"https://evan-moon.github.io/2020/12/29/2020-retrospective/","title":"중니어 개발자의 2020년 회고","pubDate":"Tue, 29 Dec 2020 03:08:06 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 얼마 전에 필자가 삽질했던 내용인 SVG로 도넛 차트 그려보기에 대해서 이야기해볼까 한다. 사실 도넛 차트를 그리는 것 자체는 SVG가 제공하는  엘리먼트를 사용하면 되기 때문에 간단하게 만들 수 있지만, 필자가 삽질했던 부분은 바로 애니메이션이었다.","link":"https://evan-moon.github.io/2020/12/12/draw-arc-with-svg-clippath/","title":"SVG와 삼각 함수로 도넛 차트 만들어보기","pubDate":"Sat, 12 Dec 2020 06:29:45 GMT"}},{"node":{"contentSnippet":"최근 필자가 활동하고 있는 루비콘 팀에서는 멘토링 프로젝트를 함께 진행했던 멘티 분들과 함께 lubycon-ui-kit이라는 작은 프로젝트를 시작했다. 뭐 시작한지 얼마 안 되어서 아직 아무 것도 없지만, 한글을 기반으로 제작해 국내의 디자이너나 개발자가 사용하기 용이한 UI 라이브러리를 만들자는 의의를 가지고 조금씩 작업 중이다.","link":"https://evan-moon.github.io/2020/11/28/making-your-components-extensible-with-typescript/","title":"타입스크립트와 함께 컴포넌트를 단계 별로 추상화해보자","pubDate":"Sat, 28 Nov 2020 12:51:07 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 개발자와 멀고도 가까운 주제인 비즈니스에 대해서 한번 이야기를 해보려고 한다. 개발자들은 늘 좋은 설계와 튼튼한 어플리케이션을 만들기 위해 고심하고 노력하는 사람들이다. 하지만 언제나 좋은 설계를 위한 선택이 옳은 선택일 수는 없다. 왜냐하면 우리는 프로그래밍을 사용하여 돈을 벌기 위해 고용된 사람들이기 때문이다.","link":"https://evan-moon.github.io/2020/10/24/buisiness-with-programming/","title":"일 잘 하는 개발자는 왜 비즈니스까지 신경쓸까?","pubDate":"Sat, 24 Oct 2020 11:30:42 GMT"}},{"node":{"contentSnippet":"In this post, I’m going to talk about the  policy which every web developer would have heard of at least once. In fact, in web development, it is not an exaggeration to say that errors occur with CORS policy violations are very common, and every web developer will experience it at least once.","link":"https://evan-moon.github.io/2020/05/21/about-cors/en/","title":"Why we need to know about CORS?","pubDate":"Tue, 06 Oct 2020 23:31:47 GMT"}},{"node":{"contentSnippet":"워라밸(Work-Life Balance)는 말 그대로 일과 내 삶의 밸런스를 의미하는 것이다. 사실 이 단어는 워낙 유명하기 때문에 필자가 굳이 이게 뭐다 설명하지 않아도 다들 아실 것이라고 생각한다. 사실 이 단어가 내포하고 있는 의미는 일을 부정적으로 바라보는 시각에서부터 출발한다.","link":"https://evan-moon.github.io/2020/09/27/work-life-balance/","title":"정말 근무 시간만이 워라밸의 전부일까?","pubDate":"Sat, 03 Oct 2020 21:19:38 GMT"}},{"node":{"contentSnippet":"필자는 지난 불규칙 속에서 규칙을 찾아내는 정규표현식 포스팅에서 정규식의 기본적인 사용 방법을 한 차례 설명한 바 있다. 그러나 아무리 정규식의 기본적인 사용 방법을 알고 있다고 해도 실제로 정규식을 사용해야하는 상황이 되면 눈 앞이 깜깜해지기 마련이다.","link":"https://evan-moon.github.io/2020/08/15/regex-example/","title":"정규식은 어떻게 사용되는 것일까?","pubDate":"Tue, 15 Sep 2020 11:22:08 GMT"}},{"node":{"contentSnippet":"개발자들은 자연어로 주어지는 문제 상황을 파악하고 프로그램을 설계하고 작성하는 사람들이다. 이런 업무를 수행하기 위해 개발자들은 불규칙하게 쏟아지는 정보들 속에서 필요한 부분들을 걸러내거나 무분별하게 선언된 클래스나 변수들을 추상화하는 등의 업무를 수행하게 된다.","link":"https://evan-moon.github.io/2020/07/24/about-regular-expression/","title":"불규칙 속에서 규칙을 찾아내는 정규표현식","pubDate":"Mon, 10 Aug 2020 09:52:57 GMT"}},{"node":{"contentSnippet":"필자는 친구들과 함께 하고 있는 루비콘이라는 팀에서 토이 프로젝트 멘토링을 재능 기부 형식으로 진행하고 있는데, 많은 분들이 도대체 돈도 안 받으면서 왜 이런 프로젝트를 왜 진행하는지, 얻고자 하는 것이 무엇인지에 대해 많이 물어보셔서 그에 대한 필자의 생각을 적어보려고 한다.","link":"https://evan-moon.github.io/2020/06/20/the-importance-of-sharing/","title":"내가 토이 프로젝트 경험을 공유하는 이유","pubDate":"Tue, 07 Jul 2020 04:34:13 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 웹 개발자라면 한번쯤은 얻어맞아 봤을 법한  정책에 대한 이야기를 해보려고 한다. 사실 웹 개발을 하다보면 CORS 정책 위반으로 인해 에러가 발생하는 상황은 굉장히 흔해서 누구나 한 번 정도는 겪게 된다고 해도 과언이 아니다.","link":"https://evan-moon.github.io/2020/05/21/about-cors/","title":"CORS는 왜 이렇게 우리를 힘들게 하는걸까?","pubDate":"Thu, 21 May 2020 12:51:21 GMT"}},{"node":{"contentSnippet":"During the last golden week, I migrated my blog that I had previously made using  to . In this post, I would like to share what I learned from this migration work and issues I didn’t expect.","link":"https://evan-moon.github.io/2020/05/09/gatsby-migration-retrospective/en/","title":"Migrating from Hexo to Gatsby","pubDate":"Fri, 15 May 2020 23:08:12 GMT"}},{"node":{"contentSnippet":"필자는 지난 황금 연휴 동안 기존에 를 사용하여 만들었던 블로그를 로 마이그레이션했다. 이번 포스팅에서는 1주일 조금 넘게 마이그레이션을 진행하면서 새롭게 학습했던 것들에 대한 공유와 더불어 의외로 예상하지 못했던 이슈들에 대한 공유를 진행해볼까 한다.","link":"https://evan-moon.github.io/2020/05/09/gatsby-migration-retrospective/","title":"Hexo에서 Gatsby로 블로그 마이그레이션 야크쉐이빙 후기","pubDate":"Sat, 09 May 2020 12:57:08 GMT"}},{"node":{"contentSnippet":"Google Tag Manager, 통칭 GTM은 지난 2012년에 출시된 인터넷 인터페이스 상에서의 웹 분석, 광고 성과 측정, 제휴 마케팅 추적 등 다양한 태그를 관리할 수 있는 구글의 이벤트 트래킹 솔루션 중 하나이다. 필자는 이전 직장에서 Google Analytics나 Segment와 같은 이벤트 트래킹 솔루션을 사용하고 있었는데, 물론 이런 툴만으로도 원하는 유저 행동 데이터를 수집하기에 큰 어려움은 없었지만 이 툴들의 단점은 PO나 마케터나 원하는 행동 데이터를 추가로 추적하고 싶을 때마다 개발자에게 코드를 심어달라고 요청해야 한다는 것이다.","link":"https://evan-moon.github.io/2020/04/19/what-is-gtm-google-tag-manager/","title":"GTM, Google Tag Manager 뜯어보기","pubDate":"Sun, 19 Apr 2020 20:50:10 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 프론트엔드 개발자와 백엔드 개발자가 만나는 지점인 API에 대한 이야기를 해보려고한다. 일반적으로 앱이나 웹 상에서 작동하는 어플리케이션을 개발할 때는 주로 HTTP나 HTTPS 프로토콜을 사용하여 API를 만들게 되는데, 이 API의 정의가 얼마나 직관적이고 명확하냐에 따라 프로젝트의 복잡도가 크게 낮아지게 될 만큼 시스템 설계에 있어서 꽤나 중요한 자리를 차지하고 있다.","link":"https://evan-moon.github.io/2020/04/07/about-restful-api/","title":"프론트엔드와 백엔드가 소통하는 엔드포인트, RESTful API","pubDate":"Tue, 07 Apr 2020 02:56:16 GMT"}},{"node":{"contentSnippet":"최근의 모던 어플리케이션은 완전히 네트워크 위에서 돌아가는 프로그램이라고 해도 과언이 아닐 정도로 프로그램의 비즈니스 로직에서 통신이 차지하는 비중이 높다. 클라이언트 어플리케이션은 백엔드에 위치한 서버와 통신하여 현재 로그인한 사용자의 정보를 받아오거나, 새로운 게시글을 생성하기도 하고, 때로는 Web Socket을 통해 서버에서 발생한 이벤트를 구독하여 푸시 메세지나 채팅과 같은 기능을 구현하기도 한다.","link":"https://evan-moon.github.io/2020/03/15/about-http-status-code/","title":"서버의 상태를 알려주는 HTTP 상태 코드","pubDate":"Sun, 15 Mar 2020 21:57:03 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 개발보다는 약간 철학적인 고민 이야기를 해보려고 한다. 사실 이 고민은 필자가 처음 개발을 시작할 때부터 가지고 있던 고민인데, 아직도 해답을 찾지 못했던 질문이기에, 이번 포스팅은 필자의 생각을 제시하는 것이 아니라 질문을 던지는 느낌으로 끄적여볼까한다. 필자가 개발을 시작하고 나서 6년 동안이나 하고 있는 이 고민은 바로 “안다는 것은 무엇인가?”이다.","link":"https://evan-moon.github.io/2020/03/02/what-is-knowing/","title":"나는 프론트엔드를 안다고 말할 수 있을까?","pubDate":"Mon, 02 Mar 2020 01:32:57 GMT"}},{"node":{"contentSnippet":"처음 프로그래밍을 배우던 시절을 떠올려보자. 아닌 사람도 있겠지만 대부분의 사람들은 처음 프로그래밍을 배울 때 학교나 학원에서 누군가에게 가르침을 받으며 공부했을 것이다. 프로그래밍을 처음 접하는 입문자에게 독학의 길은 너무 멀고 험난하기 때문이다.","link":"https://evan-moon.github.io/2020/02/11/question-driven-thinking/","title":"Question Driven Thinking - 스스로 질문하며 학습하기","pubDate":"Tue, 11 Feb 2020 01:16:13 GMT"}},{"node":{"contentSnippet":"함수형 프로그래밍에서 코드를 작성한다는 것은 프로그램에서 수행해야하는 여러가지 행위들을 함수로 표현하고, 또 그 함수들을 요리조리 잘 합성해가며 거대한 프로그램을 만들어나가는 패러다임이다. 결국 함수형 프로그래밍에서 함수를 합성하는 행위라는 것은 이 패러다임의 근간이 되는 개념이기 때문에 굉장히 큰 의미를 가질 수 밖에 없는데, 문제는 이렇게 함수를 합성하는 과정에서 크고 작은 현실적인 문제들이 빵빵 터진다는 것이다.","link":"https://evan-moon.github.io/2020/01/27/safety-function-composition/","title":"어떻게 하면 안전하게 함수를 합성할 수 있을까?","pubDate":"Mon, 27 Jan 2020 17:07:29 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 순수 함수에 이어 함수형 프로그래밍에서 중요하게 여기는 개념인 에 대한 이야기를 해보려고 한다. 사실 순수 함수를 설명하다보면 불변성에 대한 이야기가 꼭 한번은 나오게 되는데, 대부분 “상태를 변경하지 않는 것”이라는 짧은 정의로 설명하거나, 혹은 불변성을 해치는 행위들을 예시로 들고 이런 행위들을 금지 행위로 규정하며 설명을 진행하게된다.","link":"https://evan-moon.github.io/2020/01/05/what-is-immutable/","title":"변하지 않는 상태를 유지하는 방법, 불변성(Immutable)","pubDate":"Sun, 05 Jan 2020 23:48:21 GMT"}},{"node":{"contentSnippet":"이전에 작성했던 기존의 사고 방식을 깨부수는 함수형 사고 포스팅에 이어, 이번 포스팅에서는 함수형 프로그래밍이 지향하는 관점을 실제 프로그램에 구현하기 위해 알고 있어야하는 필수적인 개념 중 하나인 에 대한 이야기를 해볼까 한다.","link":"https://evan-moon.github.io/2019/12/29/about-pure-functions/","title":"수학에서 기원한 프로그래밍 패러다임, 순수 함수","pubDate":"Sun, 29 Dec 2019 17:29:15 GMT"}},{"node":{"contentSnippet":"이제 필자의 마지막 20대를 보내는 2019년도 어느덧 10일 정도 밖에 남지 않았다. 물론 서른이 된다고 해서 크게 달라지는 것은 없지만, 스무살이 되었을 때 이후 처음으로 나이 앞 자리가 바뀌는 만큼 기분이 싱숭생숭 하기도 하다.","link":"https://evan-moon.github.io/2019/12/22/2019-retrospective/","title":"20대의 마지막, 2019년을 돌아보며","pubDate":"Sun, 22 Dec 2019 23:23:26 GMT"}},{"node":{"contentSnippet":"최근 많은 언어들이 함수형 프로그래밍 패러다임을 도입하며, 이에 대한 개발자들의 관심 또한 나날히 높아지고 있다. 필자 또한 라는 책을 읽으면서 기존의 패러다임과 사뭇 다른 함수형 프로그래밍에 대해 많은 관심을 가지게 되었던 기억이 있다.","link":"https://evan-moon.github.io/2019/12/15/about-functional-thinking/","title":"기존의 사고 방식을 깨부수는 함수형 사고","pubDate":"Sun, 15 Dec 2019 22:06:03 GMT"}},{"node":{"contentSnippet":"최근 4일 동안 심한 몸살에 걸려 침대에만 누워있으면서, 컴퓨터를 멀리 하고 오랜만에 책을 읽었다. 마침 입사 예정인 회사에서 “린 스타트업”과 “파워풀” 두 권의 책을 보내주었기 때문에 무엇을 읽어볼까하는 고민을 할 시간을 줄일 수 있었다.","link":"https://evan-moon.github.io/2019/12/04/about-honestly-feedback/","title":"솔직한 피드백으로 좋은 팀워크를 만들 수 있을까? - 파워풀을 읽고","pubDate":"Wed, 04 Dec 2019 01:01:32 GMT"}},{"node":{"contentSnippet":"혼잡 제어란, 말 그대로 네트워크의 혼잡 상태를 파악하고 그 상태를 해결하기 위해 데이터 전송을 제어하는 것을 이야기한다. 네트워크는 워낙 광대한 블랙박스이기 때문에 정확히 어디서 어떤 이유로 전송이 느려지는지는 파악하기 힘들지만, 단순히  정도는 각 종단에서도 충분히 파악할 수 있다. 그냥 데이터를 보냈는데 상대방으로부터 응답이 늦게 오거나 안오면 뭔가 문제가 있다는 것이니 말이다.","link":"https://evan-moon.github.io/2019/11/26/tcp-congestion-control/","title":"사이 좋게 네트워크를 나눠 쓰는 방법, TCP의 혼잡 제어","pubDate":"Tue, 26 Nov 2019 17:23:57 GMT"}},{"node":{"contentSnippet":"은 원활한 통신을 위해 전송하는 데이터 흐름을 제어하고 네트워크의 혼잡 상태를 파악해서 대처하는 기능을 프로토콜 자체에 포함하고 있다.","link":"https://evan-moon.github.io/2019/11/22/tcp-flow-control-error-control/","title":"패킷의 흐름과 오류를 제어하는 TCP","pubDate":"Fri, 22 Nov 2019 16:46:25 GMT"}},{"node":{"contentSnippet":"저번에 작성했던 TCP의 헤더에는 어떤 정보들이 담겨있는걸까? 포스팅에 이어 이번에는 TCP의 핸드쉐이크 과정과 그 속에서 변화하는 TCP 상태에 대해서 한번 알아보려고 한다.","link":"https://evan-moon.github.io/2019/11/17/tcp-handshake/","title":"TCP가 연결을 생성하고 종료하는 방법, 핸드쉐이크","pubDate":"Sun, 17 Nov 2019 19:56:06 GMT"}},{"node":{"contentSnippet":"지난 달, 다니고 싶었던 회사의 면접에서 기초 실력 부족으로 시원하게 박살났다. 다행히 면접이 끝난 직후 필자가 대답하지 못했던, 풀어내지 못했던 질문들과 문제를 깃허브에 정리해두었기 때문에 어떤 것을 공부해야하는지 바로 알 수 있었고, 한 달동안 컴퓨터 사이언스의 기초에 대한 내용을 집요하게 파헤쳤다.","link":"https://evan-moon.github.io/2019/11/16/the-way-to-control-anxiety/","title":"불안한 마음 정면으로 마주보기","pubDate":"Sat, 16 Nov 2019 17:17:57 GMT"}},{"node":{"contentSnippet":"저번에 HTTP/3는 왜 UDP를 선택한 것일까? 포스팅을 진행하며 TCP에 대해 간단한 언급을 했었지만, 해당 포스팅에서는 기존의 HTTP에서 사용하던 TCP에 어떤 문제가 있었는지에 집중해서 이야기했었지만 이번에는 TCP 자체에 조금 더 집중해서 이야기해보려고 한다.","link":"https://evan-moon.github.io/2019/11/10/header-of-tcp/","title":"TCP의 헤더에는 어떤 정보들이 담겨있는걸까?","pubDate":"Sun, 10 Nov 2019 18:39:00 GMT"}},{"node":{"contentSnippet":"최근 많은 IT 기업들이 개발자를 채용할 때 코딩 테스트를 시행하고 있다. 회사마다 어떤 스타일의 문제를 출제하는지 차이는 있지만, 대부분 간단한 알고리즘 풀이 또는 나 와 같은 사이트처럼 실무에서 겪을 만한 상황을 살짝 섞어놓는 느낌의 문제를 선호하는 것 같다.","link":"https://evan-moon.github.io/2019/10/30/make-simple-with-math/","title":"수학과 함께 복잡한 문제를 단순하게 만들자!","pubDate":"Wed, 30 Oct 2019 22:38:26 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 이전 포스팅에 이어, 프로토타입을 사용한 다양한 상속 패턴에 대한 이야기를 해볼까 한다. 사실 자바스크립트에는 상속이나 캡슐화와 같은 개념이 명시적으로 존재하지는 않기 때문에 자바나 C++ 같은 클래스 기반 언어를 사용하던 개발자들은 자바스크립트에 클래스가 없다는 사실에 혼란스러워한다.","link":"https://evan-moon.github.io/2019/10/27/inheritance-with-prototype/","title":"[JS 프로토타입] 프로토타입을 사용하여 상속하기","pubDate":"Sun, 27 Oct 2019 00:55:05 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 자바스크립트(JavaScript)하면 빠질 수 없는 에 대해서 한번 이야기해보려고 한다. 프로토타입은 자바스크립트를 ES5 시절부터 사용해오던 분들에게는 매우 익숙하지만 ES6부터 시작하신 분들은 대부분 클래스를 사용하기 때문에 익숙한 개념은 아닐 것이라고 생각한다.","link":"https://evan-moon.github.io/2019/10/23/js-prototype/","title":"[JS 프로토타입] 자바스크립트의 프로토타입 훑어보기","pubDate":"Wed, 23 Oct 2019 21:29:01 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 대표적인 자료 구조 중 하나인 에 대한 설명과 구현을 한번 해보려고 한다.","link":"https://evan-moon.github.io/2019/10/12/introduction-data-structure-heap/","title":"최소 값과 최대 값을 빠르게 찾을 수 있게 도와주는 힙(Heap)","pubDate":"Sat, 12 Oct 2019 18:51:14 GMT"}},{"node":{"contentSnippet":"는 의 세 번째 메이저 버전으로, 기존의 HTTP/1, HTTP/2와는 다르게 UDP 기반의 프로토콜인 을 사용하여 통신하는 프로토콜이다. HTTP/3와 기존 HTTP 들과 가장 큰 차이점이라면 TCP가 아닌 UDP 기반의 통신을 한다는 것이다.","link":"https://evan-moon.github.io/2019/10/08/what-is-http3/","title":"HTTP/3는 왜 UDP를 선택한 것일까?","pubDate":"Tue, 08 Oct 2019 01:57:49 GMT"}},{"node":{"contentSnippet":"과거에는 블로그를 운영하는 개발자들이 오히려 손에 꼽을 정도였지만, 최근 많은 개발자들이 블로그를 운영하며 다양한 주제에 대한 자신의 생각이나 특정 기술에 대한 분석을 포스팅으로 기재하고 공유하고 있다. 하지만 필자는 개인적으로 블로그를 운영하는 것이 생각보다 진입 장벽이 높다고 생각하는데, 그건 블로그 세팅과 같은 기술적인 이유 때문이 아니다.","link":"https://evan-moon.github.io/2019/09/28/how-do-i-write-postings/","title":"블로그 개설을 망설이고 있는 사람들에게","pubDate":"Sat, 28 Sep 2019 08:43:03 GMT"}},{"node":{"contentSnippet":"필자는 작년인 2018년, 이라 불리는 탈진 증상을 한 차례 격하게 겪은 적이 있다. 번아웃은 2019년 5월 에서도 ICD-11에 정식으로 등록할 만큼 관심을 가지고 있는 증상 중 하나이다. WHO는 번아웃이 의학적인 질병에는 포함되지 않지만 직업 관련 증상 중 하나라고 이야기하고 있다.","link":"https://evan-moon.github.io/2019/09/23/how-to-overcome-burnout/","title":"내가 겪었던 번아웃, 그리고 극복했던 경험","pubDate":"Mon, 23 Sep 2019 08:03:54 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 I/O와 네트워크 등 전반적으로 다양한 모델에서 사용하는 개념인 가 정확히 무엇을 의미하는 것인지, 그리고 동기 방식과 비동기 방식의 차이에 대해서 한번 이야기 해보려고 한다. 그리고 이 두 가지 개념과 많이 혼동되는 개념인 과 에 대해서도 간단하게 짚고 넘어갈 예정이다.","link":"https://evan-moon.github.io/2019/09/19/sync-async-blocking-non-blocking/","title":"동기(Synchronous)는 정확히 무엇을 의미하는걸까?","pubDate":"Thu, 19 Sep 2019 08:54:09 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 많은 분들이 질문해주신 에 대해서 한번 이야기해보려고 한다. 어떻게 보면 예민한 주제일수도 있지만 주변에 이와 같은 질문을 주시는 분들도 꽤 있는데다가, 심지어 컴퓨터 공학을 전공하지 않았다는 이유로 자기 자신을 낮게 평가하시는 분도 계셨다. 필자는 이런 것들이 어떤 특정 개인에게 국한된 것이 아니라고 생각되어 이에 대한 필자의 생각을 조심스럽게 한번 적어보려고 한다.","link":"https://evan-moon.github.io/2019/09/09/major-is-not-important/","title":"비전공 개발자가 전공자보다 정말 불리할까?","pubDate":"Mon, 09 Sep 2019 08:20:39 GMT"}},{"node":{"contentSnippet":"필자는 지난 9월 1일에 체코 프라하에 도착해서 휴가를 보내고 있는 중이다. 근데 마냥 휴가라고 하기에는 뭐한게, 전 직장과 프리랜서 계약을 했기 때문에 여기서도 결국 코딩을 하고 있기 때문이다. 나름 디지털 노마드 흉내를 내고 있는 셈이다.","link":"https://evan-moon.github.io/2019/09/06/life-in-prague-tip/","title":"프라하에서 디지털 노마드로 살아남기","pubDate":"Fri, 06 Sep 2019 07:30:23 GMT"}},{"node":{"contentSnippet":"최근 필자는 산드로 만쿠소의 이라는 책을 읽게 되었는데, 이 책을 읽으며 느낀 점이 많았기 때문에 이번 포스팅에서는 산드로 만쿠소가 이 책을 통해 이야기하고자 하는 것이 무엇인지와 그에 따른 필자의 생각을 한번 편하게 적어보려고 한다.","link":"https://evan-moon.github.io/2019/09/05/about-software-craftsmanship/","title":"나는 어떤 마음으로 소프트웨어를 만들어야 하는가","pubDate":"Thu, 05 Sep 2019 07:58:15 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 Git의 머지 전략 중 대표적인 3가지인 , , 의 차이에 대해서 한번 이야기해보려고 한다. 이 3가지 머지 전략 모두 브랜치를 머지한다는 목적은 같지만, 어떤 방식을 선택하냐에 따라 가 기록되는 방식이 달라지게 된다.","link":"https://evan-moon.github.io/2019/08/30/commit-history-merge-strategy/","title":"커밋 히스토리를 이쁘게 단장하자","pubDate":"Fri, 30 Aug 2019 14:31:29 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 개발자들에게 뗄레야 뗄 수 없는 키워드인 에 대해서 포스팅 해보려고 한다. 물론 다른 직종도 마찬가지겠지만 다른 업계보다 빠르게 변화하는 IT 업계의 특성 상 개발자는 시대의 흐름을 따라가기위해 은퇴 전까지 계속 해서 공부를 하는 수 밖에 없다.","link":"https://evan-moon.github.io/2019/08/26/how-does-developer-study/","title":"개발자가 공부로 살아남는 방법","pubDate":"Mon, 26 Aug 2019 18:53:03 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 , 줄여서 흔히들 OOP라고 부르는 설계 방법론에 대해서 이야기해보려고 한다. OOP는 프로그래밍의 설계 패러다임 중 하나로, 현실 세계를 프로그램 설계에 반영한다는 개념을 기반으로 접근하는 방법이다. OOP는 90년대 초반부터 유명해지기 시작했지만 아직까지도 전 세계의 많은 개발자들이 사용하고 있는 설계 패턴 중 하나이기 때문에 알아둬서 나쁠 건 없다.","link":"https://evan-moon.github.io/2019/08/24/what-is-object-oriented-programming/","title":"알고 보면 재밌는 객체 지향 프로그래밍, OOP 흝어보기","pubDate":"Sat, 24 Aug 2019 12:35:13 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 저번 포스팅에 이어 HTML5 Audio API를 사용하여 실제로 오디오 이펙터를 만드는 과정에 대해서 포스팅 하려고 한다. 저번 포스팅에서 이미 이야기 했듯이 Audio API는 여러 개의 노드를 연결하여 오디오의 흐름을 만들어 내는 것을 기본 개념으로 가지고 있고, 이펙터를 만들기 위해 필요한 몇 개의 추상화된 노드들을 기본적으로 제공해주기 때문에 그렇게 어려울 건 없다.","link":"https://evan-moon.github.io/2019/08/21/javascript-audio-effectors-practice/","title":"[JavaScript 오디오 이펙터 만들기] 오디오 이펙터로 나만의 소리 만들기","pubDate":"Wed, 21 Aug 2019 19:59:46 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 컴퓨터는 어떻게 소리를 들을까? 포스팅에서 진행했던 오디오 파형 그리기에 이어서 오디오에 여러가지 효과를 줄 수 있는 이펙터를 만드는 과정을 설명하려고 한다. HTML5의 Audio API는 오디오에 효과를 줄 수 있는 여러가지 노드를 제공하는데, 대부분의 이펙터는 이 노드들만 사용해도 구현할 수 있을 정도로 완성도있는 API를 제공한다.","link":"https://evan-moon.github.io/2019/08/19/javascript-audio-effectors-gain/","title":"[JavaScript 오디오 이펙터 만들기] 소리의 흐름을 파악하자","pubDate":"Mon, 19 Aug 2019 13:12:07 GMT"}},{"node":{"contentSnippet":"오늘, 2019년 8월 16일을 마지막으로 지난 2년 동안 근무했던 회사를 떠나게 되었다. 지금은 사무실에서 말년의 여유를 즐기며 이 포스팅을 서두를 작성하고 있다. 그래서 이번 포스팅에서는 지난 2년 간 필자가 이 회사를 다니면서 느꼈던 점이나 입사 당시와 비교해서 지금 달라진 점들에 대해서 회고를 진행하려고 한다.","link":"https://evan-moon.github.io/2019/08/17/leave-the-company/","title":"2년 동안 근무했던 회사를 떠나며","pubDate":"Sat, 17 Aug 2019 16:52:40 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 실시간으로 빠르게 쌓이는 데이터들의 평균을 효율적으로 구할 수 있는 방법에 대해서 간단하게 설명하려고 한다. 이런 실시간 데이터의 평균을 구해야하는 경우는 생각보다 꽤 많은데, 서버 엔진의 액세스 로그에 쌓이는 응답들의 평균 응답 시간을 구한다던가, 센서에서 들어오는 값들의 평균을 구한다던가 하는 경우이다. 이때 이런 데이터들은 빠르게는  정도의 간격으로 수집되는 경우도 비일비재하기 때문에, 데이터를 입력받자마자 빠르게 처리해야하는 성능이 굉장히 중요하다.","link":"https://evan-moon.github.io/2019/08/11/average-filter/","title":"실시간 데이터의 평균을 효율적으로 구하기","pubDate":"Sun, 11 Aug 2019 18:50:43 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 최근에 고쳤던 Webpack Watch 기능의 메모리 누수에 대해서 간략하게 남겨보려고 한다. 필자가 회사에서 개발한 프로젝트가 점점 커짐에 따라서 Watch 중에 빌드를 여러 번 돌리게되면 어느 순간 갑자기 가 뜨면서 프로세스가 죽어버리는 이슈가 발생하였다. 이 문제는 사실 꽤 예전부터 발생했던 이슈지만 계속 비즈니스 이슈를 개발하느라고 외면받고 있던 이슈였는데 우연히 기회가 되어 해당 이슈를 자세히 들여다 볼 수 있었다.","link":"https://evan-moon.github.io/2019/08/08/fix-webpack-dev-memory-leak/","title":"Webpack Watch의 메모리 누수 고치기","pubDate":"Thu, 08 Aug 2019 16:20:55 GMT"}},{"node":{"contentSnippet":"우리는 개발자이기 이전에 어떤 조직의 구성원이기 때문에 조직문화로부터 생각보다 많은 영향을 받고 있다. 그렇기 때문에 우리는 한 조직의 일원으로 이 조직이 건강한 조직인지, 우리가 활기차고 생산성있게 일하고 있는지 끊임없는 관심을 가져야 한다.","link":"https://evan-moon.github.io/2019/08/06/developer-with-organizational-culture/","title":"개발자가 조직문화에 대해 관심을 가져야 하는 이유","pubDate":"Tue, 06 Aug 2019 08:30:21 GMT"}},{"node":{"contentSnippet":"1년 전, 필자는 setImmediate & process.nextTick의 차이점에 대해 설명하면서 Node.js의 이벤트 루프 구조에 대해 살짝 언급한 적이 있었다. 놀랍게도 독자 분들은 원래 설명하려고 했던 부분보다 이벤트 루프 부분에 대해서 더 많이 관심을 주었고, 필자는 그 부분에 대해서 많은 질문을 받았었다. 그래서 이번에는 Node.js의 이벤트 루프를 구성하는 로우 레벨의 동작을 자세하게 설명해보려고 한다.","link":"https://evan-moon.github.io/2019/08/01/nodejs-event-loop-workflow/","title":"로우 레벨로 살펴보는 Node.js 이벤트 루프","pubDate":"Thu, 01 Aug 2019 08:46:24 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 저번 포스팅인 Git 뉴비를 위한 기초 사용법 - 시작하기에서 설명했던 기본적인 명령어보다 좀 더 나아가서 몇 가지 개념과 명령어를 더 공부해보려고 한다. 저번 포스팅에서는 리모트 서버에서 소스를 클론하고 수정한 후 다시 리모트 서버에 업데이트하는 과정에 대해 집중해서 설명했다면 이번 포스팅에서는 Git의 메인 주제인 에 대해 더 다뤄볼 예정이다.","link":"https://evan-moon.github.io/2019/07/28/git-tutorial-advanced/","title":"Git 뉴비를 위한 기초 사용법 - 버전 관리","pubDate":"Sun, 28 Jul 2019 08:13:20 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 너도 쓰고 나도 쓰고 우리 모두 쓰고 있는 의 기초에 대해서 포스팅 하려고한다. 필자는 Git을 대학교 때 처음 접했는데 처음에는 “왠 이상한 클라우드에 소스코드를 올려놓는다” 정도로만 이해하고 사용했던 기억이 난다. 하지만 Git의 기능은 단순히 코드 공유에서 끝나지 않는 이므로 Git을 잘 쓰면 실무에서 펼쳐지는 다이나믹한 상황에 유연하게 대처할수도 있다.","link":"https://evan-moon.github.io/2019/07/25/git-tutorial/","title":"Git 뉴비를 위한 기초 사용법 - 시작하기","pubDate":"Thu, 25 Jul 2019 07:32:17 GMT"}},{"node":{"contentSnippet":"필자는 최근 라는 책을 집필했다. 필자는 책처럼 긴 글을 적는 게 사실 처음이라, 책을 쓰는 과정이나 책을 출판하는 과정에 대해서 무지한 상태로 집필을 시작했고 그래서 중간에 우여곡절도 꽤 많았다.\n그래서 이번 포스팅에서는 필자와 같이 책을 집필하고 싶어하는 분들을 위해서 2018년 8월부터 2019년 7월까지 책을 집필했던 과정과 어려움에 대해서 이야기 해보려고 한다.","link":"https://evan-moon.github.io/2019/07/21/vuejs-book-retrospective/","title":"흔한 개발랭이의 작가 입문기","pubDate":"Sun, 21 Jul 2019 08:37:04 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 필자가 많이 받은 질문 중 하나인 라는 질문에 대해서 한번 이야기 해볼까 한다. 물론 이 주제는 전 세계의 많은 개발자들 간에도 의견이 갈리는 내용이기 때문에 그냥 지나가는 개발자 한명의 생각일 뿐이라고 생각해줬으면 좋겠다.","link":"https://evan-moon.github.io/2019/07/17/programmer-with-math/","title":"개발자는 수학을 잘해야할까?","pubDate":"Wed, 17 Jul 2019 08:31:30 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 에 대해서 한번 이야기 해볼까 한다. 랜덤이란 어떤 사건이 발생했을 때 이전 사건과 다음 사건의 규칙성이 보이지 않는, 말 그대로 로 발생하는 패턴을 이야기한다. 우리가 사용하고 있는 컴퓨터도 랜덤한 패턴을 만들어야 할 때가 있고 또 실제로도 만들고 있다.","link":"https://evan-moon.github.io/2019/07/14/what-is-random/","title":"컴퓨터가 만드는 랜덤은 정말로 랜덤할까?","pubDate":"Sun, 14 Jul 2019 23:21:59 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 필자의 예전 직업이었던 의 추억을 살려서 한번 오디오에 대한 이론을 설명해볼까 한다. 하지만 이론 설명만 하면 노잼이니까 오디오 이론을 기초로 자바스크립트의 를 사용하여 간단한 오디오 파형까지 그려보려고 한다.","link":"https://evan-moon.github.io/2019/07/10/javascript-audio-waveform/","title":"컴퓨터는 어떻게 소리를 들을까?","pubDate":"Wed, 10 Jul 2019 08:21:44 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 필자가 회사에서 2019년 7월 5일 금요일 하루 동안 기존 어플리케이션에  기능을 붙힌 삽질기를 기록하려고 한다. 는 지원하지 않는 브라우저에 대한 예외처리만 꼼꼼하게 해주면 , ,  등에서 무조건 플러스 요인이기 때문에 예전부터 계속 해보고 싶었다.","link":"https://evan-moon.github.io/2019/07/06/pwa-with-notification/","title":"PWA 하루 만에 도입하기(삽질기)","pubDate":"Sat, 06 Jul 2019 21:06:37 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 에 대해서 이야기해볼까한다. 데이터 기반 의사결정은 2013년 쯤 빅데이터 열풍이 불면서 뜨기 시작했다. 이미 많은 기업들이 사용하고 있는 의사결정 방법이며 또한 여러가지 선례도 많기 때문에 나름 신뢰성을 가지는 의사결정 방법이다.","link":"https://evan-moon.github.io/2019/07/04/danger-of-data-driven/","title":"데이터 기반 의사결정, 과연 완벽한 걸까?","pubDate":"Thu, 04 Jul 2019 16:40:13 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 소프트웨어 개발 방법론 중 하나인 , 줄여서 애자일이라고 부르는 그것에 대해서 포스팅하려고 한다. 최근 많은 조직들이 애자일 프로세스를 사용하고 있고, 필자가 다니고 있는 현 직장도 마찬가지로 애자일 프로세스를 도입해서 사용하고 있다.","link":"https://evan-moon.github.io/2019/07/02/what-is-agile/","title":"애자일이 도대체 뭐길래?","pubDate":"Tue, 02 Jul 2019 08:16:10 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 구글의 V8 엔진이 어떤 방식으로 자바스크립트를 해석하고 실행하는지 살펴 보는지에 대해 포스팅하려고 한다. 은 로 작성되었지만 필자의 메인 언어가 이 아니기도 하고, 워낙 소스가 방대하기 때문에 자세한 분석까지는 아니라도 최대한 웹 상에 있는 정보들과 필자가 분석한 의 소스코드를 비교해가면서 살펴보려고 한다.","link":"https://evan-moon.github.io/2019/06/28/v8-analysis/","title":"V8 엔진은 어떻게 내 코드를 실행하는 걸까?","pubDate":"Fri, 28 Jun 2019 15:58:28 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 많이 사용되는 자료구조 중 하나인 에 대해서 정리하려고 한다. 먼저 이 무엇인지, 왜 사용하는지 알아보자!","link":"https://evan-moon.github.io/2019/06/25/hashtable-with-js/","title":"JavaScript와 함께 해시테이블을 파헤쳐보자","pubDate":"Tue, 25 Jun 2019 08:22:36 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 우리 집에서 구글까지 어떤 과정을 통해 통신을 하는지에 대해서 간략하게 얘기해보려고 한다. 필자가 대학에서 배운 것 중 재밌다고 생각했던 것이 몇 개 있는데, 그 중 대표적인 것이 바로 인터넷에 연결되어 있는 모든 컴퓨터는 실제로 케이블을 통해 연결되어있다는 사실이었다.","link":"https://evan-moon.github.io/2019/06/22/my-home-to-google/","title":"우리 집에서 구글까지 가는 길","pubDate":"Sat, 22 Jun 2019 08:32:09 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 JavaScript ES6에서 추가되었던 과  키워드에 대해서 자세히 포스팅하려고 한다. 부끄럽지만 지금까지 필자는 과 는 호이스팅이 되지 않는다고 생각하고 있었다. 하지만 얼마 전 친구와 대화하던 중에 과 도 호이스팅 대상체이지만 라는 특수한 영역을 사용하여 참조를 방어하는 것임을 알게 되었다.","link":"https://evan-moon.github.io/2019/06/18/javascript-let-const/","title":"JavaScript의 let과 const, 그리고 TDZ","pubDate":"Tue, 18 Jun 2019 15:19:49 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 Atlassian의 대표 제품 중 하나인 에 대해서 포스팅 하려고 한다. 요즘 많은 IT회사들에서 을 사용하여 소프트웨어 개발을 진행하고 있다. Jira는 애자일 방법론에서 사용하는 다양한 방법들을 좀 더 쉽고 편하게 사용할 수 있게 도와준다.","link":"https://evan-moon.github.io/2019/06/16/jira-customizing-issue/","title":"JIRA 프로젝트 이슈 커스터마이징하기","pubDate":"Sun, 16 Jun 2019 22:50:00 GMT"}},{"node":{"contentSnippet":"이 포스팅은 2017년 9월 2일에 Paul Shan이 작성한 Diving deep into JavaScript array - evolution & performance를 번역한 글입니다.  포스팅을 시작하기 전에 이 포스팅은 JavaScript 배열의 구문에 관한 것을 알려주거나 예제를 보여주는 등의 기본적인 내용은 아니라고 먼저 얘기해두고 싶다. 이 포스팅에서는 메모리 표현, 최적화, 구문에 따라 달라지는 동작의 차이, 성능 및 최근의 JavaScript 배열이 어떻게 발전했는지에 관해서만 설명할 것이다.","link":"https://evan-moon.github.io/2019/06/15/diving-into-js-array/","title":"JavaScript 배열(Array)의 발전과 성능에 대해서 자세히 알아보기","pubDate":"Sat, 15 Jun 2019 12:43:13 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 AWS(Amazon Web Service) 환경에서 HTTP/2 프로토콜을 적용하는 방법에 대해서 설명하려고 한다. AWS의 와 는 자체적으로 HTTP/2 프로토콜을 사용할 수 있는 기능들을 제공해주고 있기 때문에 별도의 작업 없이 간단하게 HTTP/2 프로토콜을 적용할 수 있다.","link":"https://evan-moon.github.io/2019/06/13/http2-with-aws/","title":"AWS와 함께 간단하게 HTTP/2 적용하기","pubDate":"Thu, 13 Jun 2019 21:44:00 GMT"}},{"node":{"contentSnippet":"2016년 처음 회사에서 개발자로 일을 시작해서 201…","link":"https://evan-moon.github.io/2019/06/06/what-is-good-programmer/","title":"좋은 개발자란 무엇일까?","pubDate":"Thu, 06 Jun 2019 20:18:03 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 필자의 현직장에서 진행했던 클라이언트 사이드 렌더링 최적화에 대해서 적어보려고 한다. 크롬 브라우저의 Audits 탭에서 현재 페이지의 퍼포먼스나 SEO 점수와 같은 지표를 확인해볼 수 있다.","link":"https://evan-moon.github.io/2019/06/03/client-render-optimizing/","title":"클라이언트 사이드 렌더링 최적화","pubDate":"Mon, 03 Jun 2019 17:22:26 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 저번 포스팅에 이어 TypeScript를 사용하여 간단한 인공신경망을 만들어본 것을 간단하게 정리하려고 한다.","link":"https://evan-moon.github.io/2019/02/26/simple-ann/","title":"TypeScript와 함께 간단한 인공 신경망 만들기","pubDate":"Tue, 26 Feb 2019 15:03:48 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 대표적인 정렬알고리즘 5가지와 대략적인 에 대해서 정리하려고 한다. 먼저, 그 5가지 정렬알고리즘은 다음과 같다.","link":"https://evan-moon.github.io/2018/10/13/sort-algorithm/","title":"정렬 알고리즘 정리 (Bubble, Selection, Insertion, Merge, Quick)","pubDate":"Sat, 13 Oct 2018 21:25:54 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 Universal Server Side Rendering에 이어서 VueJS의 공식 라이브러리인 와 를 사용하여  어플리케이션을 개발한 과정과 운영 환경에서 생겼던 문제, 그리고 그 문제를 어떻게 해결했는지 적어보려고 한다.","link":"https://evan-moon.github.io/2018/09/25/vue-ssr/","title":"Vue Server Side Rendering","pubDate":"Tue, 25 Sep 2018 23:02:33 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 최근 모던 웹 어플리케이션에서 많이 사용하고 있는 에 대해서 설명하고자 한다. 과 의 방식을 간단하게 알아보고 이 두 렌더 방식을 조합한 의 방식을 설명한다.","link":"https://evan-moon.github.io/2018/09/25/universal-ssr/","title":"Universal Server Side Rendering이란?","pubDate":"Tue, 25 Sep 2018 15:35:48 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 저번 포스팅에 이어 에 대해서 알아보려고 한다. 앞서 설명했듯, 이 알고리즘으로 인해 에서의 학습이 가능하다는 것이 알려져, 암흑기에 있던  학계가 다시 관심을 받게 되었다.","link":"https://evan-moon.github.io/2018/07/19/deep-learning-backpropagation/","title":"[Deep Learning 시리즈] Backpropagation, 역전파 알아보기","pubDate":"Thu, 19 Jul 2018 08:27:18 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 딥러닝이 무엇인지, 기존의 뉴럴네트워크와 다른 점이 무엇인지에 대해서 포스팅하려고 한다.","link":"https://evan-moon.github.io/2018/07/17/deep-learning-intro/","title":"[Deep Learning 시리즈] 딥러닝이란 무엇일까?","pubDate":"Tue, 17 Jul 2018 23:33:01 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 Paypal의 RESTful API인 을 사용하는 방법에 대해서 포스팅 하려고 한다. 진행하기에 앞서 먼저, 페이팔 샌드박스 홈페이지에 접속해서 sandbox용 계정을 만들어야 한다.","link":"https://evan-moon.github.io/2017/05/14/paypal-express-checkout/","title":"페이팔의 Express Checkout Restful API 사용하기","pubDate":"Sun, 14 May 2017 12:36:23 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 저번 포스팅에 이어 중력을 직접 JS로 구현해보려고 한다. 개발환경은 JavaScript ES7, babel, Webpack, Three.js을 사용하였다.","link":"https://evan-moon.github.io/2017/05/06/gravity-via-js-2/","title":"[JavaScript로 중력 구현하기] 2. 코딩하기","pubDate":"Sat, 06 May 2017 14:22:01 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 만유인력의 법칙을 이용하여 중력을 구현해보려고 한다. 지표면 상에서 한 방향으로 작용하는 중력이 아니라 랜덤한 질량을 가진 여러 개의 물체를 랜덤한 좌표에 뿌려놓고 서로의 운동에 어떻게 간섭하는 지를 살펴볼 수 있는 시뮬레이션을 만들어 볼 예정이다.","link":"https://evan-moon.github.io/2017/05/06/gravity-via-js-1/","title":"[JavaScript로 중력 구현하기] 1. 중력이란 무엇일까?","pubDate":"Sat, 06 May 2017 12:32:50 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 저번 포스팅에 이어 실제 궤도의 모양과 크기, 위치, 방향을 정의하고 JavaScript코드로 작성을 해보려고 한다. 실제 어플리케이션을 작성할 때는 TypeScript를 사용하였으나, 편의상 JavsScript ES6로 포스팅을 진행한다.","link":"https://evan-moon.github.io/2017/05/03/calculate-orbit-2/","title":"[JavaScript로 천체 구현하기] 행성의 움직임을 구현해보자","pubDate":"Wed, 03 May 2017 21:49:04 GMT"}},{"node":{"contentSnippet":"In this post, I am going to share how to calculate the orbits and positions of celestial bodies that I learned while developing a solar system simulation with JavaScript. Actually, I was not a good student in mathematics, so I will try to explain mathematical theories about this as easily as possible for other people like me.","link":"https://evan-moon.github.io/2017/05/03/calculate-orbit-1/en/","title":"[Calculating celestial orbit with JavaScript] What is Kepler 6 elements?","pubDate":"Wed, 03 May 2017 15:29:22 GMT"}},{"node":{"contentSnippet":"이번 포스팅에서는 태양계 시뮬레이션을 개발하면서 제일 애먹었던 천체의 궤도와 위치 계산에 대해서 알아보려고 한다. 필자는 고등학교 시절 수포자였기 대문에 필자와 같이 수포자였던 분들을 위해 최대한 간단하게 설명하는 것을 목표로 하고 있다.","link":"https://evan-moon.github.io/2017/05/03/calculate-orbit-1/","title":"[JavaScript로 천체 구현하기] 케플러 6요소 알아보기","pubDate":"Wed, 03 May 2017 15:29:22 GMT"}}]},"allFeedJimBlog":{"edges":[{"node":{"contentSnippet":"들어가며\n딥러닝을 입문하며 자연스럽게 파이썬을 함께 공부하게 되었고, 지금은 주 언어로 사용하고 있다. 하지만 나는 파이썬에 대해 얼마나 알고 있을까?\nC/C++ 같은 컴파일 언어만 사용했던 나에게 파이썬은 꽤 난해하게 다가왔다. 사실, 파이썬을 사용하면서 느낀 생각은 “파이썬의 철학(The Zen of Python)과 많이 다른데..?” 였다. 파이썬 철학의 핵심 내용은 명확한 그리고 가급적이면 유일한 하나의 방법이 존재한다(There should be one-- and preferably only one --obvious way to do it)이며, 이에 따르면 규칙이 굉장히 엄격한 언어라고 한다. 그렇지만 나에게 파이썬의 처음에는 굉장히 자유로운 언어로 보였다.\n\n애초에 멀티 패러다임 언어이기도 하면서, 명시적인 것이 암시적인 것보다 낫다(Explicit is better than implicit)고 하지만 동적 타입이기 때문에 타입 선언부터 암시적이며 굉장히 유연하다. 마찬가지로, 들여쓰기도 자유도를 제한한다고 하지만 나는 “어차피 들여쓰기는 맞춰서 하는건데 무려 괄호를 안써도 된다고?” 라고 생각했다.. 이렇게 자유도를 주었으면서, 하나의 답으로 수렴하게 되는 언어라니.. 이해가 어려웠다. 그렇지만 그 정도로 자유도가 높은 언어기 때문에 컨벤션을 중시하는 거지 싶기도 했다. 어찌되었든 언어의 자유도가 높을수록 오류가 높아질 확률도 높아질테니 말이다. 파이썬은 실행가능한 의사코드(Executable Pseudocode)라고 불릴 정도로 자연어와 유사한 high-level 언어이며, 파이썬스럽다(Pythonic)는 말까지 나올정도로 독자적이다. 사람이 쓰기 쉽도록 만든 언어이므로 사람이 읽기 편하게 하는데 가장 중점을 두며, 유연한만큼 제약이 많이 두고 있다고 생각한다.\n역시 파이썬은 아직 어렵다. 😂\n xkcd - Python!\n나는 기본적으로 어떤 언어든 본질은 같기 때문에 머리에 로직만 담고 있으면, 금방 사용할 수 있다는 생각을 하고 있었다. 이 생각이 바뀐 것은 아니지만 최근에는 여기에 +$\\alpha$로 언어의 철학을 이해하려는 노력이 필요하다는 생각을 하게 되었다. 클린 코드를 지향한다고 항상 말하지만 클린 코드가 정확히 무엇인지 깊게 고민한 적은 없었던 듯 싶다. 클린 코드를 위한 방법은 여러가지가 있겠으나, 언어의 철학은 언어를 만든 원리와도 같으며, 이를 이해하는 것이 그 첫걸음이라 생각하며 포스트를 작성한다. 파이썬이 왜 Garbage Collection과 Reference Count를 하는지, 왜 GIL 정책을 사용하는지도 파이썬의 기본 원리를 통해 이해할 수 있으리라 믿는다.(다만, 이번 포스팅에서는 Garbage Collection과 Reference Count에 대해서 깊게 다루지는 않을 듯 하다.)\n파이썬에서 멀티 쓰레딩이 사실 싱글 쓰레딩이라구요?\n최근 이런 질문을 받았다. (사실상 이 포스트를 쓰게 된 이유. 많은 도움이 되었습니다..🙇)\n\nC: 딥러닝 쪽을 주로 공부하셨으니 파이썬 많이 쓰시죠?\n나: 음.. 네.\nC: 그럼 파이썬에서 멀티 쓰레드를 쓸 때, 내부적으로 싱글 쓰레드로 동작하는 것 알고 계신가요?\n나: 아.. 들어본 것도 같긴 한데.. 그런..가요..?\n\n많이 부끄러웠다. 당황해서 이후 멀티 쓰레드와 멀티 프로세스에 대한 대화에서 반대로 말하기도 했다. 이후 이에 대해 찾아보면서 파이썬에서 GIL(Global Interpreter Lock)을 사용한다는 것을 알게되었다. 그럼 차근히 프로그램부터 프로세스와 쓰레드의 개념부터 내가 이해하는 언어로 다시 정리해보자.\n\n프로그램(Program)\n실행 가능한 파일(Executable File in File System), 메모리에 할당되지 않은 정적 상태\n프로세스(Process)\n실행 중인 프로그램(Executing Program), 프로그램의 인스턴스(Instance of Program), 프로세스가 생성되면서 운영체제로부터 자원을 할당받음, 할당된 범위를 벗어날 수 없음\n쓰레드(Thread)\n프로세스 안에서 실질적으로 작업을 수행하는 여러 흐름, 독립적으로 스케쥴링 할 수 있는 최소 단위(The Smallest Sequence), 1개의 프로세스에는 1개 이상의 쓰레드가 존재, 프로세스 내의 쓰레드는 서로 메모리를 공유(Shared Memory)\n\n각 개념은 기본적으로 위와 같으며, 결국 쓰레드는 할당받은 메모리의 범위를 벗어날 수 없는 프로세스의 한계를 보완하기 위해 사용되는 개념이다. 일반적으로 각 프로세스에는 Code, Data, Heap, Stack의 형식의 메모리 영역이 할당되며, 쓰레드는 별도의 Stack을 다시 할당받게 되고, Code, Data, Heap을 Shared Memory로 사용하게 된다. 이렇게 쓰레드를 하나의 프로세스 안에서 두 개 이상 사용하면 동시성(Concurrency) 프로그래밍을 할 수 있어 더 빠른 속도로 작업을 처리할 수 있게 된다.(Concurrency와 Parallelism의 차이도 나중에 포스팅을 해보자.)\n그러나, 메모리를 공유한다는 것(프로세스든 쓰레드든)은 양날의 검이다. 정말 개쩌는 코드를 정확하게 작성해서 사용하면 되겠지 싶지만, 혼자 사용할 코드가 아니라면 아무리 날고 기어도 항상 문제가 발생하기 마련이다. 여기서 학부시절 운영체제 강의를 들으며 교수님께서 그렇게 강조하셨던 공유 자원에 접근하는 코드(임계 영역, Critical Section)에 여러 쓰레드가 동시에 진입하게 되는 경쟁 상태(Race Condition)라는 문제가 나온다. Race Condition이 발생하면 데이터가 불일치되는 문제로 이어지기 때문에 동기화(Synchronization) 과정을 통해 Race Condition을 방지해야만 한다. 동기화하기 위한 대표적인 조건이 Critical Section에 하나의 쓰레드만 진입할 수 있도록 하는 상호 배제(Mutual Exclusion, Mutex)이며, 이에 대한 방법 중 하나가 Lock이다.\n 가망이 없어..\n파이썬에서도 동일하게 이 논리가 적용된다. 파이썬으로 공유 메모리에 접근할 때도 동기화 과정이 필요하므로 Mutex를 활용하게 된다. 다만, 파이썬에서는 그 활용 방식이 조금 독특할 뿐이다. 모든 객체에 대한 Mutex를 두어 Thread-safe하게 만들려면 너무 복잡하고 쓰레드 간 자원을 요구하는 상태가 엉켜 교착 상태(Deadlock)가 발생할 수도 있다. 그래서 파이썬에서는 그냥 다 잠궜단다.. \n 어.....\n아무튼 그렇게 등장한 것이 바로 GIL이고, 이것이 파이썬 인터프리터에서 한 번에 한 쓰레드에서만 파이썬 바이트코드를 실행하도록 하는 장본인이다. 이로 인해, 아무리 멀티 쓰레딩을 사용한들 실질적으로는 하나의 쓰레드만 모든 자원을 점유하고 다른 쓰레드는 자원이 없으므로 실행할 수 없게 된다. 흔히 “파이썬은 사용은 쉬운데 성능이 좀..”이라고 하는데는 다양한 이유가 있지만 GIL은 아마 그 지분 중 한 축을 당당하게 차지하고 있을 것이다. 그렇지만, GIL을 사용한데도 다 이유가 있을터.. 다시 파이썬이 GIL을 사용하게 된 경위를 들여다보자. (???: 그럼 느그들이 함 해봐.)\n파이썬은 왜 사서 성능을 낮추었는가\n 업데이트 예정","link":"https://jimheo.github.io/2021/07/06/global-interpreter-lock/","title":"Python GIL(Global Interpreter Lock)이 뭐에요?","pubDate":"2021-07-06T04:57:09.000Z"}},{"node":{"contentSnippet":"바둑에 대해서 잘은 모르지만 그로부터 유래된 단어는 간혹 일상에서 사용하곤 한다.\n바둑 용어 중에 복기라는 말이 있다.\n대국이 끝나고 승패가 결정되었음에도 내용을 검토하기 위해 상호 간 처음부터 두었던 순서대로 다시 두어 재연함 뜻하는 단어다.\n복기로부터 바둑 기사들은 대국에서 잘한 수와 잘못했던 수, 상대방의 전략 등을 살펴본다.\n프로 9단 바둑기사 이창호는 다음과 같이 말할 정도로 복기를 중요시 했다고 한다.\n\n승리한 대국의 복기는 이기는 습관을 만들어주고, 패배한 대국의 복기는 이기는 준비를 만들어준다\n\n조훈현 9단 또한 복기가 가지않았던 길을 갈 수 있게 한다 하였으며, 패배로 인해 아플수록 더 예민하게 들여다보고 복기해야한다는 말을 했더랬다.\n이 바둑기사들이 많은 수를 순서대로 기억하여 복기를 할 수 있는 이유는 한 수 한 수에 집중하고 의미를 부여하여 두었기 때문이라고 한다.\n\nIT분야에서도 복기와 유사한 의미로 쓰이는 회고(Retrospective)라는 용어가 있다.\n스프린트라는 일정 주기 후에 회고를 통해 달성 지표가 얼마나 되는지, 잘한 점과 잘하지 못한 점은 무엇인지, 이를 통해 개선해야 할 점은 무엇인지를 되짚는다.\n이렇듯, 복기와 회고는 앞으로 더 나은 방향으로 가기 위해, 그리고 그에 있어 최선을 다하기 위해 필요한 과정이다.\n Burn out\n최근에 심각하게 번아웃이 왔다.\n그것은 꽤 오랜 시간 지속되었고, 지금이야 번아웃에서부터 수면 위로 많이 올라온 상태지만, 아직까지도 완전히 회복된 것은 아니다.\n이전에는 아예 가라앉아 있던 상태에서 이렇게 글까지 쓸 수 있을 정도니 장족의 발전이다.\n지금 이 글을 쓰는 이유도 번아웃에서 완전히 벗어났기에 쓰는 것이 아니라, 이로부터 극복하고자 함이다.\n번아웃으로부터 벗어나고 이전보다 더 나은 길을 가기 위해 번아웃에 대한 복기를 하고자 한다.\n사실 번아웃이 온 것을 깨닫기 전부터 이에 대한 징조는 계속해서 있었다.\n다만, 스스로를 괜찮다고 억지로 다독여오며 무리를 해왔다.\n번아웃을 이겨내지 못한 결정적 계기는 다름아닌 건강의 악화였다.\n몸에 심각하게 반응이 오자, 결국 내시경 검사를 받기로 했다.\n검사 후 수면마취로 비몽사몽한 상태에서 간호사가 나에게 다행이라고 말했다.\n다행히 별일은 없었나보다라고 생각한 순간, 간호사가 말을 이어갔다.\n\n그러니까 다행이라는 말은 지금이라도 검사를 받으셔서 다행이라는 말이에요.\n더 늦으셨으면 정말 심각해졌을 수도 있어요. 조직검사를 기다려봐야겠지만, 최악은 면한 것 같아요.\n\n순간 정신이 아득했다.\n검사를 받기는 했지만, 설마 이상이야 있겠어 하는 생각이었는데 설마가 사람잡는다.\n며칠 후, 나에게 역류성 식도염, 미란성 위염1, 십이지장궤양, 장상피화생2 등이 있다는 판정을 받았다.\n허탈감이 몰려왔다.\n열심히 달렸던 것의 결과가 건강 악화라니.\n어찌할 바를 몰랐다.\n어떤 것을 해야할지 감이 오지 않았다.3\n정신도 이미 피폐해질대로 피폐해진 상태였고, 모든 것이 만신창이였다.\n그나마 다행인 점은 여지껏 열심히 해온 덕에 대학원 연구실에는 일을 하지 않고 휴식을 하는 것으로 잘 이야기가 되었다는 것이다.\n휴식을 하면서 처음에는 정말 아무것도 하지 않았다.\n그 어떤 의욕도 나지 않았고, 할 생각도 없었다.\n조금 생각을 해보았다.\n식습관과 생활패턴의 문제도 있었겠으나, 역시 몸과 정신을 상하게한 주 원인은 스트레스였다.\n나는 나름대로의 확고한 규칙과 기준을 가지고 있으며, 누군가가 이를 흔들 때 정신적으로 많이 흔들리는 편이다.\n일과 공부를 과중하게 한 탓도 있지만, 이는 부가적인 문제였다.\n또한, 나는 나의 감정이나 정신 상태를 잘 깨닫지 못하는 편이다.\n상태의 변화가 타인의 눈에도 잘 띄지 않고, 아무도 모르는 사이에 서서히 상해가다 다 곪아서야 알아차리게 된다.\n번아웃은 시간이 지나도 나아질 기미가 보이질 않았고, 아무리 무언가 하려고 시도해봐도 이입이 되지 않고 하기가 싫어졌다.4\n거의 유일했던 취미인 여행도 갈 수 있는 상황이 아니었기에 마땅한 취미생활도 없었다.\n글을 써보려 했지만, 그럴수록 글을 쓰는 행위조차 싫어지게 되었다.\n꼭 일을 해야만 하는 어쩔 수 없는 경우에는 꾸역꾸역 해내긴 했지만, 이에 다시 스트레스를 받고 그 반동으로 번아웃이 가중되었다.\n 여행을 하며 돌아다니는 것을 좋아한다. \n 2018년 1월 1일 마추픽추.\n모든 생활 패턴을 뒤엎고 처음부터 재구성해야한다고 느꼈다.\n우선, 원체 만성적으로 앓고 있던 잔병이 많았기에 이것들을 먼저 치료하기로 했다.\n만성 질병들이 시너지로 나를 더 방해한다고 생각했다.\n또한, 위장에 대한 건강도 회복해야했기에 온갖 병원이란 병원은 다 찾아다녔다.\n비염 수술을 하고, 한약을 먹고, 도수 치료를 받았다.\n정신과에도 다니며 약을 복용하기 시작했다.\n아침형 인간이 되기 위해 일찍 일어나 조깅을 했고, 건강한 음식을 챙겨먹으며, 술과 담배, 커피를 모두 끊었다.5\n위 노력으로 인해 건강이 많이 호전되고 조금은 의욕이 생기기도 했다.\n그러나, 이는 내가 당장 관리하고 있는 상태이기 때문에 그렇게 느끼는 것이며, 조금이라도 소홀해지면 아직도 몸이 상하고 있는 것이 와닿는다.\n스스로를 많이 끌어올릴 수 있게 된 계기는 나에게 동기부여를 하기 위함이 아니라 예상 외로 여자친구에게 조언을 해주면서였다.\n한참, 입사한지 얼마 되지 않은 여자친구는 궂은 회사 일로 많이 힘들어하고 있었다.\n이와 관련해서 여자친구와 많은 대화를 했고 특히, 나는 여자친구에게 자존감에 대한 이야기를 많이 해주었다.\n여러 말이 있었지만, 최종적인 요점은 자신의 한계를 인정하더라도 스스로를 깎아내리지 말고 보듬어주어야 한다, 나마저 나를 탓하면 아무도 나를 치유할 수 없다였다.\n이 말을 하던 도중 나에게 놀라운 일이 일어났다.\n살면서 거의 느껴보지 못한 어떤 행복한 감정이 분비되었던 것이다.\n내가 여자친구에게 해준 말은 나에게도 해당되는 말이었다.\n혼자 고민할 때에는 이를 찾지 못하다가, 오히려 타인에게 조언을 함으로 나 자신에 대한 정립을 하게 되었다.\n또한 이후에, 운이 좋게도 즐겨보던 유튜브 채널인 알간지Alganzi와 1분과학에서 위와 비슷한 말을 하였고, 정립을 더 굳힐 수 있게 되었다.\n그렇게 나름대로는 해야할 일을 정립해서 하나씩 해나갈 수 있는 수준이 되고 있다.\n이전에 비해 지금의 달라진 점은 공부를 하다가 집중력이 많이 떨어지면 그냥 하지 않기로 한 것과, 나의 몸을 더 신경쓰게 되었다는 것이다.\n예전에는 어떤 확고한 목표가 있었고 이에 대한 무조건적으로 목표지향적인 삶을 추구했다면, 지금은 목표보다는 나의 삶을 우선 순위로 두고 행복을 느낄 수 있는 방법을 찾고 있다.\n여전히 목표지향적인 삶은 나에게 있어 중요하며 잃고 싶지 않다.\n하지만, 그렇다고 더 이상 무리하고 스스로를 혹사시키고 싶지도 않다.\n이 둘을 조절하며 절충하는 것은 아직까지 나의 숙제로 남아있다.\n지금 이 글을 쓰며 나의 번아웃 과정을 복기하는 것도 이 둘을 맞춰나가기 위한 방법 중 하나이다.\n내가 벼랑 끝에 내몰렸을 때, 항상 이겨내고 성장을 해왔다.\n좋다고 해야할지 나쁘다고 해야할지 잘 모르겠지만, 고통을 이겨냈을 때 더 성숙하고 견고해진다.\n나는 결국에 또 나아갈 것이다.\n\n[1] 피부, 점막의 표피가 박리되어 진피나 점막하조직이 벗겨져 노출되는 정도의 위염이다.\n[2] 위의 점막을 이루는 세포가 장에 있는 세포로 바뀌어 위의 점막이 마치 장의 점막과 유사하게 변하여 제 기능을 하지 못하는 것으로, 장상피화생은 위에 염증이 생기고 다시 회복되는 일을 반복하며 생기게 되며, 위암 발생의 위험요소이다.\n[3] 내가 겪은 번아웃의 1단계\n[4] 내가 겪은 번아웃의 2단계\n[5] 위 세가지를 모두 끊은지 반년이 조금 넘은 지금은 가끔 맥주 한 잔 정도는 하고 있다.","link":"https://jimheo.github.io/2020/07/14/retrospective-to-overcome-the-burnout/","title":"번아웃 극복을 위한 복기","pubDate":"2020-07-14T07:18:39.000Z"}},{"node":{"contentSnippet":"2020년 여름의 어느 날 저녁.\n코로나로 인해 대부분의 사람들은 마스크를 쓰고 있다.\n술이 조금 취한 듯 보이는 30대 후반 정도의 남자 두 명이 지하철에 탑승했다.\n한 명은 마스크를 턱에 반 쯤 걸쳐놓았고, 다른 한 명은 아예 쓰고 있지 않았다.\n마스크를 쓰지 않은 남자가 마스크를 걸친 남자에게 노약자석이 비었으니 노약자석에 앉자고 했다.\n\n마스크를 걸친 남자는 마스크를 쓰지 않은 남자를 만류하며 앉으면 안된다고 하였다.\n그러자, 마스크를 쓰지 않은 남자가 말했다.\n\n\"잠깐 앉는 건데 뭐 어때? 괜찮아, 그냥 앉아.\"\n\n마스크를 걸친 남자가 잠시 머뭇하더니, 결국 함께 자리에 앉았다.\n마스크를 걸친 남자는 죄책감 어린 표정이었으나, 이내 언제 그랬냐는 듯 둘은 잡담을 시작했다.\n약 세 정거장이 지난 후, 마스크를 걸친 남자는 먼저 역에서 내렸다.\n마스크를 쓰지 않는 남자는 그 후로 한참을 노약자석에 앉아있었다.\n그들이 가고 난 후, 지하철 안에는 나의 잔생각 외에는 아무 소리도 들리지 않았다.","link":"https://jimheo.github.io/2020/07/03/correlation-between-masks-and-values/","title":"마스크와 가치관의 상관관계","pubDate":"2020-07-02T15:38:28.000Z"}},{"node":{"contentSnippet":"U-Net: Convolutional Networks for Biomedical Image Segmentation\nMICCAI 2015\n\nPaper\n\nhttps://arxiv.org/pdf/1505.04597.pdf\n\nSource Code Repository\n\nOfficial Model: https://lmb.informatik.uni-freiburg.de/resources/opensource/unet/\nFramework: Caffe\n\nReproduce Model: https://github.com/jakeret/tf_unet\nFramework: Tensorflow\nPaper: https://arxiv.org/pdf/1609.09077.pdf in Astronomy and Computing 2017\nDocumentation: https://tf-unet.readthedocs.io/en/latest/installation.html\n\nReproduce Model: https://github.com/zhixuhao/unet\nFramework: Keras\n\nReproduce Model: https://github.com/milesial/Pytorch-UNet\nFramework: PyTorch\n\n\nTrain Model Code, Test Model Code\n\nTrain Model Code\nReproduce Model(Tensorflow) Supports API\nhttps://tf-unet.readthedocs.io/en/latest/usage.html\n\nReproduce Model(Keras)\nhttps://github.com/zhixuhao/unet/blob/master/trainUnet.ipynb\n\n\nTest Model Code:\nReproduce Model(Tensorflow) Supports API\nhttps://tf-unet.readthedocs.io/en/latest/usage.html\n\nReproduce Model(Keras)\nhttps://github.com/zhixuhao/unet/blob/master/trainUnet.ipynb\n\n\n\nREMARK Reproduce Model(Keras) supports data augmentation docs\n\nhttps://github.com/zhixuhao/unet/blob/master/dataPrepare.ipynb\n\nPre-Trained Model\n\nUnsupported\n\nPrediction Examples\n\nRef: https://arxiv.org/pdf/1505.04597.pdf\nCitation\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n@article{akeret2017radio,\n  title={Radio frequency interference mitigation using deep convolutional neural networks},\n  author={Akeret, Joel and Chang, Chihway and Lucchi, Aurelien and Refregier, Alexandre},\n  journal={Astronomy and Computing},\n  volume={18},\n  pages={35--39},\n  year={2017},\n  publisher={Elsevier}\n}\n\n\n\nRefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation\nCVPR 2017\n\nPaper\n\nhttps://arxiv.org/pdf/1611.06612.pdf\n\nSource Code Repository\n\nOfficial Model: https://github.com/guosheng/refinenet\nFramework: MATLAB\n\nReproduce Model: https://github.com/DrSleep/refinenet-pytorch\nFramework: PyTorch\n\nReproduce Model: https://github.com/DrSleep/light-weight-refinenet\nLight Weight RefineNet\nFramework: PyTorch\n\nReproduce Model: https://github.com/eragonruan/refinenet-image-segmentation\nFramework: Tensorflow\n\n\nTrain Model Code, Test Model Code\n\nTrain Model Code: Supported\nTest Model Code: Supported\n\nPre-Trained Model\n\nSupported\nhttps://github.com/guosheng/refinenet/blob/master/libs/matconvnet/doc/site/docs/pretrained.md\n\n\nPrediction Examples\n\nRef: https://arxiv.org/pdf/1611.06612.pdf\n\nRef: https://github.com/guosheng/refinenet\nCitation\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n@inproceedings{Lin:2017:RefineNet,\n  title = {Refine{N}et: {M}ulti-Path Refinement Networks for High-Resolution Semantic Segmentation},\n  shorttitle = {RefineNet: Multi-Path Refinement Networks},\n  booktitle = {CVPR},\n  author = {Lin, G. and Milan, A. and Shen, C. and Reid, I.},\n  month = jul,\n  year = {2017}\n}\n\n\n\nPSPNet: Pyramid Scene Parsing Network\nCVPR 2017, The Winner in 2016 ILSVRC Scene Parsing Challenge\n\nPaper\n\nhttps://arxiv.org/pdf/1612.01105.pdf\n\nSource Code Repository\n\nOfficial Model: https://github.com/hszhao/PSPNet\nFramework: Caffe\n\nReproduce Model: https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow\nFramework: Keras\n\nReproduce Model: https://github.com/hellochick/PSPNet-tensorflow\nFramework: Tensorflow\n\n\nTrain Model Code, Test Model Code\n\nTrain Model Code: Supported in Reproduce Model\nhttps://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow\n\nTest Model Code: Supported in Reproduce Model\nhttps://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow\n\n\nPre-Trained Model\n\nSupported\nOffical Model\nhttps://github.com/hszhao/PSPNet\n\n\nSupported\nReproduce Model\nhttps://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow\n\n\n\nPrediction Examples\n\nRef: https://hszhao.github.io/projects/pspnet/\nCitation\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n@inproceedings{zhao2017pspnet,\n  author = {Hengshuang Zhao and\n            Jianping Shi and\n            Xiaojuan Qi and\n            Xiaogang Wang and\n            Jiaya Jia},\n  title = {Pyramid Scene Parsing Network},\n  booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2017}\n}\n\n\n\nLarge Kernel Matters: Improve Semantic Segmentation by Global Convolutional Network\nCVPR 2017\n\nPaper\n\nhttps://arxiv.org/pdf/1612.01105.pdf\n\nSource Code Repository\n\nOfficial Model: None\n\nREMARK Large Kernel Matters를 Tensorflow로 구현한 코드 및 설명이 기술된 한국 블로그\n\nhttp://research.sualab.com/practice/2018/11/23/image-segmentation-deep-learning.html\n\nTrain Model Code, Test Model Code\n\nTrain Model Code: Unsupported\nTest Model Code: Unsupported\n\nPre-Trained Model\n\nUnsupported\n\nPrediction Examples\n\nRef: https://arxiv.org/pdf/1703.02719.pdf\n\n\nDeepLab v3(+): Atrous SeparableConvolution for Semantic Image Segmentation\nECCV 2018\n\nPaper\n\nDeepLab v3: https://arxiv.org/pdf/1706.05587.pdf\nDeepLab v3+: https://arxiv.org/pdf/1802.02611.pdf\n\nSource Code Repository\n\nOfficial Model: https://github.com/tensorflow/models/tree/master/research/deeplab\nFramework: Tensorflow\n\n\nTrain Model Code, Test Model Code\n\nTrain Model Code: Supported\nTest Model Code: Supported\n\nPre-Trained Model\n\nSupported\n\nPrediction Examples\n\nRef: https://github.com/tensorflow/models/tree/master/research/deeplab\nCitation\n\n\n1\n2\n3\n4\n5\n6\n\n@inproceedings{deeplabv3plus2018,\n  title={Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},\n  author={Liang-Chieh Chen and Yukun Zhu and George Papandreou and Florian Schroff and Hartwig Adam},\n  booktitle={ECCV},\n  year={2018}\n}\n\n\n\nInplace ABN: In-Place Activated BatchNorm for Memory-Optimized Training of DNNs\nCVPR 2018\n\nPaper\n\nhttps://arxiv.org/pdf/1712.02616v3.pdf\n\nSource Code Repository\n\nOfficial Model: https://github.com/mapillary/inplace_abn\nFramework: PyTorch\n\n\nTrain Model Code, Test Model Code\n\nTrain Model Code: Supported\nTest Model Code: Supported\n\nPre-Trained Model\n\nSupported\n\nCitation\n\n\n1\n2\n3\n4\n5\n6\n\n@inproceedings{rotabulo2017place,\n  title={In-Place Activated BatchNorm for Memory-Optimized Training of DNNs},\n  author={Rota Bul\\`o, Samuel and Porzi, Lorenzo and Kontschieder, Peter},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2018}\n}\n\n\n\nTernausNetV2: Fully Convolutional Network for Instance Segmentation\n2nd place in CVPR 2018 DeepGlobe Building Extraction Challenge\n\nPaper\n\nhttp://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Iglovikov_TernausNetV2_Fully_Convolutional_CVPR_2018_paper.pdf\n\nSource Code Repository\n\nOfficial Model: https://github.com/ternaus/TernausNetV2\nFramework: PyTorch\n\n\nTrain Model Code, Test Model Code\n\nTrain Model Code: Unsupported\nTest Model Code: Unsupported\n\nPre-Trained Model\n\nUnsupported\n\nPrediction Example\n\nRef: https://github.com/ternaus/TernausNetV2\nCitation\n\n\n1\n2\n3\n4\n5\n6\n7\n\n@InProceedings{Iglovikov_2018_CVPR_Workshops,\n     author = {Iglovikov, Vladimir and Seferbekov, Selim and Buslaev, Alexander and Shvets, Alexey},\n      title = {TernausNetV2: Fully Convolutional Network for Instance Segmentation},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n      month = {June},\n       year = {2018}\n}\n\n\n\nAutomatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning\nICMLA 2018, Wining solution and its improvement for MICCAI 2017 Robotic Instrument Segmentation Sub-Challenge\n\nPaper\n\nhttps://arxiv.org/pdf/1803.01207.pdf\n\nSource Code Repository\n\nOfficial Model: https://github.com/ternaus/TernausNetV2\nFramework: PyTorch\n\n\nTrain Model Code, Test Model Code\n\nTrain Model Code: Supported\nTest Model Code: Supported\n\nPre-Trained Model\n\nSupported in https://drive.google.com/drive/folders/13e0C4fAtJemjewYqxPtQHO6Xggk7lsKe\n\nPrediction Example\n\nRef: https://github.com/ternaus/robot-surgery-segmentation\nCitation\n\n\n1\n2\n3\n4\n5\n6\n\n@article{shvets2018automatic,\ntitle={Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning},\nauthor={Shvets, Alexey and Rakhlin, Alexander and Kalinin, Alexandr A and Iglovikov, Vladimir},\njournal={arXiv preprint arXiv:1803.01207},\nyear={2018}\n}","link":"https://jimheo.github.io/2019/03/12/segmentation-neuralnet-listup/","title":"Segmentation Neural Network List Up","pubDate":"2019-03-12T11:52:45.000Z"}},{"node":{"contentSnippet":"Complete Binary Tree\nTree에 대한 기본적인 설명은 루비콘 팀 Martin Kim의 포스트를 참조한다. 🙇\nRef: https://blog.martinwork.co.kr/theory/2018/09/22/what-is-tree.html\n추가적으로 Tree에 대해 부가설명을 덧붙인다.\nTree의 정의는 다음 2가지를 따른다.\n\n1. A root node\n2. The remaining nodes are partitioned into $n(n \\geq 0)$ disjoint sets $T_1, T_2, … , T_n$ ($T_i$ : subtrees of the root)\n\n즉, 루트 노드가 존재하며, 0개 이상의 서브트리로 분리된 노드가 존재해야 한다.\n이 중 Full Binary Tree는 Depth가 $k$일 때, 총 노드의 개수는 $2^k - 1$개인 트리이며,\nComplete Binary Tree는 Depth가 $k$인 Full Binary Tree를 왼쪽에서부터 순차적으로 읽었을 때, $k-level$에서 Complete Binary Tree의 총 노드 수 $n$의 인덱스를 가진 노드가 존재하는 것을 말한다.\n이 때, $k =\\lceil\\log_{2}(n + 1)\\rceil$이 성립한다.\nComplete Binary Tree는 마지막 레벨을 제외하고 모든 레벨이 채워져 있으며, 마지막 레벨의 모든 노드는 가능한 한 가장 왼쪽에 있기 때문에 공간의 낭비가 없어 배열로 구현하는 것이 효율적이다.\n\n\n배열로 구현 시, $i$번째 노드의 자식 노드는 각각 $2i,\\ 2i + 1$이 되며, 부모 노드는 존재 시 $\\lfloor i / 2\\rfloor$가 된다. (부모 노드의 인덱스가 $0$부터 시작할 경우에는 자식은  $2i + 1,\\ 2i + 2$, 부모는 $\\lfloor(i - 1) / 2\\rfloor$가 된다.)\nHeap의 정의와 연산\n사전적 의미에서 Heap은 무엇인가를 차곡차곡 쌓아올린 더미를 뜻한다.\nHeap은 메모리에서 동적할당을 받을 때, 정렬을 할 때, 그리고 우선순위 큐(Priority Queue)나 무손실 압축 알고리즘인 허프만 코드 등에 다양하게 사용된다.\n위에서 Tree를 설명한 이유는 Heap은 Tree 구조 중 Complete Binary Tree에서 몇가지 Constraint를 더하여 파생된 자료구조기 때문이다.\nHeap은 Max Heap과 Min Heap 두가지가 있는데, Max Heap은 Complete Binary Tree이면서 노드의 값이 자식 노드의 값보다 작지 않은 경우, Min Heap은 그 반대의 경우다.\nHeap이 Complete Binary Tree를 이용하여 구현된 이유는 삽입과 삭제의 속도 때문인데, 앞서 설명 한 Complete Binary Tree에서 부모와 자식 노드의 인덱스를 빠르게 찾을 수 있는 특성 때문이다.\n이 글에서는 Max Heap을 기준으로 설명한다.\nHeap을 C++의 Class를 사용하여 정의하면 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nclass MaxHeap{\npublic:\n    MaxHeap();\n    ~MaxHeap();\n    void Insert(int x); //Push the element\n    void Delete(); //Pop the root\n    int Top(); //Return the root\n    int Top(int n); //return the n-th rank node\n    bool isEmpty(); //Determine the heap is empty or not\nprivate:\n    vector<int> heap; //Index 0 is not used\n};\n\n\nHeap의 삽입 연산의 경우 Complete Binary Tree의 특성에 따라 처음에는 Leaf 노드의 빈 곳 중 가장 왼쪽에 새로운 노드를 추가한다.\n그 후, 삽입된 노드와 그 노드의 부모와 비교 연산을 수행하고 삽입된 노드의 값이 가장 크면 부모 노드와 자리를 바꾼다.\n그리고 옮겨진 상태에서 다시 부모와 비교, Swap을 수행하며, 부모보다 값이 크지 않다면 삽입연산이 종료된다.\n그렇지 않을 경우 계속된 과정 끝에 삽입된 노드가 루트가 되며 삽입연산이 종료된다.\n이 과정을 Bubble up(거품이 올라가는 모양과 유사하여)이라 칭한다.\n삽입 연산을 도식화 한 경우는 다음과 같다.\n\n\n\n\n삽입 연산을 구현한 코드는 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\nvoid MaxHeap::Insert(int x) {\n    if(isEmpty())\n        heap.push_back(x);\n    else{\n        heap.push_back(x);\n        int i = heap.size() - 1;\n        while(i != 1){\n            if(heap.at(i) > heap.at(i / 2))\n                swap(heap.at(i), heap.at(i / 2));\n            else\n                break;\n            i /= 2;\n        } //bubble up\n    }\n}\n\n\n삭제 연산의 경우는 별도의 인덱스를 지정하는 것이 아니라, 루트 노드에 대한 연산이다.\n루트 노드를 Pop한 후에, 가장 마지막 노드를 루트로 올린다.\n그리고 자신과 두 자식(혹은 한 자식)과 비교연산을 수행한 후 가장 큰 노드를 해당 노드와 Swap한다.\n그 후, 삽입 연산과 마찬가지로 모두 적절한 자리에 놓일 때까지 반복작업을 수행한다.\n이 과정을 Trickle Down(물방울이 떨어지는 모양과 유사하여)이라 칭한다.\n삭제 연산을 도식화 한 경우는 다음과 같다.\n\n\n\n\n\n삭제 연산을 구현한 코드는 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n\nvoid MaxHeap::Delete() {\n    if(isEmpty())\n        throw \"error\";\n\n    heap.at(1) = heap.at(heap.size() - 1);\n    heap.pop_back();\n    int i = 1;\n    int last_idx = heap.size() - 1;\n    int max_child_idx;\n    while(i * 2 <= last_idx){\n        if(i * 2 + 1 > last_idx) //if child of i is only left side\n            max_child_idx = i * 2;\n        else{\n            if(heap.at(i * 2) > heap.at(i * 2 + 1)) //compare left-child to right-child  before child to parent\n                max_child_idx = i * 2;\n            else\n                max_child_idx = i * 2 + 1;\n        }\n\n        if(heap.at(i) < heap.at(max_child_idx)) //compare child to parent\n            swap(heap.at(i), heap.at(max_child_idx));\n        else\n            break;\n        i = max_child_idx;\n    } //trickle down\n}\n\n\n삽입과 삭제는 트리의 높이만큼의 시간의 소요되므로 이는 $O(\\log_{2}n) $이다.\nHeap에 가장 중요한 연산은 삽입과 삭제 두가지이나, 추가적으로 n번째로 큰 수를 리턴해주는 Ranking Search가 가능하며, 이 경우 Heap을 이용한 정렬인 Heap Sort가 사용된다.\nHeap Sort는 힙이 비게 될 때까지, Top 연산(트리의 루트노드를 리턴)과 삭제 연산을 계속해서 수행하면서 Top 연산에서 리턴받은 값들을 차례로 새로운 배열에 저장하면 정렬이 되는 방식이다.\nHeap Sort를 수행한 후 완성된 배열의 n번째 인덱스가 곧 힙에서 n번째로 큰 수를 의미한다.\nRanking Search에 대한 코드는 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n//using heapsort\nint MaxHeap::Top(int n) {\n    if(heap.size() - 1 < n)\n        throw \"error\";\n\n    vector<int> backup = heap;\n    vector<int> sorted_heap;\n    sorted_heap.push_back(0);\n    int top;\n    while(!isEmpty()){\n        top = Top();\n        Delete();\n        sorted_heap.push_back(top);\n    }\n    heap.swap(backup);\n\n    return sorted_heap.at(n);\n}\n\n\n사실 위에 while문을 n번만큼만 돌아도 Ranking Search가 가능하지만, Heap Sort의 설명을 곁들이기 위해서 모든 노드를 비웠다.\nHeap Sort의 경우에는 $O(n\\log_{2}n)$의 수행시간을 보인다.","link":"https://jimheo.github.io/2018/10/12/about-heap/","title":"자료구조, 힙(Heap)","pubDate":"2018-10-12T08:10:13.000Z"}},{"node":{"contentSnippet":"자료구조란?\n자료구조를 공부하는 것은 알고리즘을 해결하는데 뿐만 아니라, 라이브러리나 프레임워크를 사용할 때 단순히 가져다 쓰지 않고, 내부적으로 어떻게 구현되있는지에 대한 이해도가 향상되므로 라이브러리에 있는 적절한 자료형을 사용할 수 있다.\nOOP와 자료구조\n자료구조란 위키백과의 정의에 따르면 자료를 효율적으로 이용할 수 있도록 컴퓨터에 저장하는 방법이다.\n즉, 어떠한 알고리즘을 구현하는데 있어 그에 맞는 자료구조를 설계하여 사용해야 시간적으로 혹은 공간적으로 자원을 최소화 할 수 있다.\n사실, 위의 말은 이제 프로그래밍을 시작하고 자료구조라는 단어를 처음 듣는 사람들에게는 잘 와닿지 않는다.\n조금 더 구체적으로 말하자면, 모든 프로그래밍 언어들에는 int, char, boolean 등의 가장 기본적인(primitive) 자료형이 존재한다.\n그러나 primitive 자료형으로만 코딩을 했을 때, 재사용성이 떨어지고 제한적이다.\n그렇기 때문에 주로 사용자 자료형, 다시 말해 추상 자료형(Abstract Data Type, ADT)를 정의하여 사용하는 것이 바람직하며, ADT가 곧 자료구조라고 할 수 있다.\n\n물론, 배열이나 리스트 등 대표적인 자료구조는 (특히, 상위 레벨 언어에서는) 이미 예약어로 존재하지만, int와 같이 직접적으로 메모리에 할당되는 것이 아니라 언어의 버전이 올라갈수록 사용자의 편의를 위해서 구현한 것이므로 primitive로 분류하지 않는다.\nADT는 Object Oriented Design에서 Class와 사실상 거의 동일하므로, 클래스를 이용하여 구현하는 것이 효율적이다.\n클래스처럼, ADT는 attribute와 operation을 가지고 있으며, OOD의 Encapsulation을 따라서, attribute는 private로 제한하고 모든 수행은 operation을 통해서 한다.\nUML의 창시자인 Grady Booch에 따르면 OOP의 다음 3가지를 충족해야 한다.\n\n객체는 기본적인 블록 단위로 구성되어 있다.\n각 객체는 클래스의 인스턴스이다.\n클래스는 상속을 지원한다.\n\nC++이나 Java 등 대표적인 객체지향언어는 위 정의에 따라서 프로그래밍이 가능하다.\n따라서, 객체지향언어는 primitive 자료형도 클래스로 구현되어 있다.\n그렇기 때문에 아래의 두 문장은 동일하다.\n\n\n1\n2\n\nint a = 10;\nint a(10);\n\n\n대표적인 자료구조형\n\n위 그림에 있는 자료형이 자료구조의 가장 대표적인 자료형이다.\n자료구조의 operation은 내부적으로 primitive 자료형에 의해서 수행되며, 결국 operation도 일련의 알고리즘이므로, 자료구조와 알고리즘은 밀접한 관계를 가진다.\nC++에서는 C++11부터 Standard Template Library(STL)에는 동적 배열인 Vector부터 List, Stack, Queue 등의 대표적인 자료구조가 구현되어 있다.\n이 라이브러리의 내부 연산을 직접 보는 것도 자료구조를 공부하는데 효과적인 방법이다.","link":"https://jimheo.github.io/2018/10/12/concept-of-data-structure/","title":"Concept of Data Structure","pubDate":"2018-10-12T03:52:35.000Z"}},{"node":{"contentSnippet":"Multi Layer Perceptron의 문제점\n해를 거듭할 수록, 정형의 데이터보다는 비정형 데이터가 늘어나는 추세다. 비정형의 데이터를 기존의 MLP로 처리하는 것은 몇가지 단점이 존재한다.\n이미지를 예로 들어보자. 이미지는 픽셀단위로 이루어진 행렬이며, 일반적인 이미지의 해상도는 최소 64x64부터 시작하는 경우가 많다.\nGrayscale이 아닌 경우에는 채널값을 추가적으로 고려해야 한다. 그렇기 때문에, MLP로 64x64의 컬러 이미지를 처리할 경우에 12,288개의 인풋이 들어가게 된다.\n이에 따라, 기존의 MLP에서 Neural Net의 크기는 더 커지게 되며, 학습시간이 굉장히 늘어난다는 단점이 있다. 또한, 이미지의 회전이나 크기 확장 및 축소, 이동과 같은 변환이 일어날 때, 이에 대한 새로운 학습이 필요하다.\n그렇기 때문에, 비정형 데이터는 각각의 특성을 고려한 발전된 Neural Net이 필요하다. 비정형 데이터를 효과적으로 학습하기 위해 등장한 것이 CNN, RNN이다. CNN은 이미지에 특화된 Neural Net이며, RNN은 음성이나 자연어와 같은 Sequential한 모델에 특화된 Neural Net이다.\nConvolution Neural Network의 등장\n\nCNN의 위의 연구에서 부터 시작한다.\n실험 결과, 고양이가 특정 사물을 볼 때, 고양이는 사물의 전체를 한번에 보지 않고 부분적으로 나뉘어 관찰하며, 이 때 고양이의 뇌의 뉴런은 전체가 활성화 되지 않고 부분의 특징마다 서로 다른 시냅스를 통해 일부만 활성화 된다고 한다.\n이와 동일하게 CNN은 이미지의 부분별로 Filter를 사용하여 인식을 한다.\n\n위 그림은 CNN의 일반적인 구조를 시각화한 것이다. CNN은 여러층의 Convolution Layer, Activation Fuction (ReLU 혹은 그와 유사한)이 먼저 구성되며, Pooling Layer를 통해 이미지를 Sampling하게 된다. 이 층들이 몇 번 반복된 후에는 Fully-Connected Layer로 이동 하는데 Fully-Connected Layer는 우리가 앞서 보았던 MLP와 동일하다. Fully-Connected에서는 최종적으로 확률이 결과물로 나와야 하므로 Softmax를 사용한다.\nConvolution Layer는 영상처리분야에서 필수 요소로 작용하는 Convolution을 기반으로 한 Layer이다. Convolution은 신호처리에서 먼저 등장한 개념이지만, 영상처리에서 이산적인 Convolution 연산을 통해 우리가 카메라 어플 등에서 흔히 보는 Filtering 부터 Edge Detection 등의 작업을 수행할 수 있다.\n이미지의 Convolution\nConvolution, 한글로 번역하면 합성곱이라는 뜻이다. Convolution 연산은 연산을 하고자 하는 픽셀의 주변 픽셀들이 출력에 영향을 미치는 것이다. 다시 말하면, 픽셀 자신과 주변 픽셀들의 Weighted Sum이다.\n\n위 그림은 Convolution의 가장 적절한 시각화 예시 중 하나이다. 그림을 보면, Source Pixel과 주변의 픽셀들을 특정한 행렬과 Element-Wise 곱을 한 후 모두 더해 출력을 낸다.\nSource와 곱을 하는 특정 행렬을 Filter(혹은 Convolution Matrix, Kernel, Window)라 부른다. 이 Filter를 Source의 Pixel들에 순차적으로 Sweeping 하면서 계산하여 하나의 Output Matrix를 만드는 것이다. Filter의 크기는 사용자가 지정하며, 일반적으로 3x3 혹은 5x5 Filter를 사용한다.\n위를 수식으로 일반화하면 다음과 같다.\n$$h[i, j] = f[i, j] * g[i, j] = \\sum_{k=1}^n \\sum_{l=1}^m f[k, l] g[i - k, j - l]$$\n영상처리에서 Filter 내부의 값은 사용자가 직접 설계하며, 이때 값의 총 합은 주로 1이 되도록 하며, Edge Detection의 경우 0이 되도록 한다.\n그러나, 딥러닝에서는 컴퓨터가 학습을 통하여 Filter의 값들을 획득한다. 즉, Filter는 MLP에서의 가중치와 동일한 개념이다.\n다시 위 그림을 보자. 위 그림은 한번의 Convolution 연산이 끝난 상태이다. 그렇다면, 그 다음의 Convolution 연산을 위해 Filter의 이동이 필요하다. 여기서 Filter를 몇 칸 이동할 것인가를 Stride라고 한다. Stride를 몇으로 설정하는가에 따라 출력데이터의 크기가 달라지는데 출력크기는 입력크기가 NxN이고 필터크기가 FxF 일 때, (N - F) / Stride + 1이 된다.\n또한, 그림에서 Source는 7x7의 Matrix이나 출력물은 5x5가 나올 것으로 보인다. 이는 Border 부분의 Pixel은 계산이 안되기 때문이다. 이렇게 되면, Convolution Layer를 한번 거칠 때마다 이미지가 Down Sampling 되어 Layer가 깊어질 수록 데이터가 소실되는 문제가 발생한다.\n그래서 Border에 Padding을 생성하여 Down Sampling을 방지한다. Padding의 값은 주로 0으로 설정한다.\n이렇게 Convolution을 거쳐 나온 결과물을 Channel이라 한다. 즉, 1개의 필터 당 1개의 채널이 생성되며, 최종 결과물은 각각 다른 가중치의 여러개의 필터로 여러 채널을 생성하여 결과물을 두껍게 만든다. 이렇게 최종적으로 생성된 것을 Activation Map이라 부른다.\n이 과정까지가 Convolution Layer 층에서 하는 역할이며, 그 후 ReLU 등의 Activation Fuction을 거친다. 여러개의 Convolution Layer를 구성하여 이 과정들을 반복한다.\nPooling Layer\n여러 번의 Convoltion Layer를 거치는 중간중간에 Pooling Layer를 한번 씩 넣어 주는데 Pooling은 Sampling, Resizing과 같은 말이다. Pooling을 통해 Resolution을 작게 만들면서 선명했던 이미지의 형체를 점점 뭉뚱그리면 이미지의 Object 들을 더 쉽게 그룹지을 수 있다.\nPooling 기법은 여러가지가 존재하며, 대표적인 기법은 Max Pooling이다. 여러개의 값 중에서 가장 큰 값들로 구성하는 기법이다. 예를 들어, 9x9 행렬은 9개의 3x3 행렬로 쪼갤 수 있다. 3x3 행렬 각각의 element들 중 가장 큰 값을 대푯값으로 내놓는 것이다. 그렇게되면 Pooling을 거친 9x9 행렬은 하나의 3x3 행렬이 된다. 9x9 행렬이 Pooling을 거치면 항상 3x3이 되는 것은 아니고 여기서도 Stride를 설정하여 출력행렬의 크기를 정할 수 있다. Pooling은 그 외에도 최솟값이나 표준편차 등을 사용하는 기법이 존재한다. 핵심은 어떤 한 값을 대푯값으로 정하는 것이다.\nCNN의 가장 대표적인 예시로는 LeNet부터 AlexNet, GoogLeNet 혹은 최근에는 YOLO등이 있으며, 성킴 교수님의 블로그를 참조한다.\nRef: http://pythonkim.tistory.com/54?category=573319","link":"https://jimheo.github.io/2018/10/10/concept-of-cnn/","title":"Convolution Neural Network의 개념","pubDate":"2018-10-10T08:55:40.000Z"}},{"node":{"contentSnippet":"Chain Rule\nDeep Neural Network에서는 Feed Forward를 하여 나온 값과 기존의 정답 레이블 사이의 에러율을 줄이기 위한 가장 기본적인 방법으로 Gradient Descent 알고리즘을 사용한다.\nGradient Descent 알고리즘은 이를 위해서 Back Propagation을 하면서 최적의 가중치를 구하도록 한다.\n최적의 가중치는 기존의 가중치가 에러율에 미치는 영향을 구한 후 이 영향을 점차 감소시켜서 구할 수 있다.\n즉, 이 과정에서 편미분이 사용되는 것이다. 편미분은 여러개의 파라미터가 있을 때, 하나의 파라미터가 약간 변화하면 Output이 얼마나 크게 변화하는지 확인할 수 있는 도구이다.\n그러나, 딥러닝은 Multi-Layer로 구성되어 있다. 수많은 다중 함수들의 집합이라고 봐도 무방하다.\n다시 말해, 가장 처음의 입력에 대한 가중치를 업데이트 해야 하는데 이 가중치의 편미분값을 구하기가 어렵다는 것이다.\n이때 도입되는 개념이 Chain Rule이다. 정확히는 도입이 되었다기 보다는 수학적인 측면에서 보았을 때, 편미분에 당연하게 사용되는 것이다.\n초심자의 입장\n딥러닝을 처음 접하는 혹은 수학에 대하여 큰 지식이 있지 않은 사람의 입장에서 Chain Rule은 그리 간단하지는 않다.\n개념에 대해 이해를 하고 난 후라도 Multi-Layer에 대해 직접 Chain Rule을 적용한다면 굉장히 헷갈리기 시작한다.\n그래서 가장 단순하게 이해를 하기 위해서 Sigmoid 함수 하나만을 가지고 Chain Rule을 적용해보기로 한다.\n$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n위 수식이 Sigmoid 함수다. 미분 공식을 어느정도 안다고 하더라도 위 식을 미분한 결과를 내는 것은 쉽지 않을 것이다.\n그렇다면 위의 식을 잘게 쪼개보자.\n\n$$\n\\begin{cases}\nf(z) = -z \\cr\ng(f) = e^f \\cr\nh(g) = 1 + g \\cr\ni(h) = \\frac{1}{h}\n\\end{cases}\n$$\n위 4개의 수식으로 잘게 나눈다면,\n$$\\sigma(z) = i(h(g(f(z))))$$\n가 성립된다.\n여기서 $\\sigma(z)$을 미분한다는 것은 $z$의 변화량을 확인하는 것이다.\n우변의 측면에서 이를 다시보면 다음과 같이 표현이 가능하다.\n$$\n\\frac{\\partial \\sigma}{\\partial z} = \\frac{\\partial i}{\\partial z} = \\frac{\\partial i}{\\bcancel{\\partial h}} * \\frac{\\bcancel{\\partial h}}{\\bcancel{\\partial g}} * \\frac{\\bcancel{\\partial g}}{\\bcancel{\\partial f}} * \\frac{\\bcancel{\\partial f}}{\\partial z}\n$$\n즉, $\\frac{\\partial i}{\\partial h} * \\frac{\\partial h}{\\partial g} * \\frac{\\partial g}{\\partial f} * \\frac{\\partial f}{\\partial z} = f^\\prime(z) * g^\\prime(f) * h^\\prime(g) * i^\\prime(h)$ 이므로,\n4개의 함수에 대한 미분을 각각 한 후에 서로 곱해주기만 하면, Sigmoid 함수의 미분함수가 나온다.\n4개의 함수를 미분하면 아래와 같다.\n$$\n\\begin{cases}\nf^\\prime(z)=-1 \\cr\ng^\\prime(f)=e^f \\cr\nh^\\prime(g)=1 \\cr\ni^\\prime(h)=-\\frac{1}{h^2}\n\\end{cases}\n$$\n이제 이들을 곱해준다.\n$$\n\\begin{matrix}\nf^\\prime(z)g^\\prime(f)h^\\prime(g)i^\\prime(h) &=& (-1) * e^f * 1 * (-\\frac{1}{h^2}) \\cr\n&=& \\frac{e^f}{h^2} \\cr\n&=& \\frac{e^{-z}}{(1+e^{-z})^2} \\cr\n&=& \\sigma(z) * (1-\\sigma(z))\n\\end{matrix}\n$$\n최종적으로 Sigmoid 함수를 Chain Rule을 이용한 값은 위와 같은 결과가 나오는데 이는 일반적으로 알려진 Sigmoid 함수의 미분 값과 동일하다.\n이처럼 복잡한 다중함수의 미분은 Chain Rule을 이용하면 가장 단순하게 풀이가 가능하다.\nBack Propagation의 과정 또한 Chain Rule을 적용할 수 있다.","link":"https://jimheo.github.io/2018/10/10/extra-consideration-of-chain-rule/","title":"Chain Rule에 대한 추가적인 고찰","pubDate":"2018-10-10T08:54:42.000Z"}},{"node":{"contentSnippet":"해당 내용은 Andrew Ng의 Deeplearning ai 강의 C1W2까지에 기반을 두고 있습니다.\n\nIntroduction\n머신러닝이란 무엇인가?\n어떤 알고리즘을 혹은 시스템을 구축할 때 가장 추상적으로 생각할 수 있는 그림은 다음과 같다.\n\nRef: https://arbs.nzcer.org.nz/algebraic-patterns-and-relationships-different-types-numbers\n일반적인 구현에서는 위 그림의 Rule 부분을 개발자들이 직접 코드를 작성한다.\n그러나 머신러닝은 Input과 Output 혹은 Input만 가지고 여러 방법론에 의하여 컴퓨터에게 학습을 시킨다.\n그렇게 되면, 컴퓨터는 학습을 통해 Rule을 작성하게 된다.\n그 후 다음의 Input이 들어왔을 경우 작성한 Rule을 통해 Output을 예측한다.\n이는 기존에 사용하던 방법론과는 약간 상반되는 느낌이 있다.\n또한, 개발자들이 작성한 Rule은 코드를 볼 수 있기 때문에 틀린 부분을 직접 수정하거나 삭제할 수 있다.\n그러나 컴퓨터가 작성한 Rule은 사람이 볼 수 없는 코드이므로 그에 대한 직접적인 수정이 불가하다.\n이것이 머신러닝을 흔히 블랙박스라 칭하는 이유다.\n\nHouse Price Prediction\nData Mining은 주어진 Dataset으로 현재 데이터들의 상관관계를 분석하는 것이다.\n여기서 더 나아간 것이 Machine Learning이며, 머신러닝은 분석된 상관관계를 토대로 앞으로 입력될 데이터에 대한 출력을 예측하는 것이다.\n\nRef: deeplearning.ai Andrew Ng\n집의 크기로 집값을 예측한다고 가정해보자.\n집의 크기라는 파라미터에 대한 집값이라는 리턴값은 하나의 점으로 매칭이 가능하다.\n수많은 데이터들이 있을 경우 이 점들은 결국 하나의 직선 혹은 곡선으로 수렴할 수 있다.\n집의 크기라는 입력과 집값이라는 출력사이에는 하나의 Rule이 있을 것이다.\n머신러닝에서는, 특히 Neural Network는 사람의 신경망을 모티브로 만든 방법론이므로 이를 neuron 혹은 perceptron이라 부른다.\n그러나 위의 경우, 혹은 대다수의 상황에서, 하나의 변수만으로 집값을 예측한다는 것은 사실 불가능하다.\n집의 위치, 방의 개수, 땅값 등등 집값을 예측하는 데에는 많은 변수가 필요하다.\n즉, 뉴럴 네트워크는 수많은 뉴런들을 나누어 계산하고 그 결과를 다시 새로운 층에 두어 (한 층을 계산해서 나온 출력들은 다시 하나의 변수가 되므로) 마지막 출력이 나올 때까지 계산하는 방식이 되어야 한다.\n이처럼 층이 여러개인 뉴럴 네트워크를 DNN (Deep Neural Network)이라 말하며 이것이 사람들에게 흔히 알려진 Deep Learning이다.\n\nTypes of Machine Learning\n머신러닝은 구조적으로 크게 3가지 방식이 있다.\n\nRef: https://www.datasciencecentral.com/profiles/blogs/types-of-machine-learning-algorithms-in-one-picture\nSupervised Learning (지도 학습)\n지도 학습은 입력과 정답인 출력 레이블 한 쌍을 가지고 학습시키는 방법이다.\n지도 학습은 다시 Classification과 Regression으로 분류된다.\nUnsupervised Learning (비지도 학습)\n비지도 학습은 입력 레이블만을 가지고 학습시키는 방법이다.\n지도 학습의 경우에는 정답이 존재하기 때문에 학습 결과와 정답과의 오차를 계산할 수 있으나 비지도 학습은 그렇지 않다.\n그렇기 때문에 비지도 학습은 입력 데이터들의 Cluster를 찾아 학습한다.\nReinforcement Learning (강화 학습)\n강화 학습은 학습 데이터가 주어지지 않는다. 대신에 학습에 대한 보상(Reward)가 주어지며, 미래에 얻을 보상값들의 평균을 마르코프 의사 결정 과정을 통해 최대로 하는 함수를 찾는 것을 강화 학습이라 한다.\n\nBinary Classification\nClassification과 Regression의 가장 큰 차이점은 Classification은 좌표상에서 Discrete하게 표현된다는 것이며, Regression은 Continuous하게 표현된다는 것이다.\n즉, Classification은 Cluster를 나누는 기준 선을 찾는 방법이며, Regression은 데이터들이 수렴하게 되는 선을 찾는 것이다.\nClassification 중에서 Binary Classification은 정답이 0 혹은 1 두가지로만 구분하는 방법이다. 예를 들어, 64x64의 고양이 사진이 있을 때, 이것이 고양이인지 아닌지만 판별하게 된다.\n이를 판별하기 위해서는 64x64인 행렬이 RGB 3채널로 이루어진 사진을 하나의 열벡터로 재배열해야 한다.\n이때, 열벡터의 행의 개수는 $64\\times64\\times3=12288$개가 된다. (이를 $n_x$라 표현)\n이 열벡터는 정답레이블에 맞게 변환하는 과정이 필요하다.\n선형대수에 따르면 일반적으로 벡터의 변환은 Linear Combination ($v=\\alpha_1v_1+\\alpha_2v_2+…+\\alpha_nv_n$)이며, 벡터에 가중치를 곱한 것들의 합이다.\n벡터의 곱은 행렬의 곱으로 표현이 가능하며, 수식은 아래와 같다.\n$$z = w^Tx + b$$\n그러나 계산되어 나온 $z$는 $0$이나 $1$이 아니다.\n그렇기 때문에 Logistic Regression (여기서는 Sigmoid Function)을 사용하여 $0$과 $1$에 근접하게 맵핑하여야 한다.\n시그모이드 함수의 범위는 $0<\\sigma(z)<1$의 연속된 실수이며, 이것은 확률을 의미한다.\n\nRef: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n$\\hat{y}=\\sigma(z)$이라 하면 $\\hat{y}$는 $y$가 $1$인 경우에 대한 확률이며 ($\\hat{y}=P(y=1 \\mid x)$), 결론적으로 기계는 $\\hat{y}$가 $1$에 근접할 수록 고양이라고 판단하는 것이다.\nBack Propagation\n앞의 과정까지가 Forward Propagation으로 진행되는 방식이다.\n그러나 기계가 도출한 답과 실제 정답레이블과의 오차가 클 경우가 많다.\n그렇기 때문에 우리는 Forward하게 도출한 답을 토대로 다시 가중치를 수정하면서 오차율을 줄여야 할 필요가 있다.\n이를 역전파(Back Propagation)라 한다.\nLogistic Regression Cost Function\n역전파를 하기 위해 도출된 답과 실제의 답에 대한 오차율을 구해야 한다.\n하나에 학습에 대한 오차율 계산 함수를 Loss Function이라 한다.\nLoss function을 구하는 과정은 다음과 같다.\n\n$$\n\\begin{aligned}\n&if \\cr\n&&&y = 1: P(y \\mid x)=\\hat{y} \\cr\n&else \\cr\n&&&y = 0: P(y \\mid x)=1 - \\hat{y}\n\\end{aligned}\n$$\n\n즉, $P(y \\mid x)=\\hat{y}^y(1 - \\hat{y})^{(1 - y)}$가 성립하며, 이에 $\\log$를 씌우면,\n\n$$\n\\log P(y \\mid x) = \\log\\hat{y}^y(1 - \\hat{y})^{(1 - y)}\n$$\n$$\n-\\mathcal{L}(\\hat{y}, y) = y\\log\\hat{y} + (1 - y)\\log(1 - \\hat{y})\n$$\n이 성립한다.\nCost Function은 각 학습 셋에 대한 Loss Function들의 평균이다. 즉, 우리는 Cost Function을 구한 후 역전파 과정을 거쳐야 한다.\n$$ J(w, b) = -\\frac{1}{m}\\sum_{i = 1}^m (y_{(i)} \\log\\hat{y_{(i)}} + (1 - y_{(i)}) \\log(1 - \\hat{y_{(i)}}))$$\nGradient Descent Algorithm\nGradient Descent는 Cost Function을 최소화하기 위한 가중치를 찾는 알고리즘이다.\n이 공식에는 편미분을 이용한다.\n미분은 입력에 약간의 변화가 있을 때, 출력이 얼마만큼 변화하는지 확인하기 위함이다.\n즉, $f(x)=2x$에 대해서 $x_1=1$일 때, $f(x_1)=2$이며, $x_2=1.001$일 때, $f(x_2)=2.002$이다.\n이 경우 $f(x)$의 도함수 $f^\\prime(x)$는 $\\frac{0.002}{0.001}$이므로 $2$가된다.\n편미분은 미분의 일종으로 입력 파라미터가 하나가 아닌 여러개일 때, 한 파라미터가 약간 변했을 때 출력이 얼마만큼 변화하는 지를 나타내는 것이다.\n위에서는 $J(w,b)$를 $w$에서 편미분하는 것을 $\\frac{\\partial J(w,b)}{\\partial w}$로 표현하며, 마찬가지로 $b$에 대해 편미분하면 $\\frac{\\partial J(w,b)}{\\partial b}$로 표현한다.\nGradient Descent는 볼록한(convex) 함수가 있을 때 이 함수의 가장 아래 부분으로 점차 결과물을 내리는 방식이다.\n오차율 함수 $J(w,b)$는 하단으로 갈수록 오차율이 적어지는 것을 의미한다.\n$$\n\\begin{aligned}\nw:=w-\\alpha\\frac{\\partial J(w,b)}{\\partial w} \\\nb:=b-\\alpha\\frac{\\partial J(w,b)}{\\partial b} \\\n\\end{aligned}\n$$\n위 Gradient Decsent를 각 $w,b$에 적용하여 가장 낮은 오차율이 나오도록 $w$와 $b$를 수정하는 작업이다. ($\\alpha$는 learning rate을 의미)\n추가적으로 Linear Regression에서의 Cost Function은 Euclidean Distance를 응용한 Mean Squared Error\n$$J(w,b)=\\frac{1}{m}\\sum_{i=1}^m\\frac{1}{2}(\\hat{y}-y)^2$$\n를 사용하지만, Logisitc Regression에서 MSE를 사용하면 Convex하지 않은 함수가 나온다. 그렇기 때문에 위와 같은 방법을 사용하는데 이를 Cross Entropy Error라고 한다.\nBack Propagation 수행 시에는 Gradient Descent를 위한 편미분을 계산하게 되는데 위의 Binary Classification 예제는 단층 레이어 구조의 뉴럴 네트워크를 사용하기 때문에 편미분 유도가 용이하다.\n그러나 Deep Learning은 Multi Layer 구조이므로 편미분을 구할 때 다소 복잡하다.\n그렇기 때문에 각 층의 출력에 대해 한 단계씩 뒤로 가면서 각각의 편미분을 구하는 일을 반복하게 된다. 이를 Chain Rule이라 한다.","link":"https://jimheo.github.io/2018/10/10/ml-study-week1/","title":"Machine Learning study 1주차","pubDate":"2018-10-10T05:46:13.000Z"}}]},"allFeedMartinHBlog":{"edges":[{"node":{"contentSnippet":"GPU is lost\n최근 셋팅한 제 회사 컴퓨터에는 RTX-3090이 장착되어있습니다. RTX-3090을 장착한 김에 딥러닝 모델의 Hyperparameter search를 제 컴퓨터에서 수행하였는데요. 매번 프로그램을 실행할 때마다 GPU is lost라는 에러를 출력하고 프로그램이 중단되곤했습니다. 이 에러가 나오면 더 이상 OS에서 GPU에 접근할 수 없기 때문에 매번 OS를 재부팅해야하는 번거로움이 있었습니다.\n\n\n1\n2\n3\n4\n5\n\n$ python3 train.py\n...\n(중략)\n...\nUnable to determine the device handle for GPU 0000:05:00.0: GPU is lost. Reboot the system to recover this GPU\n\n\nWhat is problem?\n이 에러를 마주하고나서는 제가 세운 가설은 다음과 같았습니다.\n\n파워 서플라이의 용량이 컴퓨터의 전력 수요를 맞출 수 없다. 혹은 GPU의 발열이 너무 심하다.\nGPU가 PCI 슬롯에 제대로 장착되지 않았다.\n하드웨어 호환성 혹은 하드웨어에 문제가 있을 것이다.\n\nInsufficient power\n컴퓨터에 장착된 파워 서플라이의 용량이 GPU가 장착된 컴퓨터의 전력 수요를 맞출 수 없다면, nvidia-smi에서 GPU가 사용하는 전력량을 제한할 수 있습니다. \nGPU의 발열이 심한 경우에도 GPU가 사용하는 전력량을 제한하여 온도상승을 제한할 수 있습니다.\nGPU가 사용할 수 있는 전력량을 제한하는 명령어는 아래와 같습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n# -pm --persistence-mode=\n# Set persistence mode: 0/DISABLED, 1/ENABLED\n$ sudo nvidia-smi -pm 1\n>>\nEnabled persistence mode for GPU 00000000:02:00.0.\nAll done.\n\n\n# -pl --power-limit\n# Specifies maximum power management limit in watts.\n$ sudo nvidia-smi -pl 290\n>>\nPower limit for GPU 00000000:02:00.0 was set to 290.00 W from 390.00 W.\nAll done.\n\n\n먼저 nvidia-smi 명령어를 통해서 nvidia 커널 모듈이 지정된 GPU에 대해서 항상 활성화 상태가 되도록 강제합니다. nvidia 커널 모듈은 그래픽 GPU가 작동하기 위해 필요한데, X윈도우 상태이거나 CUDA 애플리케이션이 작동할 때에 커널 모듈이 활성화됩니다. X윈도우가 종료되거나 CUDA 애플리케이션이 더 이상 작동하지 않는 경우에는 커널 모듈이 비활성화 상태가 됩니다.\nnvidia-smi 명령어를 통한 전력제한 옵션은 nvidia 커널 모듈이 활성화되어있을 때만 적용됩니다. 즉, nvidia 커널 모듈이 활성화된 상태에서 전력제한이 적용되지만 비활성화된 상태에서는 전력제한이 풀려서 원래의 값으로 복원되게 됩니다. 전력제한이 동적으로 적용되는 것을 방지하기 위해서 persistence mode를 적용하여 nvidia 커널 모듈을 활성합니다.\nRTX-3090에 전력제한을 하고, GPU와 CPU의 온도를 모니터링하면서 같은 에러가 발생하는지 테스트하였습니다.\nGPU가 허용하는 온도는 아래의 명령어를 확인할 수 있습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n$ nvidia-smi -q | grep -i temp\n>>\n    Temperature\n        GPU Current Temp                  : 67 C\n        GPU Shutdown Temp                 : 98 C\n        GPU Slowdown Temp                 : 95 C\n        GPU Max Operating Temp            : 93 C\n        GPU Target Temperature            : 83 C\n        Memory Current Temp               : N/A\n        Memory Max Operating Temp         : N/A\n\n\nGPU가 Shutdown되는 온도는 98도임을 확인할 수 있고, 권장 최대 온도는 83도임을 확인할 수 있습니다.\nCPU 의 온도를 모니터링하기 위해서는 lm-sensor를 이용하였습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n\n$ watch -n 0.1 sensor\n>>\nEvery 0.1s: sensors                                 ketimartin3090: Fri May 21 23:39:24 2021\n\niwlwifi_1-virtual-0\nAdapter: Virtual device\ntemp1:            N/A\n\npch_cometlake-virtual-0\nAdapter: Virtual device\ntemp1:        +46.0°C\n\nacpitz-acpi-0\nAdapter: ACPI interface\ntemp1:        +16.8°C  (crit = +20.8°C)\ntemp2:        +27.8°C  (crit = +119.0°C)\n\ncoretemp-isa-0000\nAdapter: ISA adapter\nPackage id 0:  +65.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 0:        +49.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 1:        +47.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 2:        +65.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 3:        +45.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 4:        +47.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 5:        +47.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 6:        +43.0°C  (high = +100.0°C, crit = +100.0°C)\nCore 7:        +45.0°C  (high = +100.0°C, crit = +100.0°C)\n\nnvme-pci-0400\nAdapter: PCI adapter\nComposite:    +54.9°C  (low  = -273.1°C, high = +84.8°C)\n                       (crit = +84.8°C)\nSensor 1:     +54.9°C  (low  = -273.1°C, high = +65261.8°C)\nSensor 2:     +78.8°C  (low  = -273.1°C, high = +65261.8°C)\n\n\n학습을 돌려놓고, 확인해본 결과 전력제한을 걸어 전력소모와 발열량을 줄였음에도 같은 에러가 발생하는 것을 확인하였습니다.\nGPU inserted not properly in PCIE slot\nRTX-3090은 지금까지 나왔던 GPU 중에 제일 크고 무게가 있는 GPU입니다. 메인보드가 세로로 세워져있는 컴퓨터 케이스에 RTX-3090을 장착하게되면 PCIE slot에 장착된 GPU가 무게 때문에 내려앉습니다. 이로 인해 GPU가 잘 장착되지 않거나 메인보드에 PCIE 슬롯이 망가질 수 있습니다. 이를 방지하기 위해서 GPU 지지대를 사용하고있습니다만, 혹시 모르는 마음에 GPU를 탈착하고 재 장착하였습니다.\nGPU를 재장착하고 테스트한 결과 같은 에러가 발생하는 것을 확인할 수 있었습니다.\nHardware compatibility\n문제에 대해서 계속 검색하다가 리눅스 부팅 메세지를 확인하는 것을 통해서 GPU의 상태를 추적할 수 있다는 사실을 알았습니다. 부팅메세지는 dmesg명령어를 통해서 확인할 수 있었는데, 여기서 몇가지 키워드들을 확인할 수 있었습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n$ dmesg\n...\n(중략)\n[ 189.427290] NVRM: Xid (PCI:0000:01:00): 79, GPU has fallen off the bus.\n...\n(중략)\nPCIE Bus Error: Severity=Corrected after booting into Ubuntu\n...\n\n\n저는 이 키워드들에 집중했고, 모종의 하드웨어 호환 이슈로 인해 발생하는 PCIE Bus Error의 문제는 매우 흔하고, 이를 kernel parameter를 수정하여 해결할 수 있다는 것을 확인하였습니다. \n우분투는 /etc/default/grub에서 이를 설정할 수 있습니다. 해당 파일을 열어 아래의 내용을 추가해줍니다.\n\n\n1\n\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash pci=nomsi\"\n\n\n해당 내용을 추가하였다면, sudo update-grub으로 수정된 내용을 반영해주고 컴퓨터를 재부팅하면 됩니다.\n저는 최종적으로 위 내용을 반영하고나서, 위의 에러가 다시 발생하지 않는다는 것을 확인하였습니다.\n(2021.06.03 추가) 이후에 지속적으로 GPU has fallen off the bus를 겪었습니다.\n따라서, GRUB_CMDLINE_LINUX_DEFAULT의 내용을 아래와 같이 추가하였습니다.\n\n\n1\n\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash pic=nomsi pcie_aspm=off intel_idle.max_cstate=0\"\n\n\n해당 커널 파라미터를 설정한 참고자료는 아래와 같습니다.\n\nACTIVE-STATE POWER MANAGEMENT ASPM grub 설정 변경\nRHEL6 - intel_idle과 C States\nc-state와 powertop - unknown error 해결하기\n\nReference\n\nUnable to determine the device handle for GPU :GPU is lost\nXid 79, GPU has fallen off the bus.\nGPU has fallen off the bus (Nvidia)\nHow do i get the CPU temperature\nnvidia smi 꼭 알아야 할 TIP - temp,serial,topology matrix\nTroubleshooting PCIe Bus Error severity Corrected on Ubuntu and Linux Mint\nNVIDIA 우분투 마이닝 FAQ","link":"http://ssaru.github.io/2021/05/21/20210521-til_gpu_is_lost_reboot_the_system_to_recover_this_gpu/","title":"(TIL) GPU is lost. Reboot the system to recover this GPU Error잡기","pubDate":"2021-05-21T13:49:00.000Z"}},{"node":{"contentSnippet":"Installing locale in alpine Linux\n애플리케이션이 다국어를 지원할 수 있도록 docker image에 다른 locale을 설치하는 것이 필요할 수 있습니다. 올바른 locale이 설치되지 않는다면 텍스트 데이터가 올바르게 표현되지 않는 문제를 겪을 수 있습니다. alpine docker image는 locale, locale-gen 명령어가 존재하지 않으며, apk add locale로 설치를 시도할 경우 locale이라는 패키지가 없다는 에러를 확인할 수 있습니다. alpine에서 locale을 사용하기 위해서는 alpine과 호환되는 locale 패키지를 별도로 설치해줘야합니다. \n(해당 포스팅에서는 명령어는 dockerfile을 기준으로 설명합니다.)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\nFROM alpine:3.6\n\n# ---not shown here---\n\n# Install language pack\nRUN apk --no-cache add ca-certificates wget && \\\n    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://raw.githubusercontent.com/sgerrand/alpine-pkg-glibc/master/sgerrand.rsa.pub && \\\n    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.25-r0/glibc-2.25-r0.apk && \\\n    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.25-r0/glibc-bin-2.25-r0.apk && \\\n    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.25-r0/glibc-i18n-2.25-r0.apk && \\\n    apk add glibc-bin-2.25-r0.apk glibc-i18n-2.25-r0.apk glibc-2.25-r0.apk\n\n# install ko_KR locale\n# Note that locale -a is not available in alpine linux, use `/usr/glibc-compat/bin/locale -a` instead\nRUN /usr/glibc-compat/bin/localedef -i ko_KR -f UTF-8 ko_KR.UTF-8\n\nRUN /usr/glibc-compat/bin/localedef -i ko_KR -f UTF-8 ko_KR.UTF-8 && \\\n    echo \"export LC_ALL='ko_KR.UTF-8'\" >> /etc/profile && \\\n    echo \"export LANG='ko_KR.UTF-8'\" >> /etc/profile && \\\n    echo \"export LANGUAGE='ko_KR.UTF-8'\" >> /etc/profile\nENV LC_ALL=ko_KR.UTF-8\n\n# --- not show here---\n\n\ninstall locale package\n\n\n1\n2\n3\n4\n5\n6\n\nRUN apk --no-cache add ca-certificates wget && \\\n    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://raw.githubusercontent.com/sgerrand/alpine-pkg-glibc/master/sgerrand.rsa.pub && \\\n    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.25-r0/glibc-2.25-r0.apk && \\\n    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.25-r0/glibc-bin-2.25-r0.apk && \\\n    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.25-r0/glibc-i18n-2.25-r0.apk && \\\n    apk add glibc-bin-2.25-r0.apk glibc-i18n-2.25-r0.apk glibc-2.25-r0.apk\n\n\n위 명령어는 alpine linux를 지원하는 locale 패키지를 설치합니다. 해당 명령어를 실행하고나면, /usr/glibc-compat/bin 디렉토리에서 locale관련 명령어인 locale과 localedef를 확인할 수 있습니다. \ncompile locale\nlocale관련 패키지를 설치하고나면, 현재 시스템이 사용중인 locale과 사용 가능한 locale을 /usr/glibc-compat/bin/locale 명령어와 /usr/glibc-compat/bin/locale -a로 확인할 수 있습니다. 이를 확인해보면, 내가 사용하고자하는 ko_KR.UTF-8 이 없음을 확인할 수 있습니다. \nlocale을 정의한 파일들은 /usr/glibc-compat/share/i18n/locales 폴더 아래에 있고, charmap(캐릭터맵)에 대한 정보는 /usr/glibc-compat/share/i18n/charmaps 폴더 아래에 있습니다. 이 두 가지 정보가 localedef의 명령어로 컴파일 되며 컴파일을 하게되면 /usr/glibc-compat/lib/locale 폴더에 locale-archive라는 접두사(prefix)를 가진 파일이 생성되게 됩니다.\nko_KR.UTF-8 locale을 사용하기 위해서 아래 명령어로 locale을 컴파일 합니다.\n\n\n1\n\nRUN /usr/glibc-compat/bin/localedef -i ko_KR -f UTF-8 ko_KR.UTF-8\n\n\nSet locale\n이제 locale 관련 패키지도 설치하였고 필요한 locale도 컴파일 해줬으니, 환경변수에 locale을 설정해봅시다.\n\n\n1\n2\n3\n4\n5\n\nRUN /usr/glibc-compat/bin/localedef -i ko_KR -f UTF-8 ko_KR.UTF-8 && \\\n    echo \"export LC_ALL='ko_KR.UTF-8'\" >> /etc/profile && \\\n    echo \"export LANG='ko_KR.UTF-8'\" >> /etc/profile && \\\n    echo \"export LANGUAGE='ko_KR.UTF-8'\" >> /etc/profile\nENV LC_ALL=ko_KR.UTF-8\n\n\nubuntu는 기본적으로 bash shell을 사용하기 때문에, 환경변수를 로그인 쉘의 경우에는 ~/.bashrc에 미 로그인 쉘에는 /etc/profile에 설정합니다. alpine은 bash shell이 아닌 ash를 제공하는 busybox를 기본 shell로 사용합니다. 따라서 환경변수 설정은 로그인 쉘의 경우 ~/.profile, 미 로그인 쉘은 /etc/profile에 설정해야합니다.\n(저의 경우는 미로그인 쉘을 사용했으므로 환경변수를 /etc/profile에 설정하였습니다.)\nCheck locale setting\n설정을 완료하고나면, /usr/glibc-compat/bin/locale와 /usr/glibc-compat/bin/locale -a를 통해 locale이 ko_KR.UTF-8로 잘 설정되었음을 확인할 수 있습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n$ /usr/glibc-compat/bin/locale\n>>\nLANG=ko_KR.UTF-8\nLC_CTYPE=\"ko_KR.UTF-8\"\nLC_NUMERIC=\"ko_KR.UTF-8\"\nLC_TIME=\"ko_KR.UTF-8\"\nLC_COLLATE=\"ko_KR.UTF-8\"\nLC_MONETARY=\"ko_KR.UTF-8\"\nLC_MESSAGES=\"ko_KR.UTF-8\"\nLC_PAPER=\"ko_KR.UTF-8\"\nLC_NAME=\"ko_KR.UTF-8\"\nLC_ADDRESS=\"ko_KR.UTF-8\"\nLC_TELEPHONE=\"ko_KR.UTF-8\"\nLC_MEASUREMENT=\"ko_KR.UTF-8\"\nLC_IDENTIFICATION=\"ko_KR.UTF-8\"\nLC_ALL=ko_KR.UTF-8\n\n$ /usr/glibc-compat/bin/locale -a\n>>\nC\nPOSIX\nko_KR.utf8\n\n\nReference\n\nSetting up locale on alpine:3.6 docker image\n컨테이너 접속시 locale 에러 해결\nWhere to set system default environment variables in Alpine linux?","link":"http://ssaru.github.io/2021/05/09/20210509-til_set_locale_in_alpine_linux/","title":"(TIL) Alpine Linux에서 locale 설정하기","pubDate":"2021-05-09T07:14:00.000Z"}},{"node":{"contentSnippet":"Installing PyTorch with RTX 3090 support\n2021.05.05 현재 RTX3090은 CUDA11 이상을 지원하는 딥러닝 프레임워크에 버전에서만 사용할 수 있습니다. 하지만 단순하게 pip install torch==1.7.1 torchvision==0.8.2 형태로 설치하면 CUDA error: no kernel image is available for execution on the device 에러를 마주할 수 있습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n$ python3 -m pip install torch==1.7.1 torchvision==0.8.2\nCollecting torch==1.7.1\n  Using cached torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\nCollecting torchvision==0.8.2\n  Using cached torchvision-0.8.2-cp38-cp38-manylinux1_x86_64.whl (12.8 MB)\nRequirement already satisfied: numpy in /home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from torch==1.7.1) (1.20.2)\nRequirement already satisfied: typing-extensions in /home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from torch==1.7.1) (3.10.0.0)\nRequirement already satisfied: pillow>=4.1.1 in /home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from torchvision==0.8.2) (8.2.0)\nInstalling collected packages: torch, torchvision\nSuccessfully installed torch-1.7.1 torchvision-0.8.2\n\n$ python3\nPython 3.8.10 (default, May  4 2021, 22:52:00)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> torch.cuda.is_available()\nTrue\n>>> torch.randn(3,3).to(\"cuda:0\")\n/home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning:\nGeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75.\nIf you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n\n  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/tensor.py\", line 179, in __repr__\n    return torch._tensor_str._str(self)\n  File \"/home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/_tensor_str.py\", line 372, in _str\n    return _str_intern(self)\n  File \"/home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/_tensor_str.py\", line 352, in _str_intern\n    tensor_str = _tensor_str(self, indent)\n  File \"/home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/_tensor_str.py\", line 241, in _tensor_str\n    formatter = _Formatter(get_summarized_data(self) if summarize else self)\n  File \"/home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages/torch/_tensor_str.py\", line 89, in __init__\n    nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))\nRuntimeError: CUDA error: no kernel image is available for execution on the device\n\n\n이 때에는 반드시 pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html 형태로 설치해주어야합니다.\n(RTX3090을 지원하는 PyTorch는 python3.8이상을 요구한다고 하여 해당 예제는 python3.8에서 테스트하였습니다)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n$ python3 -m pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html \npython3 -m pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==1.7.1+cu110\n  Using cached https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp38-cp38-linux_x86_64.whl (1156.8 MB)\nCollecting torchvision==0.8.2+cu110\n  Using cached https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp38-cp38-linux_x86_64.whl (12.9 MB)\nRequirement already satisfied: numpy in /home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from torch==1.7.1+cu110) (1.20.2)\nRequirement already satisfied: typing-extensions in /home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from torch==1.7.1+cu110) (3.10.0.0)\nRequirement already satisfied: pillow>=4.1.1 in /home/ubuntu/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from torchvision==0.8.2+cu110) (8.2.0)\nInstalling collected packages: torch, torchvision\nSuccessfully installed torch-1.7.1+cu110 torchvision-0.8.2+cu110\n\n$ python3\nPython 3.8.10 (default, May  4 2021, 22:52:00)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> torch.cuda.is_available()\nTrue\n>>> torch.randn(3,3).to(\"cuda:0\")\ntensor([[-0.5210,  0.2223,  1.7470],\n        [ 0.5932,  0.3055,  0.8472],\n        [-0.1898, -1.0950,  1.6593]], device='cuda:0')","link":"http://ssaru.github.io/2021/05/05/20210505-til_install_rtx3090_supported_pytorch/","title":"(TIL) RTX 3090을 지원하는 PyTorch 버전설치","pubDate":"2021-05-05T13:27:00.000Z"}},{"node":{"contentSnippet":"Orthonormal Basis\nDefinition 3.9 (Orthonormal Basis)\n$n$차원의 벡터 공간 $V$과 $V$에서의 basis \\{b_{i},...,b_{n}\\} 를 생각했을 때, $\\forall\\ i,j=1,…,n$ 에서 다음을 만족하면 이를 orthonormal basis(ONB)라고 부릅니다.\n\\big =0, \\ \\ for\\ i \\neq j\\ \\ \\ \\ (3.33)\\\\ \\big = 1\\ \\ \\ \\ (3.34)\\\\\n만약에 (3.33)만 만족한다면 이 basis는 orthogonal basis라고 부릅니다. (3.34)는 모든 basis vector의 length나 norm이 1임을 의미합니다. 따라서 orthonormal basis라는 것은 basis가 서로 orthogonal하면서, length나 norm이 1인 basis를 말하는 것입니다.\nChapter 2.6.1의 내용을 다시 생각해보면, vector들의 집합으로부터 스팬된 vector space에서의 basis를 찾기 위해서 가우시안 소거법을 사용했었습니다. \n우리에게 non-orthogonal이고 unnormalized basis vecotr의 집합인 \\{\\tilde{b}_{1},...,\\tilde{b}_{n} \\}이 주어졌다고 생각해보십니다.\n이때, orthonormal basis를 얻기 위해서는 다음과 같은 과정을 반복합니다.\n\nunnormalized basis vector들의 집합들을 concatenate해서 행렬 \\tilde{B} = [\\tilde{b}_{1},...,\\tilde{b}_{n}] 를 만듭니다.\nconcatenate해서 만든 행렬을 augmented matrix(\\tilde{B}\\tilde{B}^{T}|\\tilde{B}) 형태로 변경합니다.\naugmented matrix form의 행렬을 가우시안 소거법을 적용합니다.\n\n위와 같이 반복적인 작업을 통해서 orthonormal basis \\{b_{1},...,b_{n}\\}를 구하는 방법을 Gram-Schmidt process라고 부릅니다.","link":"http://ssaru.github.io/2020/11/01/20201101-mml_book_chap3.5_orthonormal_basis/","title":"(MML Book 선형대수 Chapter 3.5) Orthonormal Basis","pubDate":"2020-11-01T13:10:00.000Z"}},{"node":{"contentSnippet":"Angle\ninner product는 벡터의 length나, 두 벡터 간의 distance를 정의하게 하는 것 외에도 두 벡터 간의 각도 $\\omega$를 정의할 수 있습니다.\n이때, 두 벡터 $x, y$ 사이의 inner product space에서 각도 $\\omega$를 정의하기 위해서 우리는 Cauchy-Schwarz inequality (3.17)를 사용합니다. \n\n Cauchy-Schwarz Inequality\n|\\big| \\leqslant ||x||\\ ||y||\\ \\ \\ \\ (3.17)\n​        \n$x \\neq 0, y \\neq0$을 가정할 때, Angle은 다음과 같습니다.\n-1 \\leqslant \\frac{\\big}{||x||\\ ||y||} \\leqslant 1\\ \\ \\ \\ (3.24)\n\nCauchy-Schwarz Inequality에서 좌항의 절대값을 제거\n-||x||\\ ||y|| \\leqslant \\big \\leqslant ||x||\\ ||y||\n\n모든 항을 $||x||\\ ||y||$ 로 나눔\n-1 \\leqslant \\frac{\\big}{||x||\\ ||y||} \\leqslant 1\\ \\ \\ \\ (3.24)\n\n​    \n이때,  $\\omega \\in [0, \\pi]$는 다음과 같이 정의되며, Figure 3.4과 같이 표현할 수 있습니다.\ncos \\omega = \\frac{\\big}{||x||\\ ||y||}\\ \\ \\ \\ (3.25)\n\n​     \n이때, $\\omega$는 두 벡터 $x, y$간의 각도입니다. 각도는 직관적으로 두 벡터의 방향성이 얼마나 일치하는지에 대해서 알려줍니다. 예를들어 inner product가 dot product이고 벡터 $x, y=4x$가 있을 때, $y$는 $x$가 스케일된 벡터이므로 Angle은 0입니다.  Angle이 0이라면 방향이 같다는 의미가 됩니다.\n\n​    \nOrthogonality\nlength, distance, angle을 유도하는 것 외에 inner product의 핵심 특징은 두 벡터가 orthogonal인지 아닌지에 대한  알 수 있게 해준다는 것입니다.\n​    \nDefinition 3.7 (Orthogonality)\n만약 $\\big< x, y \\big> =0$이라면, 두 벡터 $x,y$는 orthogonal이며 우리는 이를 $x \\perp y$라고 적습니다. 추가적으로 $||x||=1=||y||$라면 벡터는 unit vector라는 속성도 추가되어 orthonormal하다라고 이야기합니다.\n이러한 정의가 함축하는 것은 $0$-벡터는 벡터 공간에서 모든 벡터와 orthogonal하다는 것을 의미합니다.\n​    \nRemark\nOrthogonality는 inner product에 대한(bilinear forms) 수직(perpendicularity) 개념의 일반화입니다.\n기하학적인 맥락에서는 두 벡터가 orthogonal이라면 두 벡터가 서로 직각인 벡터로써 생각할 수 있습니다.\n\n\n​    \nDefinition 3.8 (Orthogonal Matrix)\n만약에 모든 column들 끼리 모두 orthonormal하다면 square matrix $A \\in \\mathbb{R}^{n \\times n}$은 orthogonal matrix입니다.\nAA^{T} = I = A^{T}A\\ \\ \\ \\ (3.29)\n그리고, 이는 아래의 식과 같은 관계를 갖습니다.\nA^{-1} = A^{T}\\ \\ \\ \\ (3.30)\n즉, 역행렬을 transpose로 간단히 구할 수 있다는 의미가 됩니다.\n​    \northogonal transformation matrix $A$를 이용한 transformation($Ax$)은 벡터 $x$의 length가 변하지 않는다는 특성이 있습니다. inner product를 dot product라고 생각하면, $||Ax||^{2}$ 은 $||x||^{2}$ 와 같습니다.\n||Ax||^{2} = (Ax)^{T}(Ax) = x^{T}A^{T}Ax = x^{T}Ix = x^{T}x = ||x||^{2}\\ \\ \\ \\ (3.31)\n​    \n거기에 두 벡터 $x, y$사이의 각도 또한 inner product로써 측정되므로 orthogonal matrix $A$를 이용해 transformation시 각도 또한 변하지 않습니다. dot product를 inner product로 가정하고 orthogonal matrix$A$ 가 있을 때, $Ax, Ay$ 간의 각도는 두 벡터 $x, y$ 와 같음을 확인할 수 있습니다.\ncos \\omega = \\frac{(Ax)^{T}(Ay)}{||Ax||\\ ||Ay||} = \\frac{x^{T}A^{T}Ay}{\\sqrt{x^{T}A^{T}Axy^{T}A^{T}Ay}} = \\frac{x^{T}y}{||x||\\ ||y||}\\ \\ \\ \\ (3.32)","link":"http://ssaru.github.io/2020/11/01/20201101-mml_book_chap3.4_angles_and_orthogonality/","title":"(MML Book 선형대수 Chapter 3.4) Angles and Orthogonality","pubDate":"2020-11-01T12:34:00.000Z"}},{"node":{"contentSnippet":"시작하기; Norm\n앞서 norm은 vector의 vector의 lenght 혹은 magnitude의 직관이라고 이야기했었습니다.\n아래 식과 같이 어떤 inner product지 간에 norm을 유도한다는 측면에서 Inner product와 norm은 매우 밀접하게 연관 되어있습니다.\n||x|| := \\sqrt{\\big}\\ \\ \\ \\ (3.16)\n위의 식에서 $\\big< \\cdot, \\cdot \\big>$는 inner product를 의미합니다.\n하지만, 모든 norm이 inner product에 의해 유도되는 것은 아닙니다. Manhattan norm은 inner product로부터 유도되지 않는 대표적인 norm입니다.\n\nnorm이 inner product에서 유도되었는지 아닌지를 확인하는 방법에 대해서 알고싶다면, “An example of a norm which can’t be generated by an inner product“과 “Parallelogram law“를 참고하세요.\n\n​    \nRemark. (Cauchy-Schwarz Inequality)\ninner product vector space $(V, \\big< \\cdot, \\cdot \\big>)$에서 유도된 norm $||\\cdot||$은 Cauchy-Schwarz inequality를 만족합니다.\n|\\big| \\leqslant ||x||\\ ||y||\\ \\ \\ \\ (3.17)\n​     \nnorm에 의해 계산되는 length는 어떤 inner product냐에 따라 값이 변할 수 있습니다. 아래 Example 3.5에서는 dot product가 아닌 다른 inner product를 사용해서 norm을 구하면 값이 어떻게 변할 수 있는지에 대해 나타냅니다.\n\n​    \nDistance와 Metric\nDefinition 3.6. (Distance and Metric)\ninner product space $(V, \\big< \\cdot, \\cdot \\big>)$을 고려했을 때, 아래의 식을 벡터 $x$와 $y$의 distance라고 부릅니다. $(x, y \\in V)$\nd(x, y) := ||x-y|| = \\sqrt{\\big}\\ \\ \\ \\ (3.21)\n만약에 inner product로 dot product를 사용한다면, 그 distance는 Euclidean distance라고 부릅니다.\n이 때, distance에 대한 mapping은 아래와 같이 표현되며, 이를 metric이라고 부릅니다.\nd: V\\times V \\rightarrow \\mathbb{R}\\ \\ \\ \\ (3.22)\\\\ (x, y) \\mapsto d(x,y)\\ \\ \\ \\ (3.23)\n​    \nRemark\n벡터의 길이와 유사하게 벡터 간의 거리를 구할 때는 inner product가 필요하지 않습니다. 단지 inner product로 유도된 norm이면 됩니다. 다만 거리는 norm이 어떤 inner product로부터 유도된 norm이냐에 따라 다를 수 있습니다.\n​    \n지금까지 distnace와 metric의 정의에 대해서 살펴봤습니다. 두 벡터 공간 $V, V$ 에서 $\\mathbb{R}$ 공간으로 mapping하는 함수 metric $d$ 는 아래와 같은 속성을 갖습니다.\n​    \n\npositive definite\n\n$d(x, y) \\geqslant 0, \\forall x, y \\in V$\n\n$d(x, y) = 0 \\Longleftrightarrow x=y.$\n\n\nsymmetric\n\n$d(x, y) = d(y, x), \\forall x, y \\in V$\n\n\nTriangle inequality\n\n$d(x, z) \\leqslant d(x, y) + d(y, z), \\forall x,y,z \\in V$\n\n\n​    \nRemark\ninner product와 metric의 속성은 매우 유사해보입니다. 하지만 위에 언급한 Definition 3.6과 이전에 챕터에서 Definition 3.3을 비교해보면 $\\big< x, y \\big>, d(x,y)$ 는 서로 반대되는 방식으로 동작합니다.\nDefinition 3.3\n$V$를 벡터공간, $\\Omega: V \\times V \\rightarrow \\mathbb{R}$을 두 벡터를 real number로 맵핑하는 bilinear mapping이라고 해봅시다.\n\npositive definite, symmetric bilinear mapping $\\Omega: V \\times V \\rightarrow \\mathbb{R}$은 벡터공간 $V$에서의 inner product라고 부릅니다.\n$(V, \\big<\\cdot, \\cdot\\big>)$는 inner product space 혹은 inner product가 있는 (real) vector space라고 부릅니다. 만약 (3.5)에서 정의한 dot product를 사용한다면 우리는 $(V, \\big<\\cdot, \\cdot \\big>)$을 Euclidean vector space라고 부릅니다.\n\n​    \n즉, 벡터 $x, y$가 서로 유사할 때, inner product는 매우 큰 값을 갖으며 metric은 매우 작은 값을 갖습니다.","link":"http://ssaru.github.io/2020/11/01/20201101-mml_book_chap3.3_lengths_and%20_distances/","title":"(MML Book 선형대수 Chapter 3.3) Lengths and Distances","pubDate":"2020-11-01T11:53:00.000Z"}},{"node":{"contentSnippet":"시작하기\n직관적인 개념을 형식화할 때, 공통적인 접근법은 다음과 같습니다.\n\n객체(object, symbol)의 집합(set)을 만든다.\n만든 객체들을 조작(manipulate)하기 위한 규칙을(rule) 만든다.\n\n이러한 접근법으로 만들어진 개념들의 집합은 우리에게 대수(Algebra)라고 알려져 있습니다.\n\n선형대수는 벡터(vector)와 벡터들을 조작하기 위한 규칙들을 연구하는 학문입니다. \n벡터라고 하면 많은 사람이 고등학교에서 배운 geometric vector를 생각합니다. 하지만 책에서는 벡터의 일반화된 개념에 대해서 논의합니다.\n​    \n벡터의 정의\nA라는 유형을 갖는 객체가 있다고 가정해봅시다. 이때,\n\n객체끼리 더함\n스칼라(scalar, 상수)를 객체에 곱함\n\n1), 2)라는 연산의 결과로 같은 유형의 객체(A라는 유형의 객체)가 나온다면 이를 벡터라고 정의합니다.\n다시 말해, 수학적인 관점에서 벡터는 아래 두 성질을 충족하면 벡터로 간주합니다.\n\n벡터끼리 덧셈 연산을(add) 했을 경우 벡터가 나옴\n벡터에 스칼라를 곱했을 때, 벡터가 나옴\n\n벡터에 대한 대표적인 예는 아래와 같습니다.\n\nGeometric vectors:\nFigure 2.1 (a) 에서 표현하고 있는 geometric vector는  고등학교 수학이나 물리에서 많이 배워 매우 친숙한 벡터입니다.\n\nPolynomial vectors:\nFigure 2.1 (b) 에서 표현하고 있는 polynomial(다항식) 또한 벡터입니다.\n두 다항식은 서로 더해질 수 있으며, 더해진 결과 또한 다항식입니다. 그리고 다항식은 스칼라( \\lambda \\in R )로 곱셈 연산을 해도 다항식입니다. 따라서 다항식 또한 벡터입니다.\n\n그 외에도 3. Audio signals, 4. Elements of R^{n} (tuples of n real numbers)도 벡터입니다.\n\nAudio signals, Elements of $R^{n}$은 내용상 크게 문제가 되지 않기 때문에 생략했습니다.\n자세히 알고싶은 분들은 MML Book 18 page를 참고하시면 됩니다.\n\n\n​    \n책에서 초점을 두고 설명하고자 하는 것\n본 책에서는 다음과 같은 내용에 초점을 두고 선형 대수를 설명합니다.\n\n선형 대수의 대부분 알고리즘은 R^{n} 공간에서 정의가 됩니다. 따라서 책에서는 주로 R^{n} 공간의 벡터에 대해서 초점을 맞춰 설명합니다. 특히 chapter 8에서 데이터를  R^{n}공간의 벡터로 간주하고 설명할 것입니다.\n유한 차원 벡터 공간에(finite dimensional vector space) 초점을 맞춥니다. 유한 차원 벡터 공간의 경우, 모든 종류의 벡터와  R^{n}공간 간의 1:1 대응이 존재합니다.\n\n​    \n선형 대수와 기계학습 알고리즘 간의 상관관계\n만약 내가 새로운 연산자(operator)를 만들었다고 생각해봅시다. 이때, 내가 제안한 연산(operation)으로 만들어지는 것들의 집합은 무엇인가?라는 궁금증이 생길 수 있습니다.\n이를 선형대수 관점에서 다시 생각해보면, 작은 벡터의 집합에서 시작해서 서로 더해지고, 스케일링 되었을 때의 그 결과인 벡터들의 집합은 무엇인가?와 같습니다. 이 질문에 대한 답은 Section 2.4에서 언급할 vector space가 됩니다. vector space의 개념과 속성은 기계학습 알고리즘의 기초가 됩니다.\n선형대수는 기계학습과 수학에서 중요한 역할을 합니다. 이번 장에서 소개되는 개념들은 chapter 3에서 geometry에 대한 아이디어로 확장됩니다. chapter 5에서는 벡터 미적분(vector calculus)에 대해서 논의합니다. 벡터 미적분은 매트릭스 연산에 대한 지식이 필수적으로 요구됩니다. chapter 10에서는 PCA를 활용한 차원 축소를 위해 사영(projection)을 사용할 것입니다. chapter 9에서는 선형 회귀(linear regression)에 대해서 논의합니다. 선형 회귀에서 선형대수는 least-square problems을 푸는 중요한 역할을 합니다.\n\n​    \n선형 시스템(Systems of Linear Equations)\n선형 시스템은 선형 대수에서 중심적인 역할을 합니다. 많은 문제는 선형 시스템으로 형식화되며, 선형대수는 이를 풀 수 있는 도구를 제공합니다.\n​    \nExample 2.1\n어떤 회사가 리소스 R_{1}, ... ,R_{N} 가 들어가는 제품 N_{1}, ... ,N_{n} 을 생산합니다. 이때, N_{j}의 일부 파트를 생산하기 위해서 리소스 R_{i}의 일부인 a_{ij}가 필요합니다. (i=1,...,m, j=1,...,n).\n우리의 목적은 한정된 리소스에서 얼마나 많은 제품을 만들 수 있는지에 대한 최적의 생산 계획을 찾는 것입니다. 예를 들어 “우리에게 리소스 b_{i}가 있을 때,  제품 N_{j}의 일부 파트인 x_{j}를 얼마나 많이 생산할 수 있는가?”와 같은 질문의 답을 찾는 것이지요.\n만약 우리가 제품과 대응되는 파트들 x_{i},...,x_{n}을 생산할 때, 다음과 같은 리소스가 필요하게 됩니다.\na_{i1}x_{1} + ... + a_{in}x_{n}\\ \\  (2.2)\n이때, 최적의 생산계획은 실수공간에 존재하게 됩니다(x_{1},...,x_{n}) \\in R^{n}. 따라서, 최적의 생산 계획은 아래의 선형 시스템을 만족하는 해를 찾으면 됩니다.\n\\begin{matrix}  a_{11}x_{1} + ... + a_{1n}x_{n} =  b_{1}\\\\   ...\\\\  a_{m1}x_{1} + ... + a_{mn}x_{n} = b_{m} \\end{matrix}, \\ \\ \\ a_{ij} \\in R, \\ b_{i} \\in R \\ \\ (2.3)\n식 (2.3)은 일반적인 형태의 선형 시스템입니다.\nx_{1},...,x_{n},\\ \\ \\ \\ (x_{1},...,x_{n}) \\in R^{n}\n은 시스템에서 미지수가 되며, 식 (2.3) 을 만족하는 해가 이 선형 시스템의 해가 됩니다.\n​    \nExample 2.2\n다음 선형 시스템이 있을 때,\n\\begin{matrix}  x_{1} + x_{2} + x_{3} = 3\\ \\ \\ \\  (1)\\\\   x_{1} - x_{2} + 2x_{3} = 2\\ \\ \\ (2)\\\\   2x_{1} + 3x_{3} = 1 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (3)\\\\\\end{matrix}\n이 선형 시스템은 해가 없습니다. (1)과 (2)를 더하면 2x_{1} + 3x_{3} = 5입니다. 이 결과를 (3)과 빼게되면, 부등식은 모순이 됩니다. 이 경우에는 선형 시스템에 해는 없게 됩니다.\n또 다른 선형 시스템을 살펴봅시다.\n\\begin{matrix}  x_{1} + x_{2} + x_{3} = 3\\ \\ \\ \\  \\ (1)\\\\   x_{1} - x_{2} + 2x_{3} = 2\\ \\ \\ (2)\\\\ \\ \\ \\ \\ \\ \\ \\ x_{2} + x_{3} = 2 \\ \\ \\ \\ \\ \\ \\ (3)\\\\\\end{matrix}\n이를 위와 같은 방식으로 풀게 되면, (x_{1} = 1, x_{2} = 1, x_{3}=1)이 유일한 해(유일해; unique solution)가 됩니다.\n마지막으로 아래 선형 시스템을 살펴봅시다.\n\\begin{matrix}  x_{1} + x_{2} + x_{3} = 3\\ \\ \\ \\  \\ (1)\\\\   x_{1} - x_{2} + 2x_{3} = 2\\ \\ \\ (2)\\\\ 2x_{1} + 3x_{3} = 5 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (3)\\\\\\end{matrix}\n이를 똑같이 풀어보면, (1)과 (2)를 더한 결과가 (3)과 같음을 확인할 수 있습니다.\n이때, (1)과 (2)를 이용해서 2x_{1} = 5 - 3x_3{}, 2x_{2} = 1+x_{3}을 얻을 수 있습니다.\n$x_{3} = a \\in R$로 자유 변수(free variable)로 정의하면, 다음과 같은 해를 찾을 수 있습니다.\n(\\frac{5}{2} - \\frac{3}{2}a, \\frac{1}{2}+ \\frac{1}{2}a, a),\\ \\  a \\in R\n이 해는 무수히 많은 해가 존재한다는 것을 의미하며, 기하학적으로 Figure 2.3과 같습니다.\n\n일반적으로 실수값을 갖는 선형 시스템으로 구성된 시스템에서 우리는 아래와 같은 종류의 해를 구할 수 있습니다.\n\n해가 없음\n단 하나의 해\n무한히 많은 해\n\n선형 회귀(linear regression)은 Example 2.1과 같은 예제에서 선형 시스템을 풀지 못했을 경우 이를 푸는 방법입니다.\n​    \n선형 시스템에 대한 기하학적 해석(Geometric Interpretation of System of Linear Equation)\n두 개의 변수(x_{1}, x_{2})를 갖는 선형 시스템에서 각각의 시스템은 x_{1} - x_{2}  평면에서 하나의 선이 됩니다.\n만약에 각 시스템을 모두 만족하는 해가 있다면, 이 해는 각 선의 교집합이 됩니다.\n이 교집합은 다음과 같은 형태로 나타납니다.\n\n점이 되거나(교차) \n선이 되거나(겹침)\n공집합(평행; 해가 없음).\n\n이를 확장하여 변수가 세 개가 된다면, 각 시스템은 면(plane)이 되며, 교집합은 선(line)이 됩니다.\n이때의 교집합은 1)면, 2)선, 3)점, 4)공집합(해가 없음)으로 나타나게 됩니다.\n​    \n행렬 (Matrices)\n행렬 또한 선형 대수에서 중요한 역할을 합니다. 행렬은 선형 시스템을 간단히 표기하는데 사용하기도 하지만, Section 2.7에서 살펴볼 선형 함수(linear mapping)를 표현하기도 합니다.\n행렬의 재밌는 주제를 논의하기 전에 먼저, 1) 행렬이 무엇이고, 2) 행렬로 어떤 연산이 가능한지 살펴봅시다.\n​    \nDefinition 2.1 (Matrix)\n $m, n \\in N,\\ \\ N \\in R$인 $(m, n)$ 행렬 $A$는 $m$ rows와 $n$ columns으로 구성된 직사각형 구조를 갖습니다. \n행렬의 원소 $a_{ij},\\ i=1,…,m,\\ j=1,…,n$ 는 $m\\cdot n$ - tuple입니다.\nA = \\begin{bmatrix}        a_{11} & a_{12} & ... & a_{1n}\\\\         a_{21} & a_{22} & ... & a_{2n}\\\\        \\vdots & \\vdots & & \\vdots \\\\        a_{m1} & a_{m2} & ... &  a_{mn}     \\end{bmatrix}, \\ \\ a_{ij} \\in R\n일반적으로 $(1, n)$-행렬은 rows라고 부르고, $(m, 1)$-행렬은 columns라고 부릅니다.\nrows, colums와 같이 특별한 행렬은 row, column vectors라고 부릅니다.\n$R^{m \\times n}$는 실수공간에 있는 $(m, n)$-행렬입니다.\n $A \\in R^{m \\times n}$는 모든 $n$ columns의 행렬이 길게 붙여진 $a \\in R^{mn}$과 같은 표현입니다.\n\n행렬의 표현이 달라질 뿐이지, 벡터 개념에서는 벗어나지 않는다는 것에 주목합시다.\n표현이 변경된 행렬은 다른 행렬을 더해도 같은 형태를 유지하는 행렬이며, 스칼라와 곱해도 같은 형태를 유지하는 행렬입니다. \n추가적으로, 우리가 알고 있는 행렬곱의 연산자(opration)를 일부 변경하면 R^{nm} 행렬 또한 행렬곱 정의가 가능합니다.\n\n\n​    \n행렬의 덧셈과 곱셈\n두 행렬 $A \\in R^{m \\times n}$, $B \\in R^{m \\times n}$의 합은 element-wise sum으로 정의됩니다\nA + B := \\begin{bmatrix}        a_{11} + b_{11} & ... & a_{1n} + b_{1n} \\\\         \\vdots &  & \\vdots \\\\        a_{m1} + b_{m1} & ... & a_{mn} + b_{mn}        \\end{bmatrix} \\in R^{m \\times n}\n두 행렬 $A \\in R^{m \\times n}$, $B \\in R^{n \\times k}$가 있을 때, 행렬곱( $C = AB \\in R^{m \\times k}$)의 결과인 행렬 C 원소 $c_{ij}$는 다음과 같이 계산합니다\nc_{ij} = \\Sigma_{l=1}^{n} a_{il}b_{lj},\\ \\ \\ \\ i = 1,...,m,\\ \\ \\ \\ \\ j = 1,...,k.\n위와 같이 $A$의 $i$th row의 원소와 $B$의 $j$th columns의 원소끼리 곱하는 연산을 Section 3.2에서는 dot product라고 부르며, $A \\cdot B$로 표기합니다.\n​    \nRemark\n\n행렬의 곱은 인접 차원이 같아야합니다. 예를 들어 $n \\times k$ 행렬 $A$는 $k \\times m$ 행렬 $B$와 곱셈 연산을 수행할 수 있습니다. 이때, 행렬 $B$는 행렬 $A$ 우측에 있어야만 합니다.\n\\underbrace{A}_{n \\times k} \\underbrace{B}_{k \\times m} =\\underbrace{C}_{n \\times m}\n프로덕트(product) $BA$는 이웃 차원이 맞지 않기 때문에 정의되지 않습니다.\n\n​    \nRemark\n\n행렬 곱은 element-wise 연산으로 정의되지 않습니다. 프로그래밍 언어에서 종종 나타나는 array 간 element-wise 곱은 Hadamard product라고 부릅니다.\n\n​    \nDefinition 2.2 (Identity Matrix) \n$R^{n \\times n}$에서 단위행렬(identity matrix)은 대각방향으로는 1, 그 외에는 0을 갖는 $n \\times n$- 행렬로 정의됩니다.\nI_{n} := \\begin{bmatrix}    1&0&...&0&...&0 \\\\     0&1&...&0&...&0 \\\\    \\vdots&\\vdots&\\ddots&\\vdots&\\ddots&\\vdots \\\\    0&0&...&1&...&0 \\\\    \\vdots&\\vdots&\\ddots&\\vdots&\\ddots&\\vdots \\\\    0&0&...&0&...&1 \\\\    \\end{bmatrix} \\in R^{m \\times n}\n지금까지 우리는 행렬의 1) 합, 2) 곱, 3) 단위행렬에 대해서 정의했습니다. 이를 이용하여 매트릭스의 속성을 살펴보면 아래와 같습니다.\n\nAssociativity (결합법칙)\n\\forall A \\in R^{m \\times n},\\ B \\in R^{n \\times p},\\ C \\in R^{p \\times q}\\ :\\ (AB)C = A(BC) \\ \\ (2.18)\n\nDistributivity (분배법칙)\n\\forall A, B \\in R^{m \\times n},\\ C, D \\in R^{n \\times p}\\ : (A+B)C = AC+BC \\ \\ (2.19a)\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ A(C+D) = AC + AD \\ \\ (2.19b)\n\nMultiplication with the identity matrix (단위행렬과의 곱셈)\n\\forall A \\in R^{m \\times n} : I_{m}A = AI_{n} = A, \\ \\ \\ \\ I_{m} \\neq I_{n}\\ for\\ \\  m \\neq n.\n\n​    \n역행렬과 전치행렬\n​    \nDefinition 2.3 (Inverse)\n정방행렬 $A \\in R^{n \\times n}$이 있을 때, 매트릭스 $B \\in R^{n \\times n}$와의 관계가 $AB = I_{n} = BA$라는 속성을 만족할 때, $B$는 $A$의 역행렬이라고 부르며 $A^{-1}$로 표기합니다.\n모든 행렬 $A$가 역행렬 $A^{-1}$를 갖진 않습니다. $A$가 역행렬을 가지려면 $A$는 regular / invertible / nonsigular(정칙행렬) 이어야 합니다. 다른 경우는 singular / noninvertible (특이 행렬)이라고 부릅니다.\n역행렬이 존재한다면, 이는 유일합니다. Section 2.3에서는 선형 시스템을 풀어 역행렬을 계산하는 일반적인 방법을 논의합니다.\n​    \nRemark (Existence of the Inverse of a $2 \\times 2$-matrix)\nA := \\begin{bmatrix}a_{11} & a_{12} \\\\a_{21} & a_{22}\\end{bmatrix}\\in R^{2 \\times 2}.\\ \\ \\ \\ (2.21)A^{'} := \\begin{bmatrix}a_{22} & -a_{12} \\\\-a_{21} & a_{11}\\end{bmatrix}\\in R^{2 \\times 2}.\\ \\ \\ \\ (2.22)\n 위와 같은 행렬이 있을 때, 두 행렬을 곱하게 되면 다음과 같은 행렬을 얻을 수 있습니다.\nAA^{'} =\\begin{bmatrix}a_{11}a_{22} - a_{12}a_{21} & 0 \\\\0 & a_{11}a_{22} - a_{12}a_{21} \\\\\\end{bmatrix} = (a_{11}a_{22} - a_{12}a_{21})I\\ \\ \\ \\ (2.23)\n 따라서 역행렬 $A^{-1}$은 아래와 같습니다.\nA^{'} := \\frac{1}{a_{11}a_{22} - a_{12}a_{21}}\\begin{bmatrix}a_{22} & -a_{12} \\\\-a_{21} & a_{11}\\end{bmatrix}\n이때, a_{11}a_{22} - a_{12}a_{21} \\neq 0 이어야 합니다. Section 4.1에서는 a_{11}a_{22} - a_{12}a_{21}가 2 \\times 2-matrix의 행렬식(determinant)임을 살펴봅니다. 우리는 행렬식을 통해 행렬이 역행렬을 갖는지 확인할 수 있습니다.\n\n​    \nDefinition 2.4 (Transpose)\n행렬 A, B가 다음과 같을 때 A \\in R^{m \\times n}, B \\in R^{n \\times m}, b_{ij} = a_{ji}면 B를 A의 전치행렬(transpose)이라고 부릅니다. 전치행렬은 B = A^{T}로 표기합니다.\n역행렬과 전치행렬의 중요한 속성은 다음과 같습니다.\n$AA^{-1} = I = A^{-1}A \\\n(AB)^{-1} = B^{-1}A^{-1} \\\n(A + B)^{-1} \\neq A^{-1} + B^{-1} \\\n(A^{T})^{T} = A \\\n(A+B)^{T} = A^{T} + B^{T} \\\n(AB)^{T} = B^{T}A^{T}$$\n​    \nDefinition 2.5 (Symmetric Matrix, 대칭행렬)\n$A \\in R^{n \\times n}$인 행렬이 대칭행렬이라면 $A = A^{T}$입니다.\n대칭행렬은 정방행렬인 $(n, n)$-행렬에서만 가능하며, 역행렬을 가지며, 역행렬은 $A^{T}$입니다.\n$(A^{-1})^{T} = (A^{T})^{-1} =: A^{-T}$\n​    \nRemark (Sum and Product of Symmetric Matrices)\n\n임의의 두 대칭행렬의 합은 항상 대칭행렬이 됩니다. $A, B \\in R^{n \\times n}$\n대칭행렬의 곱은 항상 정의되지만, 결과는 일반적으로 대칭행렬이 아닙니다.\n $\\begin{bmatrix}\n1&0 \\\n0&0\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1&1 \\\n1&1\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1&1 \\\n0&0\\\n\\end{bmatrix}$\n\n​    \n스칼라에 의한 곱셈\n행렬이 스칼라에 의해 곱해질 때를 고려해보겠습니다.\n행렬 $A \\in R^{m \\times n}$과 스칼라 $\\lambda \\in R$가 있을 때, $\\lambda A = K$입니다.\n이때, K_{ij} = \\lambda a_{ij}가 됩니다.\n이때, $\\lambda$는 $A$의 원소를 스케일링한다고 볼 수 있습니다.\n스칼라가 $\\lambda, \\psi \\in R$일 때, 행렬과 스칼라 곱의 성질은 아래와 같습니다.\n\nAssociativity (결합법칙)\n(\\lambda \\psi)C = \\lambda(\\psi C), \\ \\ \\  C \\in R^{m \\times n}\\lambda(BC) = (\\lambda B)C = B(\\lambda C) = (BC)\\lambda,\\ \\ B \\in R^{m \\times n}, C \\in R^{n \\times k}(\\lambda C)^{T} = C^{T}\\lambda^{T}  = C^{T}\\lambda = \\lambda C^{T}\\ \\ since\\ \\lambda=\\lambda^{T}\\ \\ for\\ all\\ \\lambda \\in R\n\nDistributivity  (분배법칙)\n(\\lambda+\\psi)C = \\lambda C + \\psi C,\\ \\ C \\in R \\\\\\lambda(B + C) = \\lambda B + \\lambda C,\\ \\ B, C \\in R\n\n​    \n선형 시스템을 간단하게 표현하는 방법\n\\begin{matrix}  2x_{1} + 3x_{2} + 5x_{3} = 1\\\\   4x_{1} - 2x_{2} - 7x_{3} = 8\\\\ 9x_{1} + 5x_{2} - 3x_{3} = 2 \\\\\\end{matrix}\n위와 같은 선형 시스템이 있을 때, 선형 시스템을 하나하나 표기하는 것이 아닌, 계수(coefficient $a_{ij}$)들의 집합과의 관계로 표기하게 되면, 행렬곱으로 간소화하여 표현할 수 있습니다.\n\\begin{bmatrix}  2&3&5\\\\   4&-2&-7\\\\  9&5&-3 \\end{bmatrix}\\begin{bmatrix}  x_{1}\\\\   x_{2}\\\\  x_{3} \\end{bmatrix} = \\begin{bmatrix}  1\\\\   8\\\\  2 \\end{bmatrix}\n이때, x_{1}은 첫 번째 column을, x_{2}는 두 번째 column을, x_{3}은 세 번째 column을 스케일링한다고 볼 수 있습니다.\n일반적으로 선형 시스템은 행렬 형태로(Ax = b ) 간소화하여 표현할 수 있습니다.\n프로덕트 $Ax$는 $A$의 columns의 선형 조합입니다. 우리는 Section 2.5에서 선형 조합에 대해서 논의해볼 것입니다.","link":"http://ssaru.github.io/2020/09/23/20200921-MML_Book_Chap_2.3/","title":"(MML Book 선형대수 Chapter ~2.2) 선형대수/벡터/선형시스템/매트릭스","pubDate":"2020-09-23T14:31:30.000Z"}},{"node":{"contentSnippet":"Robert Nishihara의 허락을 받아, Modern Parallel and Distributed Python: A Quick Tutorial on Ray을 번역한 글입니다.\nWhat is Ray?\nRay는 파이썬에서 병렬, 분산 프로그래밍을 위한 오픈소스 프로젝트입니다.\n병렬, 분산 컴퓨팅은 현대 애플리케이션을 구성하는 요소 중 하나로 자리잡았습니다. 우리는 필요에 따라 멀티코어나 여러 대의 머신의 리소스를 최대한 활용해서 애플리케이션을 가속해야할 필요가 있습니다. \n\n웹 사이트를 크롤링하거나 사용자 질의에 응답하는 소프트웨어들은 누군가의 노트북에서 돌아가는 single thread기반의 프로그램이 아니고, 서로 통신하고 상호작용하는 서비스 집합이라고 볼 수 있습니다.\n\n\n(클라우드 컴퓨팅은 메모리, 연산, 스토리지 등 다방면으로 끊임없는 확장성을 제공하고있습니다. 클라우드가 제공하는 이러한 이점에 적절하게 대응하기 위해서는 분산 어플리케이션을 만들 수 있는 새로운 도구가 필요합니다)\n이번 포스팅은 Ray를 사용해서 병렬,분산 어플리케이션을 만드는 방법에 대해서 설명합니다.\nWhy Ray?\n많은 튜토리얼들이 Python의 multiprocessing 모듈을 어떻게 사용하는지 설명합니다. \n하지만 Python의 multiprocessing 모듈은 한계점을 가지고 있어 현대 애플리케이션이 요구하는 분산, 병렬에 대한 필수사항을 충족하지 못합니다.\n현대 애플리케이션이 요구하는 분산, 병렬처리에 대한 필수사항은 다음과 같습니다.\n\n같은 코드를 한대 이상의 머신(machine)에서 작동시켜야함\nstate를 가지고, 통신이 가능한 actor와 microservice를 만들 수 있어야함\nmachine failures를 깔끔하게 다룰 수 있어야함\n대규모 객체와 수치 데이터를 효율적으로 다룰 수 있어야함\n\nRay는 위에서 언급한 요구사항을 모두 충족합니다. 또한 간단한 작업을 단순하게 만들며, 복잡한 동작을 하게끔 프로그래밍하는 것 또한 가능합니다.\n\n다른 회사들이 자신들의 Python 프로덕션을 확장하기 위해서 Ray를 어떻게 활용하고있는지 배우고싶다면, Ray Summit에 등록하세요!\n\nNecessary Concepts\n전통적으로 프로그래밍은 1). 함수(Functions), 2) 클래스(Classes)라는 핵심 개념에 의존합니다. 생각해보면 우리는 함수와 클래스만으로 많은 애플리케이션들을 만들어왔습니다.\n하지만, 함수와 클래스로 구성된 애플리케이션을 분산 환경으로 마이그레이션하려고하면 함수, 클래스라는 개념을 사용할 수 없게됩니다. \n따라서 현재까지 알려진 병렬, 분산 도구를 활용해서 싱글 스레드 애플리케이션을 병렬, 분산 애플리케이션으로 마이그레이션을 하기 위해서는 애플리케이션 코드를 처음부터 다시 작성해야합니다.\n현재까지 알려진 병렬, 분산도구는 저수준에서 고수준까지 다양한 도구들이 있습니다.\n먼저 저수준 도구로는 메세지의 송수신을 저수준의 프리미티브로 제공하는 OpenMPI, Python Multiprocessing, ZeroMQ이 있습니다. 이 도구들은 분산, 병렬 환경을 위한 강력한 기능들을 제공합니다. 하지만, 전통적인 프로그래밍과는 다른 추상화 개념을 사용합니다. 이로 인해 위 도구들을 활용해서 기존의 싱글 스레드 애플리케이션을 분산, 병렬 어플리케이션으로 마이그레이션하기 위해서는 코드 전체를 재작성해야합니다.\n또 다른 예로 도메인에 특화되어 고수준의 추상화를 제공하는 도구들이 있습니다. 딥러닝 모델을 학습하기 위한 TensorFlow, 데이터와 SQL 처리를 위한 Spark, 스트림 처리를 위한 Flink가 대표적입니다. 이 도구들은 neural network나 데이터셋, 스트림에 대한 고수준의 추상화 API를 제공합니다. 하지만, 고수준 추상화를 제공하는 도구들 역시 직렬화된 프로그래밍(serial programming)에서 사용하는 추상화와 다르기 때문에, 애플리케이션 코드 전체를 그에 맞게 재작성해줘야하는 단점이 있습니다.\n\n(분산 컴퓨팅을 위한 도구들. 왼쪽은 저수준의 추상화 API를 지원하는 도구, 오른쪽은 고수준의 추상화 API를 제공하는 도구)\nRay는 위에서 설명한 도구들과 같은 고수준, 저수준이 아닌 중간수준에 위치합니다. Ray는 함수와 클래스를 task, actor라고 불리는 분산환경에 적합한 형태로 변환하며, 이를 통해 병렬, 분산 컴퓨팅을 지원하는 메커니즘을 가지고 있습니다. 따라서 사용자들은 이전과 다르게 코드를 재작성 없이 기존의 함수와 클래스 구조를 유지하면서 분산, 병렬 프로그래밍을 할 수 있습니다.\nStarting Ray\nRay의 ray.init()명령어는 Ray에서 사용하는 프로세스들을 모두 구동합니다. \n만약 클러스터 환경을 이용해서 분산 컴퓨팅을 하고자 한다면, 클러스터의 주소(address)를 입력하는 코드 라인 하나만 변경하면 됩니다.\nray.init()명령어로 구동되는 Ray의 프로세스들은 아래와 같습니다.\n\nWorker : 파이썬의 함수를 병렬적으로 실행할 프로세스(대략 하나의 worker는 하나의 CPU 코어를 의미합니다).\nScheduler : task들을 worker 혹은 다른 머신에 할당하기 위한 스켸쥴러(task란 Ray에 사용되는 파이썬 함수 혹은 메소드로, Ray에 의해 스켸쥴링되는 단위).\nShared memory object store : 워커(worker)들간 객체를 효율적으로 공유하기 위한 공유메모리(copy 발생이 없는)\nInmemory database : 머신 실패(machine failure)와 같은 이벤트 상황에서 task들을 반환하기 위해 메타 데이터를 저장하는 데이터베이스\n\nRay worker는 thread가 아니며, thread와는 다른 개념의 process입니다.\nPython은 GIL(Global Interpreter Lock)으로 인해 multi-threding 지원에 한계가 있습니다.\n\nParallelism with Tasks\n@ray.remote라는 데코레이터를 함수 위에 선언해주는 것만으로 파이썬 함수를 Ray에서 실행 가능한 remote function으로 변경할 수 있습니다. \n\nremote function은 Ray의 프로세스에 의해 비동기적으로 실행됩니다.\n\n아래 예제와 같이 함수 f를 @ray.remote 데코레이터를 통해서 remote function으로 변경했다면, f.remote()를 호출해서 함수를 실행할 수 있습니다. 이때, 호출된 f.remote()는 즉각적으로 future를 반환하고 실제 함수의 실행은 백그라운드에서 진행됩니다.\n\nfuture는 나중에 반환될 함수의 출력값에 대한 참조입니다.\n\n아래 예제에서  f.remote()에 대한 호출이 즉시 반환되고 다음 remote function이 실행되기 때문에, 백그라운드에서 실행되는 f에 대한 4개의 복사본(task)은 단순히 해당 라인을 4번 실행하는 것으로 분산, 병렬로 실행할 수 있습니다.\n파이썬 함수 f를 “remote function”으로 바꾸기 위해서는 함수에 @ray.remote라는 데코레이터를 선언해줘야합니다. 그리고 함수를 f.remote()로 호출하면 즉시 future를 리턴합니다. 그리고 실제 함수의 실행은 백그라운드에서 실행됩니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\nimport ray\nimport time\n\n# Start Ray.\nray.init()\n\n@ray.remote\ndef f(x):\n    time.sleep(1)\n    return x\n\n# Start 4 tasks in parallel.\nresult_ids = []\nfor i in range(4):\n    result_ids.append(f.remote(i))\n    \n# Wait for the tasks to complete and retrieve the results.\n# With at least 4 cores, this will take 1 second.\nresults = ray.get(result_ids)  # [0, 1, 2, 3]\n\n\nTask Dependencies\ntask는 또 다른 task에 의존할 수 있습니다. \n아래 예제에서 multiply_matrices task는 두개의 create_matrix task의 결과를 사용합니다. 따라서 첫번째 두 task의 출력은 자동으로 세번째 task의 인자로 입력됩니다.\n결론적으로, 아래 예제를 실행해보면, multiply_matrices는 첫번째 두 task의 출력의 값이 반환되기 전까지는 실행되지 않습니다. \n이러한 방식으로 task들을 arbitrary DAG dependencies로 구성할 수 있습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\nimport numpy as np\n\n@ray.remote\ndef create_matrix(size):\n    return np.random.normal(size=size)\n\n@ray.remote\ndef multiply_matrices(x, y):\n    return np.dot(x, y)\n\nx_id = create_matrix.remote([1000, 1000])\ny_id = create_matrix.remote([1000, 1000])\nz_id = multiply_matrices.remote(x_id, y_id)\n\n# Get the results.\nz = ray.get(z_id)\n\n\nAggregating Values Efficiently\ntask 의존성을 잘 설계하면 효율적인 방식으로 작업을 수행할 수 있습니다.\n예를 들어 아래의 그림처럼 8개의 정수를 더한다고 생각해봅시다. \n매우 간단한 예제이지만, 실제로 이러한 형태로 큰 벡터를 통합하는 것은 애플리케이션에 큰 병목이 되기도 합니다. 이런 병목 지점에서 task 의존성을 잘 설계한다면, 단 한줄의 코드 변경으로 시간 복잡도를 선형 시간에서 로그메틱 시간으로 변경할 수 있습니다.\n\n(두 연산 그래프는 같은 결과를 반환하지만, 좌측 그림은 의존성 그래프의 깊이가 7이며, 우측 그림은 의존성 그래프의 깊이가 3입니다. 이 경우 우측 연산 그래프의 연산이 더 빠릅니다)\n위에서 설명한데로 하나의 task에서 생성된 output을 다른 task의 입력으로 사용하기 위해서는 첫번째 task로부터 반환받은 future를 두번째 task의 입력으로 넣으면 됩니다. \n이때, 두번째 task가 첫번째 task의 출력을 의존하고있으면 두번째 task는 첫번째 task가 끝나기 전에는 실행되지 않습니다.\ntask 의존성은 자동으로 ray의 스켸쥴러가 추적하고 관리하므로, 만약 분산환경일 경우, 첫번째 task의 출력은 자동으로 두번째 task가 있는 머신으로 보내져 실행되게됩니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\nimport time\n\n@ray.remote\ndef add(x, y):\n    time.sleep(1)\n    return x + y\n\n# Aggregate the values slowly. This approach takes O(n) where n is the\n# number of values being aggregated. In this case, 7 seconds.\nid1 = add.remote(1, 2)\nid2 = add.remote(id1, 3)\nid3 = add.remote(id2, 4)\nid4 = add.remote(id3, 5)\nid5 = add.remote(id4, 6)\nid6 = add.remote(id5, 7)\nid7 = add.remote(id6, 8)\nresult = ray.get(id7)\n\n# Aggregate the values in a tree-structured pattern. This approach\n# takes O(log(n)). In this case, 3 seconds.\nid1 = add.remote(1, 2)\nid2 = add.remote(3, 4)\nid3 = add.remote(5, 6)\nid4 = add.remote(7, 8)\nid5 = add.remote(id1, id2)\nid6 = add.remote(id3, id4)\nid7 = add.remote(id5, id6)\nresult = ray.get(id7)\n\n\n위의 코드는 명확합니다. 하지만, 이를 while loop를 통해 구현한다면 더 간결하게 구현할 수 있습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n# Slow approach.\nvalues = [1, 2, 3, 4, 5, 6, 7, 8]\nwhile len(values) > 1:\n    values = [add.remote(values[0], values[1])] + values[2:]\nresult = ray.get(values[0])\n\n# Fast approach.\nvalues = [1, 2, 3, 4, 5, 6, 7, 8]\nwhile len(values) > 1:\n    values = values[2:] + [add.remote(values[0], values[1])]\nresult = ray.get(values[0])\n\n\nFrom Classes to Actors\n클래스없이 좋은 애플리케이션을 만드는 것은 어려운 일입니다. 그리고 이는 분산환경에서도 마찬가지로 어렵습니다.\n클래스 데코레이터 @ray.remote를 사용하면 Ray에서 파이썬 클래스를 사용할 수 있습니다. 클래스를 인스턴스화하면 Ray는 새로운 액터(Actor)를 생성합니다. 액터는 분산환경 어딘가에서 실행되지만 객체의 복제본(object copy)을 유지하는 프로세스입니다. \n액터의 메소드를 실행하면 Ray는 해당 메소드를 액터 프로세스 위에서 작동하는 task로 변환합니다. 액터 프로세스 위에서 작동하는 task는 액터의 상태(state)에 접근이 가능하고 상태를 변경할 수 있습니다. 이러한 방법으로 액터는 액터의 상태값을 여러 task간 공유합니다.\n개별적인 액터는 메소드를 직렬로 실행하며(블럭킹), 액터의 메소드는 atomic 속성을 갖습니다. 따라서 race condition이 발생하지 않게됩니다. 액터를 이용한 병렬성은 다수의 액터를 생성하는 방식으로 구현합니다.\n아래 예제는 액터를 사용하는 간단한 예제입니다. Counter.remote()는 새로운 액터 프로세스를 생성합니다. \n액터 프로세스는 Counter 객체의 복사본을 갖으며,  c.get_value.remote()와  c.inc.remote()는 원격 액터 프로세스(remote actor process)에서 task를 실행하고 액터의 상태를 변경합니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n@ray.remote\nclass Counter(object):\n    def __init__(self):\n        self.x = 0\n    \n    def inc(self):\n        self.x += 1\n    \n    def get_value(self):\n        return self.x\n\n# Create an actor process.\nc = Counter.remote()\n\n# Check the actor's counter value.\nprint(ray.get(c.get_value.remote()))  # 0\n\n# Increment the counter twice and check the value again.\nc.inc.remote()\nc.inc.remote()\nprint(ray.get(c.get_value.remote()))  # 2\n\n\nActor Handles\n위에서 우리는 파이썬의 메인 스크립트에서 액터의 메소드를 실행하는 예제를 살펴봤습니다. \n액터의 강력한 장점은 핸들(handle)을 액터에 전달할 수 있는 것입니다. 이는 다른 액터나 다른 task가 동일한 액터의 메소드를 호출할 수 있게 해줍니다.\n아래 예제는 메세지를 저장하는 액터를 생성합니다. 몇몇의 worker task는 반복적으로 messages를 액터로 푸쉬합니다. 그리고 파이썬 메인 스크립트는 주기적으로 이 메세지를 읽습니다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n\nimport time\n\n@ray.remote\nclass MessageActor(object):\n    def __init__(self):\n        self.messages = []\n    \n    def add_message(self, message):\n        self.messages.append(message)\n    \n    def get_and_clear_messages(self):\n        messages = self.messages\n        self.messages = []\n        return messages\n\n# Define a remote function which loops around and pushes\n# messages to the actor.\n@ray.remote\ndef worker(message_actor, j):\n    for i in range(100):\n        time.sleep(1)\n        message_actor.add_message.remote(\n            \"Message {} from worker {}.\".format(i, j))\n\n# Create a message actor.\nmessage_actor = MessageActor.remote()\n\n# Start 3 tasks that push messages to the actor.\n[worker.remote(message_actor, j) for j in range(3)]\n\n# Periodically get the messages and print them.\nfor _ in range(100):\n    new_messages = ray.get(message_actor.get_and_clear_messages.remote())\n    print(\"New messages:\", new_messages)\n    time.sleep(1)\n\n# This script prints something like the following:\n# New messages: []\n# New messages: ['Message 0 from worker 1.', 'Message 0 from worker 0.']\n# New messages: ['Message 0 from worker 2.', 'Message 1 from worker 1.', 'Message 1 from worker 0.', 'Message 1 from worker 2.']\n# New messages: ['Message 2 from worker 1.', 'Message 2 from worker 0.', 'Message 2 from worker 2.']\n# New messages: ['Message 3 from worker 2.', 'Message 3 from worker 1.', 'Message 3 from worker 0.']\n# New messages: ['Message 4 from worker 2.', 'Message 4 from worker 0.', 'Message 4 from worker 1.']\n# New messages: ['Message 5 from worker 2.', 'Message 5 from worker 0.', 'Message 5 from worker 1.']\n\n\nRay의 액터는 매우 강력합니다. 액터는 파이썬의 클래스를 가져와서 다른 액터와의 작업 혹은 다른 애플리케이션에 질의할 수 있는 마이크로 서비스로 인스턴스화할 수 있습니다.\ntask와 액터는 Ray가 제공하는 핵심적인 추상입니다. 이 두 가지 개념은 매우 일반적이면서 정교한 애플리케이션 구현에 사용할 수 있습니다.\nRay는 딥러닝에 사용되는 정교한 애플리케이션 중 하나인 분산 강화학습, 하이퍼파라미터 튜닝 도구, 가속화된 판다스를 제공하니 한번 살펴보시기 바랍니다.\nReference\n\nModern Parallel and Distributed Python: A Quick Tutorial on Ray","link":"http://ssaru.github.io/2020/08/27/20200827-A_Quick_Tutorial_on_Ray/","title":"(번역) Modern Parallel and Distributed Python-A Quick Tutorial on Ray","pubDate":"2020-08-27T02:31:30.000Z"}},{"node":{"contentSnippet":"들어가며\n이제 2019년이 가고 2020년이 되었다.\n지난 2019년 초에 많은 계획들을 세웠던 것 같은데, 예상치 못한 일들로 다사다난했던 해였다.\n2020년을 맞이하여 2019년 회고하고 2020년 계획을 다짐하는 포스팅을 작성한다.\n2019년 계획했던 일들\n2019년을 시작하면서 그때의 상황에 맞춰 여러 계획들을 많이 세웠다.\n큰 카테고리로 나누어보자면 아래와 같이 나눌 수 있다.\n\n건강\n경제적 자립\n업무와 연관된 학습\n업무와 연관되지 않은 비-기술적인 학습\n\n1. 건강 👎\n일을 오래 잘하려면 체력이 좋아야하고 건강한 것이 필수이다. 2019년엔 그에 맞춰 아래와 같은 계획을 세웠었다.\n\n삼시세끼 잘 먹을 것 👎\n근력, 근지구력 열심히 키울 것 👎\n규칙적인 수면 습관을 들일 것 👎\n\n성향상 아침 잠이 많은 편이고 점심, 저녁은 회사 업무를 하다보면 잘 챙겨먹는 편이었던 나는 세 가지를 한꺼번에 잡기 위해서 규칙적인 생활 아래에서 이른 기상으로 아침 식사와 운동을 잘 잡으려 노력했다. 운동은 원래 즐겨하던 비보이 연습과 회사에서 헬스를 병행하는 방식으로 했다. 2월 달을 기점으로 살짝 흔들렸던 것 같고 본격적으로 9월부터는 전혀 해당 목표에 대해서 관리하지 못했다.\n2. 경제적 자립 👌\n이제는 나이도 나이인지라 미래의 나를 위해 경제적인 부분에서 자립할 수 있을 정도로 스스로 지출 설계 및 소비 패턴 제어가 필요했다. 이를 위해 아래와 같은 작업을 했다.\n\n고정 지출 분석 👍\n데일리 로그에서 데일리 지출 기록 👍\n매달 말 지출 내역 분석 및 회고 👎\n\n이 또한 9월까지는 순조롭게 잘 관리했고 년초에 목표했던 금액에 80%를 달성할 정도로 근사한 달성률을 보였다. 9월 이후에는…. 👀\n3. 업무와 연관된, 연관되지 않은 학습 😭\n항상 언제든지 도태될 수 있다는 생각을 달고 살았던지라 자기계발을 놓고 살지는 않았다. 2019년에는 아래와 같은 내용으로 자기계발 내용을 조금 더 구조화하고 체계적으로 바꿀까했다.\n\n1주일 1개 paper 리딩 후 TIL 👍\n한달에 한권씩 책읽고 후기 남기기 👎\nObject Detection 구현 모임 👎\n경량화 네트워크 프로젝트 진행 👍\n공인인증 영어성적 획득 👎\n\n1주 1일 Paper\n현실감각이 없었던 계획이었지만, 연구직이라는 업무 특성과 대학원 수업에서 강제적인 논문 발표 수업으로 인하여 논문을 꽤나 읽게되어 나름 만족으럽게 80%의 달성률을 보였다. 읽은 논문 리스트는 아래와 같다.\nSqueezeNet / BinaryConnect / Binarized Neural Network / XNOR Net / EfficientNet / EfficientDet / Trained CNNs are biased towards texture / rafiqi  / TFX / TrIMS / Clipper / 기타 헬스케어 관련 논문 20편 내외\n경량화 네트워크 프로젝트\n9월달 대학원에 입학하자마자 석사 1기생들과 팀을 꾸려 인공지능 수업에서 BinaryConnect와 Binarized Neural Network를 구현하는 프로젝트를 진행하였다. Binarized Neural Network까지 구현을 하지는 못하였지만 BinaryConnect까지는 진행 완료하게 되었다.\n2019년 계획하지 않았던 일들..\n2019년 초에 세웠던 계획과 다르게 삶에 터닝 포인트가 되거나 여러 사건들이 많이 생기게 되었다. 년초에 세웠던 계획들이 크게 틀어지게된 계기는 대학원 입학과 여러 새로운 업무들이 제일 컸다.\n\n성남시 빅데이터 이노베이션 해커톤 부분 최우수상 수상\nEO-Detection 대회 준비\n시흥 시청 빅데이터 분석 주강사, 보조강사\n대학원 입학\n하이퍼커넥트 CS기초 스터디\nMML-Book 스터디\n여러 새로운 업무들\n\n2019년을 보내며..\n2019년에 여러 일들을 겪으며 내가 얻게된 화두의 키워드는 아래와 같다.\n\n함께 자라기\n의연하게 대처하기\n잘 예측하고 책임감 있게 수행하기\n감사하기\n꾸준하기\n\n함께 자라기\n지금까지는 호흡이 맞는 사람들과 프로젝트를 했다면 2019년에는 회사 업무와 학교 프로젝트에서 호흡을 맞춰 보지 않았던 사람들과 호흡을 맞췄다. 호흡을 맞추던 과정에서 당연히 알 것이라고 생각했던 것들을 사람들이 모를 수 있다는 것을 깨달았다. 덕분에 예상하지 못했던 부분에서 많은 시간을 투입하게 되었고 스스로가 병목이 되는 경험을 해볼 수 있었다. 다음번부터는 팀원들의 능력에 따른 예상시간을 추정해보고 그에 따라서 업무를 진행해보려고 한다. 또한 스스로 병목이 되는 구간에 대해서는 어디까지 병목을 허용하고 허용하지 않을지에 대한 지점을 찾아볼까 한다.\n의연하게 대처하기\n2019년 9월부터 대학원과 회사 업무를 병행하게 되었다. 대학원은 SoC연구실로 입학하게 되었고 회사에서는 인공지능 모델 서빙을 위한 클라우드 연구에서 마이크로 아키텍쳐 서비스 시스템에 GPU를 연동하기 위한 설정과 마이크로 아키텍쳐 서비스에 사용되는 컨테이너 변경하는 작업, 마이크로 아키텍쳐 시스템을 시연하는 여러 데모 프로그램을 작성하게 되었다. 그 과정에서 개인적으로 힘듦을 느껴 주변에 스스로의 상태에 대해 많이 토로했던 것 같다. 새해가 지나고 천천히 돌아보니 나뿐만 아니라 모두가 그런 일상을 보내고 있었다라는 것을 깨닫고 부끄러웠다. 계속 경험하다보면은 자연스러워지겠지만 당분간은 의식적으로 마음의 여유를 잘 찾아서 의연함을 잃지 않게 노력해보려고 한다.\n잘 예측하고 책임감 있게 수행하기\n2019년도 과제를 마무리하기 위해서 회사에서 9월부터 12월까지 여러 작업을 진행하면서 순간순간 예측을 벗어나는 일들이 많았던 것 같다. 예측을 크게 벗어나 몇번 할당된 업무를 제때 수행하지 못할 뻔한 위기상황을 많이 겪었는데, 프로젝트를 관리하는 측면에서 나의 이러한 상황은 굉장히 불안정해보일 수 있다는 점을 깨달았다. 이는 년초에 추정했던 업무량을 상회해서 발생했다라고 판단하고 있고, 2019년도에 크게 한번 겪어봤으니 2020년에는 추가적인 업무량과 예상되는 업무량 그리고 내가 수용가능한 업무량을 잘 조율하여 업무를 매끄럽게 수행해볼 수 있게 노력해보고자 한다.\n감사하기\n2019년에 자율차 플랫폼 연구부서에서 현재 부서로 이동하고나서 정말 다양한 경험을 하게되었다. 대학원을 병행하면서 갈증이 있던 지식을 수업을 통해서 해소했고 해보고싶었던 연구도 즐겁게 했다. 더 중요한 것은 할당된 업무를 혼자 독립적으로 충분히 업무를 진행할 수 있다는 자신감이다. 이런 유의미한 경험들이 다 주변 사람들이 믿어주고 기회와 충분한 시간을 주었던 덕분이 아닐까 싶다. 이런 좋은 환경에서도 상황이 급박해져서 불만을 종종 토로하곤 했는데 이런 행동은 스스로에게 그렇게 도움이되는 태도는 아니라고 생각한다. 2020년에는 조금 더 감사히 내게 주어진 환경에서 즐겁고 열심히 최선을 다하는 한해가 되었으면 한다.\n꾸준하기\n비록 년초에 새웠던 공부 일정은 달성하지 못했지만 몇가지 꾸준히 진행했던 프로젝트 및 스터디는 유의미한 결과를 얻어 내년에도 지속할 수 있는 결과들을 얻었다. 1~2년을 꾸준히 진행했던 덕분이 아닐까 싶다. 생각 외로 스터디에서 진행했던 공부들이 업무에 직접적으로 또는 간접적으로 크게 영향을 많이 준다는 것을 깨달았는데, 2020년에는 더 깔끔하게 정제되게 꾸준히 하는 스터디를 진행해보았으면 하는 바람이다.\n2020년을 맞이하며..\n2020년에도 2019년과 다를게 없이 관심사는 아래와 같이 같은 카테고리로 나뉘어질 듯 하다.\n\n건강\n경제적 자립\n업무 관련 학습\n기타 학습\n\n건강\n9월~12월 동안 급격하게 흘러갔던 업무 상황으로 운동도 내팽겨친채 밤새 일했던 시간이 많았다. 그 결과로 12월 말에 몸이 2~3주 동안 계속 아팠는데 체력적으로 많이 부족하다는 생각이 들었다.\n\n오전에 할 수 있는 운동 진행(크로스핏, 수영, 등등)\n\n경제적 자립\n분당으로 이사오고 나서 초기에 세웠던 경제적 목표가 크게 틀어졌다. 2020년에는 더 긴장하고 경제적 목표를 채워볼까 한다.\n\n고정 지출, 고정 수입 확인\n가계부 작성\n매달 가계부 분석 및 회고\n\n업무 관련 학습\n2020년은 인공지능 모델 서빙관련 프로젝트가 종료되는 해이다. 이번년도는 더 탄탄해진 업무 프로세스와 함께 내가 접근해야하는 기술 스택이 더 많아졌는데 시간이 된다면 아래와 같은 학습을 더 해볼까 한다. 추가적으로 대학원 졸업을 위해서 하드웨어관련 지식을 더 학습할 예정이다.\n\nGolnag\n쿠버네티스\ngRPC\nVerilog\nFPGA\nTDD\n\n기타 학습\n이번년도는 대학원 졸업 및 미래의 이직을 위한 여러가지 학습을 진행하려고 한다. 따라서 협업을 위한 스킬이라던가 면접을 위한 공부와 코딩 테스트를 위한 공부를 추가적으로 진행할 예정이다.\n\n자료구조/알고리즘\nOS\n디자인패턴\n개발자 교양도서 독서 모임\nEO-Detection 대회","link":"http://ssaru.github.io/2020/01/05/20200105-2019_retrospective/","title":"2019년 회고","pubDate":"2020-01-05T14:45:55.000Z"}},{"node":{"contentSnippet":"이번 포스팅은 논문 ImageNet - Train CNNs are biased towards texture; increasing shape bias improves accuracy and robustness을 읽고 읽은 내용을 포스팅한다.\n\n무엇이 맞는 말일까??\n컨볼루션 뉴럴 네트워크(Convolutional Neural Network)가 무엇에 편향되는지 바라보는 두 가지 관점\n형상 정보 가설과 질감 정보 가설(Shape Hypothesis vs Texture Hypothesis)\n형상정보 가설(Shape Hypothesis)\n많은 사람들은 컨볼루션 뉴럴 네트워크(Convolutional Neural Network; 이하 CNN)가 “레이어가 깊어질 수록 저-레벨(low level)의 특징(feature)을 조합해서 고-레벨(high level) 특징(feature)을 만들고, 이 특징들을 이용하여 객체인식을 한다.” 이라는 직관에 동의할 것이다.\n\n\n[그림 1] ZF-Net으로 시각화한 layer들의 특징\nCNN이 객체 분류를 어떻게 하는지에 대한 공통된 해석은 여러 문헌에서도 나타나는데 Kriegeskorte과 LeCun은 그들이 저술한 논문 혹은 아티클에서 아래와 같이 언급했다.\n\nCNN은 각 카테고리(강아지, 고양이)가 가지고있는 고유한 패턴과 같은 지식을 얻는다.  … (중략) … 고-레벨(high level)의 특징들은 실제 영상(생성되거나 조작되지 않은 영상)에서 나타나는 형상(shape; 이하 형상) 정보를 배우는 것 처럼 보인다.\nCNN의 중간 레이어들은 같은 카테고리의 객체들에게서 유사한 부분을 인식한다. …(중략) … 객체를 검출하는 것은 이러한 부분들의 조합이다.\n\n이렇게 CNN이 형상 정보를 학습한다는 주장을 형상 정보 가설 (Shape Hypothesis)부른다.\n형상 가설은 수많은 실험을 통해서 지지를 받는데, 대표적으로 ZF-Net이 있다. ZF-Net은 모델의 레이어로부터 특징들을 추출한다. 그 후 역-컨볼루션(De-Convolution)이라는 연산을 통해서 추출한 특징을 시각화한다. [그림 1]은 ZF-Net에서 추출한 특징을 시각적으로 보여준다.\nKubilius라는 연구자는 CNN을 사람의 시각 인지 모델로써 제안했으며 Ritter는 CNN이 아이들과 유사하게 형상 정보를 개발해나간다는 것을 밝혀냈다. Ritter가 이야기하는 형상 정보를 개발한다는 것의 의미는 색감(Colour) 정보 보다는 형상 정보가 객체 분류를 하는데 더 중요한 역할을 한다는 것을 의미한다. \n실제로 형상 정보 가설은 사람의 인지 체계와 매우 비슷하기도 하다. 사람은 모양이 바뀌면 다른 카테고리로 분류하지만 질감 및 크기가 변화하더라도 같은 모양이면 같은 객체 카테고리로 인식한다. 이는 해당 연구 결과를 통해서 확인되었다.\n\n[그림 2] 사람은 질감과는 관계없이 모양에 따라 같은/다른 물체로 인식한다.\nSummary\n\n형상정보 가설(Shape Hypothesis)는 CNN이 객체의 거시적인 형상을 보고 인식을 한다는 가설이다.\n\n가설을 지지하는 실험들이 여러 논문들을 통해서 확인되었다.\n  ZF-Net, A Shape Bias Case Study, \n\n이러한 형상정보 가설은 사람의 인지체계와도 제일 유사한 측면을 갖는다.\n\n질감 정보 가설(Texture Hypothesis)\n몇몇 연구 결과들은 형상 정보 가설과 상반되는 결과를 얻기도 했다. 대표적으로 연결이 부분적으로 끊어진 네트워크에서 학습된 모델은 질감 정보(Texture; 이하 질감)가 더 중요한 역할을 한다는 연구가 있다. 해당 연구에서는 CNN이 전반적인 형상 정보가 없어도 질감 정보를 이용해서 영상을 잘 분류할 수 있다는 것을 증명한다. 오히려 질감 정보가 없는 형상 정보(스케치 그림)만으로 학습한 CNN은 나쁜 인식 성능을 갖는다.\n\n[그림 3] CNN의 질감 정보만을 이용한 분류 예시\n\n\n[그림 4] 질감 정보가 없는 스케치 데이터로만 학습했을 때, 새(bird) 데이터를 얼마나 많이, 자주 틀리는지에 대한 예시\n두 가지 결과는 질감 정보와 같은 지역적인 정보(Local feature)만을 이용해서 객체 인식 문제를 충분히 해결할 수 있다는 것을 시사한다. 이를 증명한 연구결과는 여기에서 확인할 수 있다. \n질감 정보의 중요성을 이야기하는 또 다른 연구에서는 질감 정보를 학습하고 특정 질감 정보를 생성하는 네트워크를 만들었다. 이렇게 질감 정보를 종류별로 학습하고 생성한다는 것은 모델이 세 가지의 기능을 수행한다는 의미를 갖는다.\n\n종류별 질감 정보를 분류한다.\n종류별 질감 정보의 분포를 학습한다.\n특정 질감 정보의 분포를 생성 한다.\n\n해당 네트워크는 분류(Classification)를 하는 네트워크가 아니다. 하지만 질감 정보를 종류별로 생성한다는 것은 “질감 정보의 분포가 서로 다르기 때문에 질감 별로 분리해서 학습할 수 있다”는 것을 내포한다. 이런 맥락에서 논문의 저자는 질감 정보만으로 객체 분류 문제를 풀 수 있다고 이야기하는 듯 하다.\n\n[그림 5] 질감정보를 생성하는 네트워크\n그 외에도 다른 연구에서는 최대 리셉티브 필드의 크기에 제한을 둔 BagNet을 설계했다. BagNet은 최대 리셉티브 필드 크기의 제한으로 CNN이 국부적인 정보만 볼수 있게끔 시야가 제한이 되었다. 그럼에도 불구하고 ImageNet 데이터에서 놀라울 정도로 높은 정확도를 갖는 결과를 얻었다. 이러한 결과들을 보았을 때 국부적인 질감 정보는 객체 분류를 하는데 충분한 정보를 가지고 있음을 알 수 있다. CNN은 이런 질감 정보을 추론하는데 이를 질감 정보 가설(Texture Hypothesis)라고 부른다.\n\n[그림 6] 최대 리셉티브 필드의 크기가 제한된 Bag Net의 최종 Layer에서 확인된 Feature들\nSummary\n\n질감정보 가설(Texture Hypothesis)는 CNN이 객체의 국부적인 질감정보를 보고 인식을 한다는 가설이다.\n가설을 지지하는 실험들이 여러 논문을 통해서 확인되었다.\n\n지금까지 형상 정보 가설(Shape Hypothesis)와 질감 정보 가설(Texture Hypothesis)를 살펴보았다. 이렇게 상반되는 두 가지 가설을 해결하는 것은 인공지능 커뮤니티와 뇌 과학자 커뮤니티 모두에게 중요하다. \n해당 논문에서는 StyleGAN을 이용하여 질감-형상 정보가 모순된 이미지(Cue Conflict)를 만들었다. 이를 이용하면 CNN과 사람의 시각 능력에 대해서 정량적으로 측정할 수 있게 된다. 이렇게 만든 질감-형상 정보가 모순된  이미지(Cue Conflict)를 이용해서 총 97명의 실험 참가자와 함께 9종류의 정신 물리학 실험(Psychophysical Experiments) 을 진행하였다. 총 실험의 횟수는 48,560번이었다.\n본 논문의 기여는 아래와 같다.\n\nCNN과 사람의 인지 차이를 확인함\nCNN의 편향의 변경할 수 있음을 확인함\n편향의 변경으로 생기는 효과를 확인함\n\n실험\nCNN과 사람의 시각 시스템의 차이를 확실하게 확인하기 위해서 해당 논문에서는 여러가지 실험을 진행했다.\n\n\nCNN과 사람의 시각 시스템은 서로 어떻게 다를까?(Psychophysical Experiments)\n데이터셋\n모순된 영상을 보여준다면? (Cue Conflict)\n실험 결과\nCNN이 사람과 비슷하게 보게 하려면? 비슷하게 보게 된다면?\n\nCNN과 사람의 시각 시스템은 서로 어떻게 다를까? (Psychophysical Experiments)\nCNN과 사람이 어떻게 물체를 인식하느냐(질감 정보를 기반으로 인식하느냐? vs 형상 정보를 기반으로 인식하느냐?)를 확인하기 위해서 정신 물리학(Psychophysical Experiments; 이하 정신 물리학) 실험을 진행하였다.\n실험 내용은 16-Class-ImageNet이라는 데이터 셋을 사람과 CNN에게 똑같이 보여주고 이미지 분류 작업 결과를 확인하였다. 실험 참가자는 총 97명이었으며 16-Class-ImageNet의 데이터수는 49K(49,000)였다.\n사람과 CNN이 본질적으로 큰 차이가 있기 때문에 실험은 굉장히 신중하게 설계 되었어야 했다. CNN은 단일의 영상 정보를 한번만 네트워크에 입력하지만 사람의 시각 시스템은 연속된 정보(동영상)을 획득하고 뇌에서는 이를 기반으로 각 신경끼리 피드백을 주고받는다. 따라서 단순하게 영상을 보여주고 분류하는 실험은 CNN과 사람에게 단 한장의 영상만 보고 분류를 한다라는 실험 조건에서 단 한장이 서로 다를 수 있다.\n실험을 최대한 공평하게 진행하기 위해서는 사람과 CNN의 실험 조건을 같게 설정 해야했다. 사람의 시각 시스템에서 발생하는 피드백은 영상을 제공하는 순서와 타이밍을 조절하여 최소화하였다. 이러한 방법은 이미 정신 물리학적 실험에서는 잘 알려진 사실이라고 논문에서는 언급한다. 하지만 의학적인 참고자료가 제시되어있지 않아서 더 자세한 근거는 확인하지 못했다.\n\n200ms 동안 분류작업을 할 영상을 보여준다.\n300ms 동안 고정된 사각형 이미지를 보여준다.\n1/f 스펙트럼을 가진 노이즈 마스크를 200ms동안 보여준다.\n\n\n[그림 7] CNN과 사람의 시각 시스템의 비교\n데이터 셋\n해당 연구의 실험에서는 사람과 CNN이 서로 어떤 가설을 기반으로 인지를 하는지 확인하는 것이 목적이므로 16-Class ImageNet 데이터를 왜곡하여서 진행하였다. \n\nOriginal\nGreyScale\nSilhouette\nEdges\nTexture\nCue Conflict (모순된 영상)\n\n\n[그림 8] 16-Class ImageNet 예시\n모순된 영상(Cue Conflict)\n기본적인 이미지 왜곡 외에도 서로 다른 형상 정보와 질감 정보가 섞여 있어 모순을 일으키는 Cue Conflict 영상으로도 실험을 진행하였다.\n\n[그림 9] Silhouettes를 이용한 Cue Conflict 영상(위에서 3번째)과 Style transfer를 이용한 Cue Conflict 영상(위에서 네 번째)\nCue Conflict 영상을 만들기 위해서 두 가지 방법을 적용하였다.\n\n세그멘테이션 맵을 이용해 다른 영상 정보를 혼합하는 방법(filled silhouettes)\nStyle GAN을 이용하여 영상 정보를 혼합하는 방법(style transfer)\n\n연구자들이 막상 이러한 영상을 만들고 나니까 문득 든 생각이 해당 영상들이 실제 형상 정보와 질감 정보가 완전히 모순 되어 있다는 것을 어떻게 증명할 수 있을까? 였다. 이를 증명하기 위해서 연구자들은 앞서 언급했던 Bag Net를 사용하였다.\nBag Net는 최대 리셉티브 필드의 크기를 제한하여 CNN이 이미지의 전체 형상을 보지 못하게 만든 모델이다. 국부적인 특징(local feature)들만 이용했음에도 불구하고 BagNet은 굉장히 좋은 성능을 나타내었는데 만약 국부적인 특징(Texture)이 전체적인 특징(Shape)과 모순되어있다면(Cue conflict) Bag Net의 성능이 급격히 하락할 것이다.\n실제로 Bag Net을 이용하여 Cue conflict 영상을 추론해본 결과 기존 ImageNet 데이터에서 성능이 잘 나오던 모델이 약 85% 정도의 성능 하락 발생한 것을 할 수 있었다.(ImageNet, 70.0% top-5 accuracy → Cue conflict, 10.0% top-5 accuracy)\n이를 토대로 생성된 Cue conflict 영상이 형상 정보와 질감 정보가 모순되어 있다는 것을 확인할 수 있었다.\n실험 결과\n실험 결과 사람과 CNN이 물체를 분류하는 작업에서 큰 차이를 볼 수 있었다. 사람은 굉장히 형상 정보에 편향되어 있고, CNN은 질감 정보에 편향되어 있음을 확인할 수 있었다.\n이는 고양이 형상에 코끼리 가죽 질감 정보가 섞여있는 Cue conflict 영상을 사람과 CNN에게 보여주었을 때, CNN은 이를 “코끼리”로 사람은 이를 “고양이”로 인식한다는 이야기가 된다.\n\n[그림 10] CNN과 사람의 시각 시스템의 차이. 왼쪽은 형상 정보 편향을 의미하고 오른쪽으로 질감 정보 편향을 의미한다. 해당 그림에서 사람은 형상 정보에 편향 되어있음을 CNN은 질감 정보에 편향 되어있음을 확인할 수 있다.\nCNN이 사람과 비슷하게 보게 하려면? 비슷하게 보게 된다면?\n연구자들은 이제 CNN이 사람과 비슷하게 형상 정보에 편향되면 어떻게 될까?가 궁금해졌다. 그래서 기존의 IN(ImageNet) 데이터와 SIN(Sytle transfered ImageNet)데이터를 이용하여 학습을 진행하였다.\n학습 방법에 대해서는 다양한 실험을 진행했다\n\nIN 학습 → IN으로 평가\nIN 학습 → SIN으로 평가\nSIN 학습 → IN으로 평가\nSIN 학습 → SIN으로 평가\nIN과 SIN을 혼합해서 학습 → IN으로 평가\nIN과 SIN을 혼합하여 학습 후 IN으로 파인튠 → IN으로 평가\n\n학습 후에는 모델이 효과적으로 형상 정보 편향이 되었는지 확인하기 위해 16-Class-ImageNet 데이터를 이용하여 이를 확인하였다. 전부는 아니지만 많은 클래스에 대해서 CNN이 질감 정보 편향에서 형상 정보 편향으로 이동 하였음을 확인할 수 있었다.\n\n[그림 11] IN/SIN 데이터를 학습한 모델과 사람의 차이. 데이터를 혼합해서 학습한 모델이 그림 10. 과 비교했을 때 상대적으로 형상 정보 편향으로 이동되었음을 확인할 수 있다.\n결론적으로 해당 실험에서 SIN 데이터(형상 정보 편향)만으로는 성능을 더 개선시킬 수는 없었다. 하지만 IN 데이터와 SIN데이터를 혼합해서 학습하는 경우에는 ImageNet 데이터 셋만 이용해서 학습한 것보다는 성능이 더 좋아진다는 것을 확인하였다. 또한 형상 정보 편향이 된 CNN을 이용해 객체 검출(Object Detection) 모델에 전이 학습(Transfer learning)했을 때, 기존의 객체 검출 모델보다 성능이 더 개선 되었음을 확인할 수 있었다. 이는 SIN데이터를 혼합해서 학습하는 것이 모델의 일반화(Generalization)에 더 기여한다고 해석할 수 있다.\n\n[그림 12] IN데이터와 SIN 데이터를 이용한 CNN 학습 결과\n그 외에 노이즈를 이용한 영상 왜곡 실험을 추가로 진행했다. 이 경우에도 형상 정보에 편향된 CNN이 노이즈 왜곡에도 모델이 더 강인해짐을 확인할 수 있었다.\n\n[그림 13] 추가 영상 왜곡 실험에서 사용한 노이즈 예시\n\n[그림 14] 형상 정보, 질감 정보에 편향된 CNN이 노이즈 왜곡에 따라 나타내는 성능\n요약\n\n기존의 CNN이 이미지 분류를 할 때, 형상 정보를 기반으로 인식을 한다는 형상 정보 가설(Shape Bias Hypothesis)와 질감 정보를 기반으로 인식한다는 질감 정보 가설(Texture Bias Hypothesis)가 존재했다.\n\n사람과 CNN이 영상 데이터를 어떤 관점에서 분류하는지 확인하기 위해 정신 물리학 실험을 수행했다. 해당 실험을 통해서 사람은 형상 정보에 편향 되어있고, CNN은 질감 정보에 편향 되어  있음을 확인할 수 있었다.\n\n모델을 SIN/IN 데이터를 혼합하여 학습하면 모델을 형상 정보로 편향시킬 수 있다. 형상 정보로 편향된 모델이 기존 IN 데이터로만 학습된 모델보다 더 좋은 성능을 나타냄과 동시에 전이 학습(Transfer learning)에서도 성능 개선이 유효함을 확인할 수 있었다. 따라서 본 논문에서 제안한 SIN/IN 데이터 학습이 인공지능 모델을 일반화(Generalization) 시키는데 기여한다라고 이야기할 수 있다.\n\n참고자료\n\nIMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS\n\nThanks to\n\n정미연\n김형섭\n권해용\n김철환(markkch@naver.com)\n김보섭","link":"http://ssaru.github.io/2019/08/06/20190806-How_the_Eye_of_AI_Sees/","title":"How the Eye of A.I Sees","pubDate":"2019-08-06T07:23:55.000Z"}},{"node":{"contentSnippet":"Abstract\n현재 쿠버네티스(Kubernetes, 이하 쿠버네티스)를 이용하여 딥러닝을 지원하는 마이크로 아키텍쳐(Micro Architecture, 이하 마이크로 아키텍쳐) 프레임워크 개발 및 연구를 진행 중이다.\n일반적으로 개발 및 연구 프로젝트의 원활한 진행을 위해서는 개발한 기능을 테스트하기 위한 개발 환경 구축이 필요하게 된다. 필자는 이전에 개발한 기능을 테스트하기 위해서 쿠버네티스 기반으로 작성되어있던 프레임워크를 미니쿠베(minikube, 이하 미니쿠베)에서 구동시키고 테스트하는 작업을 수행했었다.\n이번에는 GPU관련 기능을 테스트하기 위해서 GPU 컨테이너 사용이 가능하게끔 미니쿠베를 설정하는 작업을 했으나 정확하지 않은 공식 문서와 인터넷 상에 알파, 베타 버전등 파편화된 가이드 문서로 인해 미니쿠베에서 GPU 인식이 안되어 어려움을 겪었다.\n본 포스팅은 아래 세 가지를 전달하기 위해 작성되었다.\n\nGPU 컨테이너 사용을 지원하는 미니쿠베 설정 방법 소개\n순서 별로 작업 시 해당 작업이 잘 되었는지 테스트하는 방법\n몇 가지 에러와 해결 방법\n\nIntroduction\n필자는 쿠버네티스를 이용하여 딥러닝을 지원하는 마이크로 아키텍쳐 프레임워크 개발을 하고있으며 사내에서는 서비스 혹은 테스트용으로 GPU 컨테이너 사용을 지원하는 마이크로 아키텍쳐 프레임워크가 구축되어있다.\n여러 명의 개발자와 협업하여 프로젝트를 진행 하다보면 일반적으로 따르는 프로세스가 있다.\n\n개발자가 기능 개발 수행\n개발 환경에서 개발된 기능을 테스트하고 이상이 없음을 확인한다\n기능 작동 및 테스트가 완료 되면 해당 기능은 프로젝트에 병합한다\n\n이 때, 개발자의 개발 환경 혹은 테스트 환경은 서비스 배포 환경과 유사하게끔 설정되어야한다. 필자의 프로젝트가 딥러닝을 지원하는 마이크로 아키텍쳐 프레임워크 개발이기 때문에 필자의 개발 환경은 GPU 컨테이너를 지원하는 미니쿠베 환경을 갖춰야 했다.\nRelated Work\n미니쿠베의 GPU 지원 문서$^{[1]}$를 확인해보면 미니쿠베가 GPU를 지원하는 방법은 크게 두 가지로 나뉘어진다.\n\nkmv2 가상환경을 이용해서 지원 (--vm-driver=kvm2)\n호스트의 운영체제 환경을 이용해서 지원 (--vm-driver=none)\n\nkvm2 가상환경(--vm-driver=kvm2)\nkvm2환경에서 GPU사용이 가능한 미니쿠베 설정 방법을 소개한다. 이 과정은 기존에 운영체제에 연결되었던 GPU 장치를 kvm2 가상환경에 직접적으로 연결(GPU Passthrough)하는 작업들을 포함$^{[2-9]}$하고 있기 때문에 과정이 복잡하다.\n필자의 경우 해당 방법이 두 가지 사항을 요구했기 때문에 선택하지 않았다.\n\n기존 nvidia 드라이버에 연결되어있던 GPU 연결 해제\nnvidia 그래픽 드라이버 삭제 필요\n\n필자의 업무는 마이크로아키텍쳐 프레임워크 개발 외에도 인공지능 모델 개발도 있다. kvm2 환경에서 요구하는 두 가지 사항은 cuda와 cudnn 등 인공지능 모델 개발을 위한 환경설정을 작동하지 않도록 만든다.\n필자가 kvm2환경을 사용하게 되면 업무 별로 nvidia 그래픽 드라이버 및 의존 패키지를 설치하고 삭제하는 작업을 반복적으로 수행해야한다. 이런 이유로 필자는 kvm2 환경을 선택하지 않았다.\n호스트 운영체제 환경(--vm-driver=none)\n필자는 공식 문서를 믿고 이를 따라서 미니쿠베 설정을 진행하였다. 적용 결과 기존에 작성된 마이크로 아키텍쳐  프레임워크에서 GPU를 사용하는 파드(Pods)가 무한 대기(pending)에 빠지는 현상이 발생했다.\n확인 결과 미니쿠베에서 GPU 디바이스를 인식하지 못했거나 GPU 디바이스는 인식했으나 관련 드라이버 및 패키지가 존재하지 않아서 일어난 일이였다.\n미니쿠베, 엔비디아-도커의 세부사항을 몰랐던 필자는 방대한 구글을 헤엄칠 수 밖에 없었다$^{[10-17]}$.\nMethod\nPrerequisite\n본 포스팅에서는 몇가지 준비 되어야하는 컴퓨터 환경과 작업이 있다. 필자는 아래의 패키지와 Ubuntu 18.04 환경에서 작업하였다.\n\nDocker-CE ( ≥ 18.09)\n\nNvidia-Docker( ≥ 2.03)\n\nNvidia GPU를 장작한 컴퓨터\n\nNvidia 그래픽 드라이버 설치\n\nMinikube(≥ 1.2.0)\n\n\n\n\nNvidia-Graphics-Driver\n  $ nvidia-smi  Fri Jul 19 21:53:48 2019         +-----------------------------------------------------------------------------+  | NVIDIA-SMI 390.116                Driver Version: 390.116                   |  |-------------------------------+----------------------+----------------------+  | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |  | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |  |===============================+======================+======================|  |   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |  | 33%   30C    P8    24W / 250W |    599MiB / 11175MiB |      1%      Default |  +-------------------------------+----------------------+----------------------+  |   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |  | 33%   31C    P8    16W / 250W |      2MiB / 11178MiB |      0%      Default |  +-------------------------------+----------------------+----------------------+  +-----------------------------------------------------------------------------+  | Processes:                                                       GPU Memory |  |  GPU       PID   Type   Process name                             Usage      |  |=============================================================================|  |    0      2117      G   /usr/lib/xorg/Xorg                            40MiB |  |    0      2183      G   /usr/bin/gnome-shell                          50MiB |  |    0      2975      G   /usr/lib/xorg/Xorg                           334MiB |  |    0      3116      G   /usr/bin/gnome-shell                         170MiB |  +-----------------------------------------------------------------------------+\n\nDocker\n  $ docker version  >>>  Client:   Version:           18.09.4   API version:       1.39   Go version:        go1.10.8   Git commit:        d14af54266   Built:             Wed Mar 27 18:35:44 2019   OS/Arch:           linux/amd64   Experimental:      false  Server: Docker Engine - Community   Engine:    Version:          18.09.4    API version:      1.39 (minimum version 1.12)    Go version:       go1.10.8    Git commit:       d14af54    Built:            Wed Mar 27 18:01:48 2019    OS/Arch:          linux/amd64    Experimental:     false\n\nNvidia-docker\n  $ nvidia-docker version  >>>  NVIDIA Docker: 2.0.3  ...\n\nSet docker default-runtime\n미니쿠베에서는 기본 런타임을 Nvidia-Docker가 아닌 Docker-CE로 인식한다. GPU 사용을 위해서 도커의 기본 런타임을 Nvidia-Docker로 변경해준다.\n/etc/docker/daemon.json 파일에서 default-runtime을 nvidia로 변경한다.\n{    \"default-runtime\": \"nvidia\",    \"runtimes\": {        \"nvidia\": {            \"path\": \"nvidia-container-runtime\",            \"runtimeArgs\": []        }    }}\n변경이 다 되었다면,  도커를 재시작한다.\n$ sudo service docker restart\nMinikube start\n미니쿠베를 실행한다. 필요한 옵션에 대해서는 표에 설명해 두었다.\n$ sudo -E minikube start --vm-driver=none --apiserver-ips 127.0.0.1 --apiserver-name localhost --docker-opt default-runtime=nvidia --feature-gates=DevicePlugins=true --kubernetes-version v1.15.0>>>🟟  minikube v1.2.0 on linux (amd64)🟟  Creating none VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...🟟  Configuring environment for Kubernetes v1.15.0 on Docker 18.09.4    ▪ opt default-runtime=nvidia    ▪ kubelet.resolv-conf=/run/systemd/resolve/resolv.conf🟟  Downloading kubeadm v1.15.0🟟  Downloading kubelet v1.15.0🟟  Pulling images ...🟟  Launching Kubernetes ... 🟟  Configuring local host environment ...⚠️  The 'none' driver provides limited isolation and may reduce system security and reliability.⚠️  For more information, see:🟟  https://github.com/kubernetes/minikube/blob/master/docs/vmdriver-none.md⌛  Verifying: apiserver proxy etcd scheduler controller dns🟟  Done! kubectl is now configured to use \"minikube\"\n\n\nnamedescription\n\n—docker-opt default-runtime=nvidia미니쿠베의 기본 도커를 엔비디아 도커로 설정한다\n—feature-gates=DevicePlugins=trueGPU 지원은 쿠버네티스에서 알바/베타 단계에 속한다. 따라서 이를 사용하기 위해서는 feature-gates 옵션을 이용해서 GPU 사용 옵션을 변경해줘야한다\n—kubernetes-version v1.15.0NVIDIA 드라이버를 쿠버네티스와 연결해주는 k8s-device-plugin$^{[18]}$은 1.10이상의 쿠버네티스 버전을 요구한다\n\n\n미니쿠베 작동을 확인한다.\n$ kubectl get pods --all-namespacesNAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGEkube-system   coredns-5c98db65d4-nks96               1/1     Running   0          149mkube-system   coredns-5c98db65d4-ns9dr               1/1     Running   0          149mkube-system   etcd-minikube                          1/1     Running   0          148mkube-system   kube-addon-manager-minikube            1/1     Running   0          148mkube-system   kube-apiserver-minikube                1/1     Running   0          148mkube-system   kube-controller-manager-minikube       1/1     Running   0          148mkube-system   kube-proxy-wdhfd                       1/1     Running   0          149mkube-system   kube-scheduler-minikube                1/1     Running   0          148mkube-system   storage-provisioner                    1/1     Running   0          149m\n\n\nCrashLoopBackOff Error\n만약 kube-system의 coredns에서 CrashLoopBackOff Error가 발생한다면, coredns 설정에서 Corefile안의 loop를 삭제한다.\n$ kubectl -n kube-system edit configmap coredns>>># Please edit the object below. Lines beginning with a '#' will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1data:  Corefile: |    .:53 {        errors        health        kubernetes cluster.local in-addr.arpa ip6.arpa {           pods insecure           upstream           fallthrough in-addr.arpa ip6.arpa           ttl 30        }        prometheus :9153        forward . /etc/resolv.conf        cache 30        loop -> remove this line        reload        loadbalance    }kind: ConfigMapmetadata:  creationTimestamp: \"2019-07-19T10:34:27Z\"  name: coredns  namespace: kube-system  resourceVersion: \"189\"  selfLink: /api/v1/namespaces/kube-system/configmaps/coredns  uid: 8aa1d75a-0986-457f-81d6-d0339308a98a\n이제 기존의 포드(Pods)를 삭제하고 새로운 설정이 적용된 파드를 생성한다.\n$ kubectl -n kube-system delete pod -l k8s-app=kube-dns\n\n\nCheck GPU status\n미니쿠베의 GPU 마운트 상태를 확인한다. 지금은 GPU가 <none>인 것을 확인할 수가 있다.\n$ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\">>>NAME       GPUminikube   <none>\nk8s-device-plugin\nk8s-device-plugin$^{[18]}$을 미니쿠베에 적용한다. k8s-device-plugin은 미니쿠베에서 GPU 디바이스를 인식할 수 있게 해준다.\n$ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta/nvidia-device-plugin.yml$ kubectl get pods --all-namespaces>>>NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE...kube-system   nvidia-device-plugin-daemonset-4xlfc   1/1     Running   0          146m...\nk8s-device-plugin이 Running상태로 바뀌였다면, GPU 상태를 다시 한번 확인해본다. GPU의 개수가 2개로 변경되었음을 확인할 수 있다.\n$ kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\">>>NAME       GPUminikube   2\nGPU-demo.yaml\nGPU 컨테이너를 생성할 yaml파일을 생성한다. 이는 실제로 컨테이너가 미니쿠베에서 실행되었을 때, 제대로 작동하는지 확인하기 위함이다.\nGPU-demo.yaml\napiVersion: v1kind: Podmetadata:  name: gpuspec:  containers:  - name: gpu-container    image: nvidia/cuda:9.0-runtime    command:      - \"/bin/sh\"      - \"-c\"    args:      - nvidia-smi && tail -f /dev/null    resources:      requests:        nvidia.com/gpu: 2      limits:        nvidia.com/gpu: 2\n 미니쿠베에 컨테이너를 띄워보자.\n$ kubectl apply -f GPU-demo.yaml>>>pod/gpu created$ kubectl get pods -n default>>>NAME   READY   STATUS    RESTARTS   AGEgpu    1/1     Running   0          157m$ kubectl logs gpu>>>+-----------------------------------------------------------------------------+| NVIDIA-SMI 390.116                Driver Version: 390.116                   ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A || 33%   30C    P8    24W / 250W |    599MiB / 11175MiB |      1%      Default |+-------------------------------+----------------------+----------------------+|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A || 33%   31C    P8    16W / 250W |      2MiB / 11178MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================||    0      2117      G   /usr/lib/xorg/Xorg                            40MiB ||    0      2183      G   /usr/bin/gnome-shell                          50MiB ||    0      2975      G   /usr/lib/xorg/Xorg                           334MiB ||    0      3116      G   /usr/bin/gnome-shell                         170MiB |+-----------------------------------------------------------------------------+\nAccess container\n조금 더 확실하게 하기 위해서 미니쿠베에 배포한 컨테이너에 접속해보자. 접속 한 후에는 nvidia-smi와 nvcc -V명령어로 GPU가 잘 연결되어있는지 확인한다.\n$ kubectl exec gpu -it g -- /bin/bash>>>    root@gpu:/# nvcc -V    >>>    nvcc: NVIDIA (R) Cuda compiler driver    Copyright (c) 2005-2017 NVIDIA Corporation    Built on Fri_Sep__1_21:08:03_CDT_2017    Cuda compilation tools, release 9.0, V9.0.176    root@gpu:/# nvidia-smi    +-----------------------------------------------------------------------------+    | NVIDIA-SMI 390.116                Driver Version: 390.116                   |    |-------------------------------+----------------------+----------------------+    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |    |===============================+======================+======================|    |   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |    | 33%   30C    P8    24W / 250W |    599MiB / 11175MiB |      1%      Default |    +-------------------------------+----------------------+----------------------+    |   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |    | 33%   31C    P8    16W / 250W |      2MiB / 11178MiB |      0%      Default |    +-------------------------------+----------------------+----------------------+    +-----------------------------------------------------------------------------+    | Processes:                                                       GPU Memory |    |  GPU       PID   Type   Process name                             Usage      |    |=============================================================================|    |    0      2117      G   /usr/lib/xorg/Xorg                            40MiB |    |    0      2183      G   /usr/bin/gnome-shell                          50MiB |    |    0      2975      G   /usr/lib/xorg/Xorg                           334MiB |    |    0      3116      G   /usr/bin/gnome-shell                         170MiB |    +-----------------------------------------------------------------------------+\nTip of Testing & Debugging\nHelpful command\n필자는 쿠버네티스와 도커환경이 익숙하지 않아서 생각보다 많이 헤맸는데, 매 스텝 스텝 작업할 때마다 에러가 발생하면 kubectl describe, kubectl logs, minikube logs lspci -nn | grep -i nvidia 를 열심히 사용해서 디버깅을 진행하였다.\n\nkubectl describe, kubectl logs는 미니쿠베의 파드(Pods)의 상태 및 로그 메세지를 확인할 수 있다\nminikube logs는 미니쿠베 자체의 로그 정보를 확인할 수 있다\nlspci -nn | grep -i nvidia는 연결되어있는 디바이스 정보를 확인할 수 있다. 컨테이너에서 이를 확인하기 위해서는 별도의 Dockerfile을 작성해서 lspci util을 설치해야 컨테이너 내부에서 사용이 가능하다\n\nConfiguration file clearning\n미니쿠베로 작업을 하다가 잘못되어서 혹은 재현을 위해서 재설치를 하는 경우가 종종 있다. 이 때 기존의 설정 파일 삭제를 해줘야한다. 그렇지 않으면 이전에 설정들이 같이 따라와서 이전 작업에서 발생한 에러가 그대로 발생하는 경우가 있다. 대표적으로 아래 파일들을 삭제했는지 꼭 확인하자.\n\n~/.kube\n~/.minikube\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n\nReference\n\n(Experimental) NVIDIA GPU support in minikube\nHow to enable IOMMU on Ubuntu 18.04\nUbuntu 18.04 - KVM/QEMU Windows 10 GPU Passthrough\n[GUIDE] Linux PCI GPU VFIO Passthrough\nTesting VFIO with GPU\nKVM: GPU Passthrough\nKVM 기반의 GPU Passthrough 환경\nUSERSPACE I/O와 VFIO\nPassthrough\n0/1 nodes are available: 1 Insufficient nvidia.com/gpu\nminikube - GPU support\nKubeflow - Troubleshooting\nRunning TensorFlow Kubernetes\nKubernetes - Schedule GPUs\nKubernetes - Feature Gates\nKubeflow Setup\nKubernetes에서 gpu pod 생성\nNVIDIA device plugin for Kubernetes\n\nThanks to\n검수자\n\n문동욱\n정미연\n김수정\n김보섭","link":"http://ssaru.github.io/2019/07/25/20190725-Connect_GPU_to_Minikube/","title":"Connect GPU to Minikube","pubDate":"2019-07-25T12:56:53.000Z"}}]},"allFeedMartinKBlog":{"edges":[{"node":{"contentSnippet":"01. NestJS란\nNodeJS 진영에도 여러개의 프레임워크가 있다. 그 중 가장 대표적인 것이 Express 라고 볼 수 있다. Express 를 이용하여 개발을 하다보면 항상 아쉬웠던 부분은 프레임워크 안에서 지원해주는 것이 너무 적으며, 하다못해 Typescript를 사용하려고 해도 설정을 개발자가 직접 설정해줘야 한다는 점에서 많은 골치가 아팠다. 그러던 중, 예전 직장 동료가 이야기했던 NestJS에 대해 떠올리게 되었고, 튜터리얼을 살펴보면 중 Springboot와 많은 점에서 유사하다는 것을 느낄 수 있으며, 기본적으로 Typescript를 지원한다는 것을 알 수 있다. 그래서 이참에 현재 진행하고 있는 사이드 프로젝트의 Front-end 서버는 NestJS를 이용하여 구축해보기로 하였다. \n일단 NestJS는 기본적으로 Typescript를 지원하며 Javascript로 애플리케이션을 작성하는 것도 가능하다. 뿐만 아니라 OOP(Object Oriented Programming), FP(Functional Programming), FRP(Functional Reactive Programming) 등등 좋은 키워드는 모두 나온다. 이 중에서 NestJS를 사용해야겠다 라는 생각을 하게된 결정적인 이유는 OOP, 즉 객체 지향 프로그래밍이었다.\n\nNestJS는 NodeJS 프레임워크인 Express나 Fastify 등의 API를 직접 사용할 수도 있으며 혹은 NestJS 에서 제공하는 추상화된 인터페이스를 이용할 수도 있다. 이러한 NestJS는 Angular로부터 영감을 받아 만들어졌다. 일단 제일 Javascript와 Typescript를 이용하여 주로 개발하는 나로서는 Frontend코드와 Backend코드를 하나의 언어로 작성할 수 있다는 점이 가장 큰 장점이었다. 무엇보다 항상 제일 고민이었던 부분은 Typescript를 이용하여 개발을 하다보니 항상 생기는 서버 애플리케이션과의 Model 타입에 대한 불일치였는데, NestJS를 이용하여 개발을 하면 이러한 부분에 대해서 해결할 수 있을 것 같다는 생각을 하였다.\n요즘 대부분의 라이브러리나 프레임워크에서 제공해주듯 NestJS 역시 CLI를 통해 프로젝트 스케폴딩을 쉽게 할 수 있다. 셋팅하기에 앞서 @nestjs/cli가 Global로 설치가 되어있어야 하나 혹여라도 전역에 설치하기 싫다면 npx를 이용하여 셋팅을 해도 될 것 같으며, 아예 디펜던시부터 하나 하나 설치하여 진행하는 방법도 있다.\n일단 나는 Next와 함께 이용하여 사용할 예정이기 때문에 프로젝트의 이름은 nest-next-typescript로 지었다.\n\n\n1\n2\n\n$ npm i -g @nestjs/cli\n$ nest new nest-next-typescript\n\n\n프로젝트가 생성된 후 각각의 디렉토리를 살펴보면 아래와 같다.\n\n\n1\n2\n3\n4\n5\n\n├── src\n     ├── app.controller.ts\n     ├── app.service.ts\n     ├── app.module.ts\n     ├── main.ts\n\n\n기본적으로 생성되는 단일 컨트롤러 파일(app.controller.ts)과 간단한 텍스트를 요청한 controller에 전달해주는 서비스 파일(app.service.ts), 그리고 애플리케이션 내의 모듈을 설정하는 파일(app.module.ts) 그리고 에플리케이션의 엔트리 파일(main.ts) 등이 생긴다. controller나 module, 엔트리 파일의 경우에는 익숙하겠지만, service 파일에 경우에는 개념이 조금 헷갈릴 수 있다. 이에 대해서는 뒤에서 별도로 설명하며, 지금은 아래의 사진과 같이 비즈니스 로직을 처리하는 비즈니스 레이어 정도로만 이해하고 넘어가도록 하자.\n\n이 중 엔트리 파일을 살펴보면 안에 NestJS 애플리케이션 인스턴스를 만드는 각각의 설정 값들이 bootstrap 함수 안에서 실행되는 것을 볼 수 있다. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\nimport { NestFactory } from '@nestjs/core';\nimport { AppModule } from './app.module';\n    \nasync function bootstrap() {\n  const app = await NestFactory.create(AppModule);\n  await app.listen(3000);\n}\nbootstrap();\n\n\n구조 자체적으로 Springboot 와 동일하다는 것을 알 수 있다. 그래서인지 뭔가 앞으로도 NestJS를 좋아할 것 같다는 생각이 든다. 일단은 폴더링에 대한 큰 고민 없이 Springboot 애플리케이션과 동일하게 가져가도 좋지 않을까 싶다.","link":"http://blog.martinwork.co.kr/nestjs/2020/03/22/what-is-nestjs.html","title":"01. NestJS란","pubDate":"2020-03-22T14:21:51.000Z"}},{"node":{"contentSnippet":"이번 포스팅에서는 MYSQL의 데이터 타입에 대해서 살펴본다. MYSQL에는 크게 4가지의 데이터 타입이 존재하며, 각각의 특징과 범위 그리고 저장 가능한 사이즈가 다르다. 데이터 타입을 살펴보고 상황에 따라 테이블을 생성할 때 어떤 타입이 필요한지 고민해보도록 하자.\n숫자형 데이터 타입\n\n타입정의범위UNSIGNED 사용가능 여부\n\nTINYINT(n)정수형 데이터 타입- 1 Byte(2^8)\n - 128 ~ + 127 또는 0 ~ 255 수 표현 가능  \n- 1Byte(2^8) \n- 128 ~ + 127 또는 0 ~ 255 수 표현 가능O\nSMALLINT(n)정수형 데이터 타입- 2 Byte(2^16)\n - 32,768 ~ 32,167 또는 0 ~ 65536수 표현 가능O\nMEDIUMINT(n)정수형 데이터 타입- 3 Byte\n - 8,388,608 ~ 8,388,607 또는 0 ~ 16,777,215 수 표현 가능O\nINT(n)정수형 데이터 타입- 4 Byte\n - 2,147,483,648 ~ 2,147,483,647 또는 0 4,294,967,295 수 표현 가능O\nBIGINT(n)정수형 데이터 타입(LONG)- 8 byte\n - 2^64 - 1 표현 가능(무한 수 표현 가능이라고도 함)O\nDECIMAL(m, d)- 고정 소수형 데이터 타입고정(길이+1 byte)\n - 화폐 데이터와 같이 데이터의 정확도를 요하는 경우에 주로 사용\n - M의 최대값은 65, D는 소수 자릿수이며 0이면 소수점 가지지 않음- 소수점을 사용한 형태\n - Default: m ⇒ 10X\nFLOAT(n)부동 소수형 데이터 타입- 4 byte \n- 부동 소수점을 사용한 형태X\nDOUBLE(n)부동 소수형 데이터 타입- 8 byte \n - DOUBLE을 문자열로 저장X\n\nUNSIGNED 속성\nUNSIGNED 속성을 부여하게 되면 해당 속성을 부여받은 컬럼에는 음수값을 대입하지 못한다. 결제 내역에 대한 테이블을 예시로 들어보도록 하겠다. 결제 금액은 음수값이 될 수 없음을 누구나 알 수 있을 것이다. 이런 경우 UNSIGNED 속성을 부여하여 음수값을 대입하지 못하게 할 뿐만 아니라 대입 가능한 숫자의 범위를 늘려줄 수 있다. 예제를 살펴보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n\nCREATE TABLE payment\n(\n    uid BIGINT NOT NULL AUTO_INCREMENT,\n    total_consume_mount INT UNSIGNED DEFAULT 0 NOT NULL,\n    PRIMARY KEY (uid)\n);\n\n\n작성된 payment 테이블에 음수값을 대입해보도록 하자. 아마 아래와 같이 에러가 발생하는 것을 확인할 수 있다.\n\n\n1\n2\n3\n4\n\nINSERT INTO payment (uid, total_consume_mount)\nVALUES (1, -100);\n\n-- ERROR 1264 (22003): Out of range value for column 'total_consume_mount' at row 1\n\n\n또한 기존 SIGNED 속성을 가진 INT 타입에는 최대 2,147,483,647까지 대입 가능한 반면 UNSIGNED 속성을 부여하면 대입 가능한 값이 4,294,967,295까지 늘어난 것을 확인할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n-- 대입 가능한 범위가 늘어남을 볼 수 있다.\nINSERT INTO payment VALUES (1, 4294967295);\n\n-- 하지만 그 이상의 값을 대입하려 할때면 다음과 같은 에러가 발생하는 것을 확인할 수 있다.\nINSERT INTO payment VALUES (1, 4294967296);\n-- ERROR 1264 (22003): Out of range value for column 'total_consume_mount' at row 1\n``` \n\n#### Integer 타입의 N의 의미\nTINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 공통적으로 MAXIMUM 값은 항상 N과는 무관하게 허용 가능한 범위는 동일하다. 그렇다면 이 N은 언제 사용하는 것일까? 그에 대한 답은 아래의 예제에서 찾을 수 있다.\n\n```sql\nCREATE TABLE payment\n(\n    uid BIGINT NOT NULL AUTO_INCREMENT,\n    total_consume_mount INT(3) ZEROFILL DEFAULT 0 NOT NULL, -- ZEROFILL 속성을 부여한다.\n    PRIMARY KEY (uid)\n);\n\nINSERT INTO payment VALUES (1, 11);\n\nSELECT * FROM payment;\n+-----+---------------------+\n| uid | total_consume_mount |\n+-----+---------------------+\n|   1 |                 001 |\n+-----+---------------------+\n\n\nN 값은 ZEROFILL 속성과 함께 사용할 때 의미가 있다. 테이블을 생성할 때, ZEROFILL 속성을 주게되면 N 자릿수 이하의 값에 대해서 부여된 자리수를 항상 0으로 채운다. 만약 N 자릿수를 초과하게 되면 크게 의미가 없다. 또한 ZEROFILL 속성을 부여하게 되면 자동으로 UNSIGNED 속성이 붙게 된다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\nDESC payment;\n\n+---------------------+--------------------------+------+-----+---------+-------+\n| Field               | Type                     | Null | Key | Default | Extra |\n+---------------------+--------------------------+------+-----+---------+-------+\n| uid                 | bigint(20)               | NO   | MUL | NULL    |       |\n| total_consume_mount | int(3) unsigned zerofill | NO   |     | 000     |       |\n+---------------------+--------------------------+------+-----+---------+-------+\n\n\n고정 소수형 데이터 타입(DECIMAL)과 부동 소수형 데이터 타입(FLOAT, DOUBLE)\nDECIMAL 타입과 FLOAT, DOUBLE의 가장 큰 차이점은 고정 소수형 데이터 타입이냐, 부동 소수형 데이터 타입이냐에 따라 결정되게 된다. DECIMAL 타입의 경우 앞에서 총 2개의 설정을 값을 부여할 수 있다. 아래의 예제를 살펴보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n\nCREATE TABLE wallet (\n    uid BIGINT NOT NULL AUTO_INCREMENT,\n    dollar DECIMAL(5, 2) NOT NULL DEFAULT 0, \n    mileage FLOAT NOT NULL DEFAULT 0,\n    PRIMARY KEY (uid)\n);\n\n\ndollar 컬럼에 DECIMAL(5,2)의 데이터 타입을 지정하였다. DECIMAL(M, D)에서 M에 해당하는 5의 경우 총 5자리의 숫자를 사용할 수 있다는 의미이고, D에 해당하는 2는 소수점을 2자리까지 지정하겠다라는 의미이다. 그렇다면 정수의 타입은 M-D(5-2)까지 저장할 수 있다는 것을 유추할 수 있다. 그렇게 DECIMAL(5,2) 데이터 타입의 범위는 -999.99 ~ 999.99이다.\n각각의 DECIMAL과 FLOAT 타입에 동일한 데이터를 대입해보도록 하자.\n\n\n1\n2\n\nINSERT INTO wallet VALUES (default, 10.91, 10.91);\nINSERT INTO wallet VALUES (default, 10.91, 10.91);\n\n\n값을 값을 대입한 후, 조회를 해보면 값은 값이 저장된 것을 확인할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\nSELECT * FROM wallet;\n\n+---------+--------+---------+\n| user_id | dollar | mileage |\n+---------+--------+---------+\n|       1 |  10.91 |   10.91 |\n|       2 |  10.91 |   10.91 |\n+---------+--------+---------+\n\n\n이렇게만 보면 큰 차이점은 없어 보인다. 하지만 아래와 같이 연산을 해보면 서로 다른 결과를 보인다.\n\n\n1\n2\n3\n4\n5\n6\n7\n\nSELECT SUM(dollar) decimal_type, SUM(mileage) float_type FROM wallet;\n\n+--------------+-------------------+\n| decimal_type | float_type        |\n+--------------+-------------------+\n|        21.82 | 21.81999969482422 |\n+--------------+-------------------+\n\n\nDECIMAL 타입은 고동 소수형 데이터기 때문에 연산을 해도 정해진 자리수로 떨어지는 것을 확인할 수 있지만 FLOAT 타입의 경우, 의도치 않은 결과를 반환하는 것을 볼 수 있다. 그렇기 때문에 앞서의 표에서 화폐 데이터와 같이 정확한 데이터를 요할 경우 주로 사용한다. \n문자형 데이터 타입\n\n타입정의길이\n\nCHAR(n)- 고정 길이 데이터 타입 \n- 지정된 길이보다 짧은 데이터 입력 시 나머지 길이는 공백으로 채워짐 \n- 검색시, PAD_CHAR_TO_FULL_LENGTH 모드를 설정하지 않으면 공백은 제거됨0 ~ 255 (byte)\nVACHAR(n)- 가변 길이 데이터 타입\n- 지정된 길이보다 짧은 데이터 입력시 공백으로 채우지 않음\n -저장시 1-byte 혹은 2-byte 길이 Prefix 데이터를 저장. 이 Prefix 데이터는 값의 바이트 수에 대한 정보를 담는다.(https://dev.mysql.com/doc/refman/8.0/en/char.html)0 ~ 65,535 (byte)\nTINYTEXT(n)- 문자열 데이터 타입(최대 255 byte)\n- TINYBLOB와 같은 길이값을 저장 가능(단 차이점은 저장 될때 nonbinary string으로 저장)\nhttps://dev.mysql.com/doc/refman/8.0/en/blob.html0 ~ 255 (byte)\nTEXT(n)- 문자열 데이터 타입(최대 65,535 byte)\n - BLOB와 같은 길이값을 저장 가능(단 차이점은 저장 될때 nonbinary string으로 저장)0 ~ 65,535 (byte)\nMEDIUMTEXT(n)- 문자열 데이터 타입(최대 16,777,215 byte)\n - MEDIRMBLOB와 같은 길이값을 저장 가능(단 차이점은 저장 될때 nonbinary string으로 저장)0 ~ 16,777,215 (byte)\nLONGTEXT(n)- 문자열 데이터 타입(최대 4,294,967,295 byte) \n- LONGBLOB와 같은 길이값을 저장 가능(단 차이점은 저장 될때 nonbinary string으로 저장)0 ~ 4,294,967,295 (byte)\n\n범위값 초과\n먼저 예시로 유저 테이블을 생성해보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n\nCREATE TABLE user (\n    uid     BIGINT NOT NULL AUTO_INCREMENT,\n    name    VARCHAR(10) NOT NULL,\n    password CHAR(10) NOT NULL,\n    join_date DATETIME NOT NULL DEFAULT NOW(),\n    PRIMARY KEY (uid)\n);\n\n\n생성된 테이블에 샘플 데이터를 넣게 되어보도록 하자. 아래와 같이 비밀번호 컬럼의 범위 N 값 이내의 경우에는 정상적으로 데이터가 추가되는 것을 확인할 수 있다.\n\n\n1\n\nINSERT INTO user VALUES (default, 'Martin', '1234567890', default);\n\n\n하지만 N값의 범위를 초과하는 경우, 에러를 반환하는 것을 확인할 수 있다. 만약 strict mode가 설정되어 있지 않다면 나머지 값은 길이에 맞게 짤리고 저장은 될 것이다.\n\n\n1\n2\n\nINSERT INTO user VALUES (default, 'Martin', '12345678900', default);\n-- ERROR 1406 (22001): Data too long for column 'password' at row 1\n\n\n날짜형 데이터 타입\n\n타입정의길이형식길이\n\nDATE날짜(년도, 월, 일) 형태의 기간 표현 데이터3 byte0000-00-00 (YYYY-MM-DD)1000-01-01 ~ 9999-12-31\nTIME시간(시, 분, 초) 형태의 기간 표현 데이터3 byte00:00:00.\nDATETIME날짜와 시간 형태의 기간 표현 데이터8 byte0000-00-00 00:00:00 (YYYY-MM-DD hh:mm:ss)1000-01-01 00:00:00.000000 ~ 9999-12-31 23:59:59.999999\nTIMESTAMP날짜와 시간 형태의 기간 표현 데이터 타입 시스템 변경 시 자동으로 그 날짜와 시간이 저장4 byteInteger.\nYEAR년도 표현 데이터 타입1 byte0000.\n\n이진 데이터 타입\n\n타입정의길이\n\nBINARY(n) & BYTE(n)CHAR 형태의 이진 데이터 타입최대 255 byte\nVARBINARY(n)VARCHAR 형태의 이진 데이터 타입최대 65,535 byte\nTINYBLOB(n)이진 데이터 타입최대 255 byte\nBLOB(n)이진 데이터 타입최대 65,535 byte\nMEDIUMBLOB(n)이진 데이터 타입최대 16,777,215 byte\nLONGBLOB(n)이진 데이터 타입최대 4,294,967,295 byte\n\nENUM 타입\nENUM 타입의 경우에는 실제 실무에서 사용할 수 있는 경우가 크게 많아 보이지는 않는다. 새로운 ENUM 타입이 생겼을 경우나 혹은 정렬을 해야하는 경우 비효율적이기 때문이다. 그래도 어떠한 타입인지 알고 쓸지 말지를 결정하는 것이 더 좋기 때문에 예제를 통해 한번 살펴보도록 하자. 먼저 아래와 같이 테이블 생성해도록 하자. 여기에서 중요한 것을 size\n\n\n1\n2\n3\n4\n\nCREATE TABLE enum_test (\n  size ENUM('x-small', 'small', 'medium', 'large', 'x-large') NOT NULL,\n  color ENUM('red', 'blue', 'black', 'white', 'pink')\n);\n\n\n이렇게 선언된 테이블에 총 2가지 방법은 데이터 삽입이 가능하다. 첫번째 방법은 아래와 같이 ENUM 타입 중 한가지 타입을 이용하여 넣는 방법이다.\n\n\n1\n2\n3\n4\n5\n6\n7\n\nINSERT INTO enum_test VALUES ('x-small', 'red');\n\n+---------+-------+\n| size    | color |\n+---------+-------+\n| x-small | red   |\n+---------+-------+\n\n\n두번째 방법은 아래와 같이 저장된 ENUM 타입의 ordering 숫자로 저장하는 법이다. \n\n\n1\n2\n3\n4\n5\n6\n7\n\nINSERT INTO enum_test VALUES (1, 1);\n\n+---------+-------+\n| size    | color |\n+---------+-------+\n| x-small | red   |\n+---------+-------+\n\n\n기본적으로 ENUM 값을 지정하지 않으면 NULL로 삽입되지만, 만약 NULL값을 원하지 않는다면, size 컬럼과 같이 NOT NULL 속성을 추가해주면 된다. \nENUM 타입 조회하기\nENUM 타입은 앞에서 살펴보았듯 ordering 숫자로 저장된다. 그렇기 때문에 해당 필드값을 기준으로 정렬하여 조회할 수 있다. 일단 정렬되는 데이터를 확인하기 위해 몇가지 데이터를 추가해보자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\nINSERT INTO enum_test VALUES (1, 1);\nINSERT INTO enum_test VALUES (3, 2);\nINSERT INTO enum_test VALUES (2, 4);\nINSERT INTO enum_test VALUES (4, 1);\nINSERT INTO enum_test VALUES (5, 4);\n\n+---------+-------+\n| size    | color |\n+---------+-------+\n| x-small | red   |\n| x-small | red   |\n| medium  | blue  |\n| small   | white |\n| large   | red   |\n| x-large | white |\n+---------+-------+\n\n\n몇가지 데이터를 추가한 후, 한번 아래와 같이 데이터 정렬을 해자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nSELECT * FROM enum_test ORDER BY size;\n\n+---------+-------+\n| size    | color |\n+---------+-------+\n| x-small | red   |\n| x-small | red   |\n| small   | white |\n| medium  | blue  |\n| large   | red   |\n| x-large | white |\n+---------+-------+\n\n\nsize를 기준으로 정렬을 했을 경우, 위와 같이 저장된 ordering 번호에 맞게 정렬되는 것을 확인할 수 있다. 그렇다면 만약 알파벳 순서대로 조회하고자 한다면 어떻게 해야할까? 제일 좋은 것은 알파벳 순서대로 저장하는 것이지만, 현실 가능성은 떨어진다. 중간에 어떠한 ENUM 타입이 생길지도 모르는데, 그걸 모두 고려해서 알파벳 순서대로 테이블을 추가하기란 쉽지 않다. 알파벳 순서대로 조회하기 위해선 다음과 같이 조회하면 된다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nSELECT * FROM enum_test ORDER BY CAST(size AS CHAR);\n\n+---------+-------+\n| size    | color |\n+---------+-------+\n| large   | red   |\n| medium  | blue  |\n| small   | white |\n| x-large | white |\n| x-small | red   |\n| x-small | red   |\n+---------+-------+\n\n\nENUM 타입인 size를 CHAR 형태로 타입 캐스팅 해준 후, 조회를 하는 것이다. 하지만 이또한 데이터가 많아질수록 성능이 안좋아질테니, 되도록이면 ENUM 타입보단 다른 형태의 데이터를 이용하여 저장하고 각각의 ENUM 타입에 대한 맵핑은 각각의 애플리케이션에서 정의하는 것이 좋을 것 같다.\nSET 데이터 타입\n마지막으로 살펴볼 타입은 SET 데이터 타입이다. SET 데이터 타입은 ENUM 타입이랑 다르게 중복값을 허용한다. 먼저 예시를 보기 위해 샘플로 아래와 같은 영화 장르 테이블을 생성하도록 하자.\n\n\n1\n2\n3\n\nCREATE TABLE genre (\n   size SET('horror', 'comic', 'romance', 'documentary', 'sf')\n);\n\n\n모두 알다시피 하나의 영화에는 여러개의 장르를 선택할 수 있다. 코믹이면서 로맨스일 수도 있고, 공포이면서 SF일 수도 있다. 일단 하나의 영화에 하나의 장르만 존재한다고 가정하고 데이터를 추가해보자.\n\n\n1\n2\n\nINSERT INTO genre VALUES ('horror');\nINSERT INTO genre VALUES ('comic');\n\n\n그렇다면 만약 여러개의 장르를 가져야할때는 어떻게 해야할까? \n\n\n1\n\nINSERT INTO genre VALUES ('comic,romance');\n\n\n중복으로 넣어줄 데이터를 쉼표(,)와 함께 추가해주면 된다. 다만 여기에서 중요한 것은 쉼표 뒤에 띄어쓰기가 되어있으면 안되고, 무조건 붙어있어야 한다. 만약 띄어쓰기를 한 후 데이터를 추가하려고 하면 아래와 같은 에러가 발생한다.\n\n\n1\n2\n3\n\nINSERT INTO genre VALUES ('comic,romance');\n\n-- ERROR 1265 (01000): Data truncated for column 'size' at row 1\n\n\n또 다른 방법으로는 SET 데이터 타입의 주요한 특징에서 찾을 수 있다. SET 데이터의 경우 하나의 값마다 순서대로 Binary 값이 부여된다. Jenre 데이터 테이블의 SET 데이터를 바이너리로 표현하면 다음과 같이 표현할 수 있다.\n\nVALUEDECIMALBINARY\n\nhorror100001\ncomic200010\nromance400100\ndocumentary801000\nsf1610000\n\n이를 이용하여 다음과 같이 넣어줄 수 있다.\n\n\n1\n2\n\nINSERT INTO genre VALUES (1); -- INSERT INTO genre VALUES ('horror'); 와 같다.\nINSERT INTO genre VALUES (7); -- INSERT INTO genre VALUES ('horror,comic,romance'); 와 같다.\n\n\nSET 데이터 타입 조회하기\nSET 데이터를 조회하는 방법은 여러가지가 있다. 일단 첫번째로 일반적인 방법으로 조회할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n-- 단일 조건을 이용하여 조회하는 경우\nSELECT * FROM genre WHERE size='horror'; -- 정확하게 horror 값만 가지고 있는 데이터만 조회 가능하다.\nSELECT * FROM genre WHERE size LIKE '%horror%'; -- horror 값을 포함한 데이터 모두가 조회 가능하다.\n\n-- 여러 조건을 이용하여 조회하는 경우\nSELECT * FROM genre WHERE size='horror,romance'; -- 정확하게 두가지 값을 모두 가지고 있는 데이터만 조회 가능하다.\nSELECT * FROM genre WHERE size LIKE '%horror%' AND size LIKE '%romance%'; -- 'horror'와 'romance' 값 모두 가지고 있는 데이터 모두 조회 가능하다.\nSELECT * FROM genre WHERE size & 1 AND size & 4; -- 위와 도잉ㄹ하다.\n\n\n두번째 방법은 비트 연산자를 활용하는 방법이다.\n\n\n1\n2\n\nSELECT * FROM genre WHERE size & 1; -- SELECT * FROM genre WHERE size LIKE '%horror%'; 와 동일하다.\nSELECT * FROM genre WHERE size & 00001; -- 위와 동일하다.\n\n\n마지막 방법은 FIND_IN_SET 함수를 이용하는 것이다. 이 함수는 첫번째 인자로는 SET 의 값을, 두번째 인자로는 필드 이름을 넣어주면 저장되 있다면 저장된 순서값을, 저장되어있지 않다면 0을 반환한다. 아래의 예제를 살펴보도록 하자.\n\n\n1\n\nSELECT FIND_IN_SET('romance', size) FROM genre;\n\n\n이와 같이 조회하면 romance가 저장되어 있다면 3을 그렇지 않다면 면 0을 반환하는 것을 확인할 수 있다. 이 함수를 이용한다면 다음과 같은 조건으로 조회할 수 있다.\n\n\n1\n\nSELECT * FROM genre WHERE FIND_IN_SET('romance', size) > 0; -- romance 값이 저장된 모든 데이터가 조회 가능하다.\n\n\n지금까지 MYSQL 의 데이터 타입에 대해서 살펴보았다. 혹여라도 이와 관련된 소스를 확인하고 싶다면 여기에서 확인할 수 있다.","link":"http://blog.martinwork.co.kr/mysql/2020/01/17/mysql-data-type.html","title":"MYSQL 데이터 타입","pubDate":"2020-01-16T15:50:51.000Z"}},{"node":{"contentSnippet":"큰 맥락으로 2019년은?\n2019년은 가장 크게는 몇가지의 이벤트로 인해  일을 시작한 이후 가장 의미있는 한 해이며, 그 동안 생각해왔던 마음가짐이 달라졌던 해였다. 일단 2019년의 가장 큰 이벤트를 간략하게 정리하면 다음과 같다.\n\n‘커피 한 잔 마시며 끝내는 VueJS’ 출판\n쿠팡으로의 이직, 그리고 고마운 동료들과의 만남\n사이드 프로젝트 ‘스낵 뉴스’ 오픈\n글또 3기 마무리\n외부 활동\n\n이 이벤트 중 개인적으로 가장 특별한 이벤트 아무래도 나만의 책을 출판했다는 것이겠지만, 2019년 상반기 회고와 ‘흔한 주니어 개발자의 VueJS 책 집필기’ 라는 포스팅에서 이야기했기 때문에 자세한 내용은 해당 포스팅을 보는 것을 추천한다.\nVueJS 책 출판\n2019년 7월 30일, 그 동안의 길고긴 여정을 끝으로 드디어 VueJS 책을 출판하게 되었다. 일단 주변에서 이야기하는 많은 묻는 질문은 많이 팔렸냐인데, 솔직히 말해서 나도 잘 모르겠다. 물론 출판사의 담당자 분을 통해 판매 지수에 대해 묻고자하면 물을 수는 있겠지만, 결과보단 과정에 큰 의미를 두어서 집필 과정에 큰 의미를 두어서인지 크게 궁금하지 않았던 것도 있다. 또 막상 조만간 판매 지수에 대한 결산을 받아보면 또 생각이 달라질지도..\n\n물론 제일 많이 들려왔던 말은 정말 커피 한잔 마시며 끝낼 수 있냐라는 말이였다. 제목에 대해 냉소적인 태도를 취하는 사람들도 없지는 않았고, 지인들은 나에게 그러한 이야기를 전해주기도 했다. 물론 그때마다 우스갯소리로 ‘누구를 기준으로 했냐에 따라 커피 한잔이 될 수도 있지 않을까요?’ 라는 대답으로 일관했지만, 이러한 이야기들을 전해들으며 되도록이면 사람들 앞에 나를 내보이는 것에 대해 거부감을 가지게 된 원인 중 하나가 되었다. (사람들 앞에 나를 내보이는 것에 대한 거부감의 근본적인 이유는 뒤에서 설명하겠지만 따로 있다.)\n쿠팡으로의 이직 그리고 이상적인 동료들과의 만남\n전 회사에 대한 리뷰를 쓰고나서 1년도 안되었는데 이직에 대한 이야기를 하려니 민망하긴 하지만… 뜻하지 않게 이직을 하게 되었다. 사실 이직 준비를 하는 기간을 두고 이직 준비를 한 것도 아니였고, 이직에 대한 생각도 크게 하고 있지 않은 상태에서 이직을 하게 되었다.\n\n물론 이렇게 이야기하면 쉽게 이직을 했다는 것처럼 보일 수 있으나, 준비를 하지 않았다는 것이지 홈워크나 면접 등이 쉬었던 것은 아니다. 검색만 해도 심심치 않게 나올 수 있지만, 쿠팡의 면접 과정은 험난한 길이기도 했다. 하루의 면접으로 끝나지만 그 하루의 면접이 4시간에서 5시간동안 진행되기 때문이다. 그 시간동안 여러 명의 면접관들을 거쳐야 하기 때문에 정신력 싸움이였던 것 같다. 만약 나처럼 첫 면접에서 어버버하기 시작하면 그 뒤로는 무슨 이야기를 했는지 기억조차 나지 않는다. 보통은 면접 끝나고 항상 주변 카페에 가서 면접 질문과 과정에 대해 복기한 후 내가 무엇이 부족했는지 적어둔 나만의 정리 노트가 있는데 유일하게 쿠팡은 면접 질문에 대한 복기가 없다. \n\n그리고 2019년 8월 29일, 내가 생각하는 이상적인 개발 문화와 동료들과 조우하였다. 일단 말하기 전에 미리 최소한의 방어책을 세워놓자면…. 팀마다 분위기가 다를 수 있고, 사람마다 느끼는 점이 다를 수 있기 때문에 쿠팡의 모든 개발팀의 문화가 이렇다라는 의미도 아니며 모든 사람이 쿠팡이라는 회사에서 일을 하며 혹은 일했던 경험이 나와 같지는 않을 수 있다. \n일단 우리 팀의 이상적인 개발 문화를 가지고 있다라고 하는 몇 가지 이유가 있는데 크게는 다음과 같다.\n\n한달 정도의 온보딩 기간을 가지고 이 기간동안 팀의 문화와 일하는 방식을 배우며, 비즈니스 구현 외적으로 팀에 기여할 수 있는 것을 찾아 기여하도록 한다.\n온/오프라인 가리지 않는 빡빡한 코드 리뷰를 하며 보통 한번의 코드 리뷰를 진행하면 회의실에서 짧게는 1시간 길게는 2시간 3시간까지도 진행한다.\n대부분의 시니어 개발자들도 끊임없이 공부를 하며 그러한 지식에 대해 매일 매일 공유하는 시간을 갖는다.\n\n이 외에 애자일 방식의 업무, 자율근무제, 재택 근무 등에 대해서 쿠팡 전체적으로 적용되기 때문에 따로 이 팀만의 이상적인 개발 문화라고 이야기 싶진 않다. 10년을 넘게 일해온 시니어들조차도 끊임없이 공부하며, 동료들과 지식 나누기를 즐기며, 코드 리뷰를 몇시간동안 진행을 하더라도 지친 기색없이 열정적으로 리뷰 하는 모습을 보며 과연 10년 후에 나도 이런 시니어 개발자가 될 수 있을까 라는 생각을 들게끔 한다.\n쿠팡의 개발 문화나 팀 문화 혹은 채용 관련해서 궁금하신 분은 언제든 편하게 연락주세요 :)\n사이드 프로젝트 ‘스낵 뉴스’ 오픈\n전 직장에서 만난 동료들 4명을 시작으로 이제는 개발자 4명, PO 1명, 디자이너 1명, 에디터 2명이라는 지금은 작지 않은 사이드 프로젝트 팀이 되었다. 물론 모든 사이드 프로젝트 특성상 회사 일이나 개인의 일 때문에 꾸준히 하진 못했지만 긴 기간동안 준비했던 프로젝트였다.   \n\nAWS에 대해 잘 이해도 못한 상태에서 서비스를 이용하다가 하마터면 사용 범위를 초과하여 배포를 못했던 우스운 일도 있었고, 원격으로 코드 리뷰를 진행하기도 하는 등의 기억에 남는 일도 많았다. 이에 대해서는 이후에 회고글에서 다룰 예정이다. 아마도.. VueJS와 Typescript 그리고 Firebase를 이용한 애플리케이션 개발기라는 이름으로 개발하며 겪었던 이슈 등을 함께 공유하지 않을까 싶다.\n글또 3기 마무리\n5월 27일, SNS의 피드를 통해 글또 라는 모임을 처음 알게 되었다. \n\n글또는 글쓰는 개발자 모임 으로 내가 참여했을 때는 세번째 기수였다. 일단 이 모임을 처음 참여하게 된 계기는 꾸준한 블로깅을 하기 위해서였다. 내가 알던 지식, 새롭게 알게 되는 지식들을 글로써 정리를 한다면 조금더 머릿 속에 차곡차곡 저장할 수 있다고 느꼈기 때문이다. 물론 나의 경우 대부분의 경우에서 느꼈지만, 혼자서 목표없이 진행을 하게되면 금방 흐지부지해짐을 느꼈다. 그래서 강제성을 부여하기 위해 기회를 엿보던 중, 나의 목적과 부합하는 모임이기에 참여를 하게 되었다. 글또의 모임에 참여하며 한달에 2번의 블로깅을 강제적으로 하게 되었다. 물론 처음에는 내가 모르던 지식을 정리해야지 라며 의지가 뿜뿜했지만, 어느 순간부터는 패널티를 부여받지 않기 위해 글을 쓰고 있게 되었다. 물론 이러한 강제성을 부여받고자 했던 모임이기 때문에 목적은 충분히 달성했다. 개인적인 만족도가 높은 모임이기도 했으며, 다음에 기회가 된다면 또 참여 하고 싶은 모임이기도 하다.\n외부 활동\n2018년과 다른 점은 올 한해는 다른 회사의 개발자들과 많이 만나서 이야기를 나눠보고 생각을 들어보고자 했다는 것이다. 그래서 멋쟁이 사자처럼 해커톤에 멘토로 참여하기도 했으며, 모각작(모여서 각자 작업하기) 모임을 만들어 다같이 애자일스러운 스크럼과 회고를 진행하기도 했다. Chit Chat 이라는 모임을 주최하기도 했으며, mini LAZYCON 이라는 모임을 기획하기도 했다.\n\n혹시나 무슨 활동이었는지 궁금하다면 각각의 링크를 통해 확인할 수 있다.\n\n멋쟁이 사자처럼\n모각작(모여서 각자 작업하기) 모임\nmini LAZYCON\nCHIT Chat\n\n물론 다른 누군가에겐 적은 수이긴 하지만, 컨퍼런스나 강연 등을 포함하여 모든 외부 활동의 횟수가 1년에 1번 참여할까 말까하는 나에게는 적은 수는 아니였다. 이러한 모임을 통해 배운 것도 많았으며, 느낀 점 역시 적지 않았다. 많은 좋은 개발자 분들과 이야기를 나눠볼 수 있었으며, 개인적으로 궁금한 분은 먼저 연락을 드려 대화를 나눠보는 시간도 가져보았다. 무엇보다 생각 외로 먼저 연락 드려 대뜸 만나보고 싶다고 하여도 이상하게 바라보지 않았던 분들 모두에게 감사의 표현을 전하고 싶다.\n 하지만 2020년 이후부터는 이러한 모임들을 주최하거나 참여하지 않을 예정이며, 마찬가지로 블로깅 역시 기존과는 다른 성격의 말 그대로 메모장에 가까운 내가 무엇을 공부하고 있고, 무엇을 몰랐는지에 대해 기록하는 블로그 가 될 예정이다. 혹시나 오해의 소지가 있어 미리 이야기하자면 절대 외부 활동에 대한 경험이 좋지 않아서가 아니다. 올해의 목표와 방향에 대한 일단 가장 큰 이유는 결국 선택과 집중이 아닐까 싶다. 지금 현재 프론트 엔드 엔지니어로서 부족한 점, 그리고 앞으로 있을 커리어 전환을 위해 필요한 하드 스킬들 등이 가장 큰 이유이다. 2019년의 외부 활동 등으로 인해 많은 좋은 경험을 하였고, 새로운 사람들도 많이 만났지만 반대로 그러한 활동을 통해 내가 얼마나 부족한지, 얼마나 더 많은 시간을 투자해야 하는지에 대해 깨닫는 한해가 되었다. 오히려 내가 아는 지식과 경험의 수준이 높지 않은 상태에서 다른 누군가에게 나를 내 보였다는 것이 한편으로 부끄럽기도 했던 한해 였다. 지금 함께 하는 동료들이 이 글을 볼지는 모르겠지만, 좁은 시야를 넓혀준 동료 모두에게 감사의 표현을 전하고 싶다.\n 과거 2018년의 회고글을 보며 느낀점이 많듯, 2019년의 회고를 2020년에 읽으며 더 성장하는 나를 조우하기를 기대해본다.","link":"http://blog.martinwork.co.kr/review/2020/01/07/remembrance-in-2019.html","title":"2019년의 회고","pubDate":"2020-01-06T15:50:51.000Z"}},{"node":{"contentSnippet":"우리는 사람을 구별할 때 보통 이름과 얼굴, 성별 등과 같은 요소로 구별을 한다. 하지만 이러한 요소들을 고유하지 않으며, 그렇기 때문에 서류상에 등록할 때 사람들 각각을 구분하기 위하여 주민번호를 발급하여 구분하며 이러한 주민번호는 고유한 값이다.\n\n이것은 마치 데이터베이스에 있는 PK값과 같아 각자를 고유한 개개인을 구분하는데 사용할 수 있다. 마찬가지로 네트워크상에서 각각의 컴퓨터는 이러한 주민번호와 같은 고유한 값을 가지게 되는데 이러한 값을 하는 것이 IP이다. 현재 우리가 사용하는 IP 체계는 32Bit 체계를 사용하며, 대략 42억개의 IP 주소를 사용할 수 있다고 한다.\n\n처음 IPv4, 즉 32 bit 체계를 가질 때만 해도 컴퓨터가 지금처럼 보급화되지 않았기 때문에 42억개 정도로도 충분했지만, 지금은 개인이 여러개를 가지는 경우도 있기 때문에 거의 고갈되어 2010년도 기준, IPv6 체계까지 하여 총 2가지 체계를 가진다고 한다. \n과거에는 IP를 할당할 때 클래스 단위로 나누어 IP를 할당하였지만 지금은 현재 CIDR(사이더)이란 방식을 사용한다고 한다. 오늘은 과거에 사용하던 클래스 단위 IP 주소 체계에 대해 살펴볼 예정이다. IP를 클래스 단위로 할당할 때, 각각의 기준에 따라 총 3가지 클래스로 나누어 구분할 수 있다.\n\nA Class\nB Class\nC Class\n\n각각의 클래스는 가지는 네트워크 주소(대표 주소)와 Host 주소(IP)의 규모가 다르며, IP 대의 영역이 있기 때문에 대략적으로 IP 주소만 봐도 어떠한 클래스에 속하는지 구별할 수 있다. \nA Class\n먼저 A Class를 구분할 때는 아래의 그림에서 네트워크 주소에서 1번째 비트 즉 구분 비트가 0이어야 한다.\n\n그렇다라는 의미는 반대로 최대로 할당될 수 있는 네트워크는 아래의 그림과 같이 표현될 수 있다.\n\n이렇게 표현되어 있는 2진수를 10진수로 바꾸었을 때, 이는 0~127 범위 안에 속하여 이 사이에 있는 IP 주소는 A Class라고 볼 수 있다. 네트워크 주소 내에서 첫번째 구분 비트를 제외하면 총 2^7 개의 네트워크 그룹이 존재하며, 이 네트워크 그룹은 2^24개(16,777,216개)로 약 1,600만개의 IP 주소를 가질 수 있다. 예를 들어 54.15.135.12의 IP는 A Class에 속해 있으며, 대표 IP는 네트워트 주소만으로 표현됨으로 54.0.0.0이라고 볼 수 있다. 간단하게 정리하면 다음과 같이 정리 할 수 있다.\n\nA Class IP의 범위는 0.0.0.0 ~ 127.255.255.255 이며, 약 1,600만개의 Host 주소를 가진 네트워크 그룹은 총 127개 있다.\n\nB Class\nB Class는 구분 비트 10 이다. A Class 에 비해 표현할 수 있는 네트워크 주소의 범위가 더 커졌으며, 호스트 주소의 범위는 줄어들었다.  \n\n위와 마찬가지로 최대로 할당될 수 있는 네트워크 범위의 수는 아래의 그림과 같이 10111111까지이며 이는 10진수로 표현했을 때 191이다.\n\n이러한 B Class는 네트워크 영역 중 고정되는 앞의 2개의 구분 비트를 제외하면 총 2^14개(16,384개) 네트워크 그룹이 있으며 각각의 네트워크 그룹은 2^16개(65,536개)로 대략 65,000개의 IP 주소를 가질 수 있다. B Class의 범위는 앞의 A Class 범위 이후부터이기 때문에 다음과 같이 정리할 수 있다. \n\nB Class IP의 범위는 128.0.0.0 ~ 191.255.255.255 이며, 약 65,000개의 Host 주소를 가진 네트워크 그룹이 대략 16,000개 존재한다.\n\n만약 153.39.16.3의 IP를 사용하고 있다면 이는 B Class 안에 포함되며 대표 IP는 네트워크 영역 안의 153.39.0.0이 된다.\nC Class\n마지막으로 C Class는 가장 넓은 범위의 네트워크 범위를 가진다. 하지만 반대로는 호스트 주소의 범위는 줄어든다.\n\nC Class는 구분자 비트는 무조건 110이어여 하며, 범위는 최대 11011111까지이다. 이는 10진수로 표현했을때, 223에 해당하는 값이다. \n\nC Class는 총 2^23개(2,097,152개)의 네트워크 주소를 가지며, 각각의 네트워크는 2^8(256개)개의 호스트 주소를 가질 수 있다. 그렇기 때문에 C Class의 IP 범위는 아래와 같이 정리할 수 있다.\n\nC Class IP의 범위는 192.0.0.0 ~ 223.255.255.255까지 이며, 254개의 호스트 주소를 가지는 네트워크가 대략 200만개 존재한다. \n\n만약 203.18.41.12의 IP를 사용하고 있다면 이는 C Class 안에 포함되며 대표 IP는 203.18.41.0 이다. 위의 설명을 살펴보면 사용할 수 있는 호스트 갯수를 254개 라고 설명했는데 이는 0-255 범위로 총 256개의 IP 보다 2개 적은 숫자이다. 이는 대표 IP인 203.18.41.0번과 브로드 캐스트 IP인 203.18.41.255를 제외한 숫자이다. \n이상으로 대략적인 IP 주소 체계에 대해서 살펴보았다. 사실 그동안 관심을 안가져서 어려웠지 막상 공부를 하고나니 대략적으로나마 IP 주소 체계에 대해 이해할 수 있었던 계기가 되었다.","link":"http://blog.martinwork.co.kr/theory/2019/12/15/ipv4-address.html","title":"IPv4 주소 체계","pubDate":"2019-12-14T15:50:51.000Z"}},{"node":{"contentSnippet":"아래의 예제를 통해 Kotlin 내에서 특별하게 취급되는 타입 중 하나인 Nothing 타입에 대해 살펴보도록 하자. 그 전에 알고가야할 것은 아래의 예제 코드 속에서 엘비스 연산자의 우항에 return 문이 대입되어 있다는 점이다. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nfun equals(o: Any?): Boolean {\n  val otherPerson = o as? Person ?: return false\n\n  return otherPerson.firstName == firstName &&\n         otherPerson.lastName == lastName\n}\n\nval address = person.company?.address\n      ?: throw IllegalArgumentException(\"No address\")\n\n\n\n지금까지 Javascript만 이용하여 개발을 해왔던 나로서는 제일 이해가 안되는 것 중 하나이기도 하다. 어떻게 문이 값처럼 취급 될 수 있는 것인가? 그러던 중 코틀린의 문서에서 코틀린 내에서는 throw 나 return 등의 연산도 식으로 판단한다. 라는 문구를 읽게 되었다. 그렇기 때문에 위의 코드처럼 엘비스 연산자의 우항에 return, throw 등의 연산을 넣을 수 있던 것이다. 위의 예제 코드와 같이 에러에 해당하는 throw의 경우 Nothing이라는 특별한 타입을 가진다.\nNothing 타입\nnothing 타입은 “이 함수가 정상적으로 끝나지 않는다“ 라는 걸 명시적으로 표현하는 타입이다. Nothing 타입은 아무런 값도 포함하지 않으므로, 함수의 반환 타입 등에 사용한다.\n그 외에는 변수에 아무 값도 저장할 수 없으므로 큰 의미가 없다.\n\n\n1\n2\n3\n4\n5\n6\n\n// 6.1.4_3_Nothing_type.kt\nvar a: String? = null\na = \"테스트\"\nval x: Nothing? = null           // 'x' has type `Nothing?`\nx = \"\" // => Type missmatch\nval l: List<Nothing?> = listOf(null)   // 'l' has type `List<Nothing?>\n\n\n이러한 Nothing 타입은 Nothing 함수를 반환하는 함수가 “정상적으로 종료하지 않음”을 알고 그 함수를 호출하는 함수를 분석할 때 유용하게 사용할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n// 6.1.4_3_Nothing_type.kt\nfun fail(message: String?): Nothing {\n  throw IllegalAccessException(message);\n}\n\nfun main (args: Array<String>) {\n  // 아래와 같이 null 일 경우, 정상적으로 종료하지 않는다는 사실을 쉽게 알 수 있음\n  val address = company.address ?: fail(\"adress\")\n  println(address.city)\n}","link":"http://blog.martinwork.co.kr/kotlin/2019/11/24/nothing-type-in-kotlin.html","title":"Kotlin의 Nothing 타입","pubDate":"2019-11-23T15:50:51.000Z"}},{"node":{"contentSnippet":"React Hooks 톺아보기\nReact 에는 Class 컴포넌트와 Functional 컴포넌트 등 총 2가지 형태로 구분하여 컴포넌트를 작성할 수 있다. 과거 처음 React를 이용하여 개발을 시작할 때는 Functional 컴포넌트가 Presentation 컴포넌트이자, Stateless 컴포넌트이고, Class 컴포넌트가 Container 컴포넌트이자, Stateful 컴포넌트라고 생각하였다. 하지만 공부 안한 나의 잘못된 지식을 가지고 있었다는 걸을 탓하며, Class 컴포넌트도 Presentation 컴포넌트가 될 수 있고, Functional 컴포넌트도 Container 컴포넌트가 될 수가 있었던 걸 깨달았다. 이에 또 Hooks라는 또다른 기술부채가 쌓이는 상황이 발생하게 되었다. 이참에 쌓인 기술 부채의 일부를 청산해보가 학습해보게 되었다.  \nHooks의 개요\n일단 Class 컴포넌트 형태로도 작성할 수 있는 데도 불구하고 Hooks를 사용하는 얻는 이점이 뭔지를 살펴보고자 한다.  \n1. Stateful 코드를 재사용하기가 힘들다.\nClass 컴포넌트로 작성하게 되면 컴포넌트 간의 코드 재사용성을 위해 HOC(고차함수 컴포넌트) 혹은 render props 형태로 작성해야만 했다. 하지만 이러한 코드의 경우 일단 첫번째는 코드가 다소 직관적이지 않다는 단점이 있으며, React devtools에서 디버깅을 하게 되는 경우 아래와 같이  Wrapper Hell 에 빠지는 것을 쉽게 볼 수 있다.\n\n하지만 Hooks는 컴포넌트의 계층 구조와는 상관없이 코드의 재사용성을 높여준다. 이러한 점은 Custom Hooks를 이용하여 이점을 극대화 시킬 수 있다.\n2. Class 컴포넌트의 로직을 관심사에 따라 분리할 수 있다.\nClass 컴포넌트로 컴포넌트를 작성하다보면 각각의 생명 주기(Life cycle) 안에서 여러가지 로직을 추가하는 경우가 있다. 예를 들어 componentDidMount 훅에서는 컴포넌트에 필요한 초기 데이터를 위한 API를 호출하거나 EventListener 를 등록 해줄 수 있다. 뿐만 아니라 이렇게 설정된 데이터 혹은 EventListener 들은 componentWillUnmount 훅에서 제거하거나 데이터를 초기화 해줄 수 있다. 이러한 부분들이 컴포넌트의 생명주기 메소드 내의 가독성과 관심사 분리를 해칠 수 있으며, 이에 따라 버그나 무결성을 해칠 수 있는 여지가 충분하다. \n이러한 이유로 인해 간단하게 작성되기 시작한 Class 컴포넌트도 시간에 따라 점점 더 비대해지며, 정작 중요한 비즈니스 코드가 눈에 한 눈에 들어오지 않게 되는 경우가 발생하게 된다. 관심사별 비즈니스 코드를 분리하기 위해 Redux나 Mobx와 같은 상태 관리 라이브러리를 사용해야 하는 경우가 생긴다. 이러한 문제점에 대해 Hooks는 로직 별로 작은 단위에 컴포넌트에 집중할 수 있는 해결점을 제공해준다.\n3. Class 문법에 대한 진입 장벽을 낮춘다.\n개인적으로는 React에 대한 진입 장벽은 JSX 문법이라고만 생각을 해왔는데, React 공식 문서에 따르면 React 에 대한 진입 장벽 중 하나로 Class 문법을 언급하고 있다. Class로 컴포넌트를 설계하게 되면 this를 이해하지 못하고서는 사용하기가 힘들다. 그래서 bind 메소드를 이용한 this를 바인딩하는 등의 여러가지 작업을 해줘야하는 경우가 있다. 마찬가지로 props, state 혹은 Class 컴포넌트 내의 메소드들을 참조할때도 항상 this 에 대한 스코프가 유지되어야 한다. \n하지만 Hooks의 경우 Functional 컴포넌트 내에서 사용되기 때문에 Class 컴포넌트에 대한 this 장벽을 허물 수 있다는 장점을 가진다. 이뿐만 아니고도 Class 컴포넌트의 경우는 Tree shaking이 원활하게 지원되지 않는 등의 문제를 해결할 수 있다고 있다.\n관련된 자세한 내용은 공식 문서의 Motivation에서 확인할 수 있다.  \nHooks이란 무엇인가?\nHooks는 Functional 컴포넌트도 Hooks를 이용하여 Stateful 컴포넌트가 될 수 있도록 만들어 준다. 리액트의 공식 문서를 살펴보면 다음과 같은 문구를 확인할 수 있다.\n\n\n1\n\nThe Effect Hook lets you perform side effects in function components\n\n\n여기에서 이야기하는 side effects 이란, 데이터를 받아오거나, 데이터를 구독하거나 혹은 DOM을 직접 조작하는 행위를 말한다고 한다. 본격적으로 Hooks에 대해서 살펴보도록 하자.\nuseState\n일단 기본적으로 사용하는 문법은 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n\nconst FunctionalComponent = () => {\n  const [count, counting] = useState(10);\n  return (\n    <div> {count} </div>\n  )\n}\n\n\nuseState 의 인자로는 상태의 초깃값을 주입해줄 수 있으며, 초기값은 Primitive 타입 혹은 Object 타입 상관없이 주입할 수 있다. 이 초기값은 처음 컴포넌트가 랜더링될 경우에만 사용되며, 이후부터는 useState에서 반환하는 Setter 함수를 통해 변경된 값이 State 에 담긴다. useState의 인터페이스를 살펴보면 다음과 같이 되어 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n\n/**\n * Returns a stateful value, and a function to update it.\n *\n * @version experimental\n * @see https://reactjs.org/docs/hooks-reference.html#usestate\n */\nfunction useState<S>(initialState: S | (() => S)): [S, Dispatch<SetStateAction<S>>];\n\n\nuseState는 제네릭 타입 S(initialState)를 인자로 받은 후, 튜플 타입으로 첫번째 값은 State를 반환하며 두번째 값으로는 State를 업데이트할 수 있는 Dispatch 함수를 반환한다. 여기에서 중요한 것은 Dispatch 함수는 Redux의 Dispatch 함수가 아닌 리액트의 인터페이스에 정의되어 있는 Void 타입의 함수이다. \n위의 인터페이서에서 살펴볼 수 있듯, useState 는 State와 Dispatch 함수로 이뤄진 배열을 반환하기 때문에 useState를 사용할 때 구조 분해 할당 구문을 통하여 다음과 같이 사용할 수 있다.\n\n\n1\n\nconst [count, addCount] = useState(10);\n\n\n시간을 카운팅하는 코드를 먼저 Class 컴포넌트 형태로 작성하게 되면 다음과 같이 작성할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\nclass ClassComponent extends React.Component {\n  state = {\n    count: 10\n  }\n  componentDidMount() {\n        // 최초 컴포넌트가 실행될 때, setState를 일으킨다.\n    setTimeout(() => this.setState({ count:  this.state.count - 1}), 1000);\n  }\n  componentDidUpdate(prevProps, prevState, snapshot) {\n        // 이후 state의 변화에 따라 setState를 지속적으로 일으킨다.\n    setTimeout(() => this.setState({ count:  this.state.count - 1}), 1000);\n  }\n\n  render () {\n    return (\n      <div>{this.state.count}</div>\n    )\n  }\n}\n\n\nClass 컴포넌트는 처음 컴포넌트가 마운트되며 setTimeout 함수를 한번 실행할 것 이다. 이후 State 값이 변경됨에 따라 setTimeout 함수가 재호출되며 render 함수를 통해 DOM이 리랜더링 된다.\n하지만 이 코드를 Functional 컴포넌트의 useState를 이용하여 작성하면 다음과 같이 작성할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n\nconst FunctionalComponent = () => {\n  const [count, counting] = useState(10);\n  setTimeout(() => counting(count-1), 1000);\n  return (\n    <div> {count} </div>\n  )\n}\n\n\nFunctional 컴포넌트는 컴포넌트가 랜더링되며 setTimeout 함수를 반복적으로 호출하는 형태로 작성되어진다. \nuseEffect\nuseState Hook을 사용 하다보면 상황에 따라 Class 컴포넌트의 componentDidMount, componentDidUpdate, componentWillUnmount 등과 같은 특정 상황에 Lifecycle 안에서 특정 로직을 실행시켜 줘야 하는 경우가 있다. 예를 들어 컴포넌트가 마운트되었을 때 컴포넌트에 필요한 초기 데이터를 API를 호출을 통해 받아와야 한다던지, 혹은 DOM에 EventListener를 등록하거나 언마운트될 때 등록된 EventListener를 해제해줘야하는 상황이 있다. 이러한 경우 useEffect를 이용한다면 Class 컴포넌트에서 제공해주는 생명주기 API들을 Functional 컴포넌트에서도 마찬가지로 사용할 수 있다.  \n공식 문서를 살펴보면 팁으로 다음과 같이 정보를 제공해준다.\n\n\n1\n2\n3\n\nTip\n\nIf you’re familiar with React class lifecycle methods, you can think of useEffect Hook as componentDidMount, componentDidUpdate, and componentWillUnmount combined.\n\n\n이 말인 즉, Hook의 useEffect를 잘 이해하여 사용한다면 Class 컴포넌트의 componentDidMount, componentDidUpdate, componentWillUnmount 메소드들과 같은 효과를 볼 수 있다는 것이다. 먼저 useEffect 함수의 인터페이스를 살펴보도록 하자. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n/**\n * Accepts a function that contains imperative, possibly effectful code.\n *\n * @param effect Imperative function that can return a cleanup function\n * @param inputs If present, effect will only activate if the values in the list change.\n *\n * @version experimental\n * @see https://reactjs.org/docs/hooks-reference.html#useeffect\n */\nfunction useEffect(effect: EffectCallback, inputs?: InputIdentityList): void;\n\n// InputIdentityList 은 아래와 같이 정의되고 있다.\n// type InputIdentityList = ReadonlyArray<any>;\n\n\nuseEffect 함수는 EffectCallback 함수를 인자로 받으며, 두번째 인자로는 state값의 배열을 받는다. 여기에서 우리가 주의깊게 봐야할 것은 첫번째 인자인 EffectCallback 은 Mandatory 값이며, 두번째 인자인 InputIndetiryList는 optional 값이라는 것이다. 만약 InputIdentityList 값이 변경된다면 EffectCallback 함수가 실행될 것이다.  \nuseEffect로 작성된 컴포넌트를 살펴보기 전에 먼저 우리에게 익숙한 Class 컴포넌트로 해당하는 상황을 살펴보면 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\nclass ClassComponent extends React.Component {\n  state = {\n    user: null\n  }\n  async componentDidMount() {\n    const user = await fetchUser();\n    this.setState({ user });\n  }\n\n  render () {\n    if (!this.state.user) {\n      return null;\n    }\n    return (\n      <dl>\n        <dt>유저의 이름</dt>\n        <dt>{this.state.user.name}</dt>\n        <dt>유저의 나이</dt>\n        <dt>{this.state.user.age}</dt>\n      </dl>\n    )\n  }\n}\n\n\n처음 컴포넌트가 마운트될 때 fetchUser 라는 API 함수를 호출한다. 그 후, 받아온 user의 데이터를 setState를 이용하여 state에 저장을 해주는 형태이다. 이러한 코드를 useEffect 를 이용하여 변경하면 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\nconst FunctionalComponent = () => {\n  const [user, setUser] = useState(null);\n  useEffect(() => {\n    async function fetchData () {\n      const user = await fetchUser();\n      setUser(user);\n    }\n    fetchData();\n  }, []); // <- 빈 배열임을 꼭 기억해야 한다.\n  return user && (\n    <dl>\n      <dt>유저의 이름</dt>\n      <dt>{user.name}</dt>\n      <dt>유저의 나이</dt>\n      <dt>{user.age}</dt>\n    </dl>\n  )\n}\n\n\nuseEffect 함수가 실행되는 시점은 컴포넌트가 처음 마운트되는 시점을 포함하여 컴포넌트가 리랜더링된 이후 매번 실행된다. 하지만 위에서 설명했듯 만약 두번째 인자에 배열 형태의 값을 넣게 되면 useEffect의 콜백함수는 배열의 값에 변화가 있을 경우 실행될 것이다. 위의 예제를 살펴보면 useEffect의 두번째 인자로 빈 배열을 넣어주었는데, 빈 배열을 넣어주게 되면 더이상의 변화가 없기 때문에 EffectCallback 함수는 처음 컴포넌트가 마운트될 때 한번 실행될 것이다. 만약 이것을 감안하지 않고 아래와 같이 인자를 던지지 않으면 DOM이 변경되면서 useEffect 함수가 실행되며 계속 API를 호출하는 상황이 발생할 것이다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\nconst FunctionalComponent = () => {\n  const [user, setUser] = useState(null);\n  useEffect(() => {\n    async function fetchData () {\n      const user = await fetchUser();\n      setUser(user);\n    }\n    fetchData();\n  }); // <- InputIdentityList를 넣어주지 않으면 계속해서 fetchData 함수가 실행되는 상황이 생긴다.\n  return user && (\n    <dl>\n      <dt>유저의 이름</dt>\n      <dt>{user.name}</dt>\n      <dt>유저의 나이</dt>\n      <dt>{user.age}</dt>\n    </dl>\n  )\n}\n\n\n빈 배열이 아닌 아래와 같이 작성한다면 user 데이터가 변경될 때마다 useEffect의 콜백함수가 실행될 것이다. \n\n\n1\n2\n3\n4\n\n// ...\nuseEffect(() => {\n  console.log(\"User State changed!\")\n}, [user])\n\n\n그렇다면 해제 함수를 실행 시켜줘야 할때는 어떻게 해야할까? 그런 경우는 아래와 같이 EffectCallBack 함수에서 해제 함수를 반환해주면 된다. \n\n\n1\n2\n3\n4\n5\n\nuseEffect(() => {\n  return () => {\n    // 해제 함수. \n  }\n});\n\n\n이러한 해제 함수는 컴포넌트가 언마운트될 때 뿐만 아니라, 재랜더링될때도 마찬가지로 실행된다. 만약 언마운트될 때만 실행시켜주고자 한다면 위에서와 같이 두번째 인자를 빈 배열로 주입하게 되면 useEffect 의 EffectCallback 함수는 처음 마운트 될 때 한 번, 언마운드 될때 한번 해서 두번만 실행되며 DOM이 업데이트 되지 않는다. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\nuseEffect(() => {\n  console.log(\"Component did mount\");\n  async function fetchData () {\n    const user = await fetchUser();\n    setUser(user);\n  }\n  fetchData();\n  return () => {\n    console.log(\"Component will unmount.\")\n  }\n}, []);\n\n\n끝으로\n이 외에도 useContext나 useReducer 같은 또다른 Hook들도 있으며, 필요에 따라 상황에 맞게 사용하면 되지 않을까 한다. 다만 사용하기 전에 대략적으로나마 동작하는 방식에 대해 이해하고 있어야 문제가 생겼을 때 문제를 해결할 수 있지 않을까라고 생각한다.","link":"http://blog.martinwork.co.kr/react/2019/11/10/react-hooks.html","title":"React Hooks 톺아보기","pubDate":"2019-11-09T15:50:51.000Z"}},{"node":{"contentSnippet":"이 포스팅에서 사용한 모든 소스는 깃헙에서 확인할 수 있습니다. \n\n이전 포스팅에서는 로컬에서 NodeJS 프레임워크 중 하나인 Express를 이용하여 API 서버를 만들 수 있도록 기본적인 환경을 구축해보았다. 사실 내가 이 포스팅을 쓰며 정리하고자 했던 내용은 이제부터 본격적인 시작이 아닐까 싶다.\n\nTypescript 적용하기\n이전의 다른 포스팅에서도 여러번 언급을 했지만 나는 서비스 개발 시 Typescript를 이용하여 개발을 하는 것이 서비스의 안정성을 높이는 일이라고 생각한다. 그렇기 때문에 역시나 MOCK 서버에서도 마찬가지로 Typescript를 이용하여 개발할 수 있도록 설정을 해줄 예정이다. 여기에서의 핵심은 클라이언트에서 정의한 모델(Model)에 대한 타입과 서로 타입을 공유하여 MOCK 서버의 응답값(Response)와 타입을 유기적으로 잘 엮어주는 것이다. 하지만 이번 포스팅에서 이러한 내용까지 다루기에는 클라이언트 코드까지 작성해야한다는 번거로움이 있기 때문에 별도로 다루진 않는다. 만약 이러한 상황이 생긴다면 꼭 서로 유기적으로 연결하여 서로의 타입에 대해 실시간으로 검증할 수 있도록 작성하도록 하자(실제 회사에 적용된 MOCK 서버 역시 말한 것과 같이 클라이언트와 모델(Model)에 대한 타입을 공유하여 사용하고 있다.)\n일단 먼저 타입스크립트를 Express에서 사용할 수 있도록 필요한 모듈을 설치한다.\n\n\n1\n\n$ npm install typescript ts-node @types/node @types/express --save-dev\n\n\n설치가 되었다면 앞서 추가한 nodemon에 ts-node를 사용할 수 있도록 수정해보도록 하자. 수정되기 전은 아마도 아래와 같이 설정되어 있을 것이다.\n\n\n1\n2\n3\n4\n5\n\n{\n  \"scripts\": {\n    \"start\": \"nodemon ./bin/www\"\n  }\n}\n\n\n이 코드를 아래와 같이 변경하면서 www 파일의 확장자를 ts로 변경해준다.\n\n\n1\n2\n3\n4\n5\n\n{\n  \"scripts\": {\n    \"start\": \"nodemon --exec 'ts-node' bin/www.ts\"\n  }  \n}\n\n\n변경된 후에는 tsconfig.json 파일을 하나 생성하여 각자의 입맛에 맞는 설정을 추가한다. 이 포스팅에서는 tsc --init 옵션을 통해 생성해주는 기본 tsconfig.json 파일을 사용한다. 만약 tsc 옵션을 사용하고자 한다면 typescript를 전역으로 설치해주면 사용할 수 있으니 참고하도록 하자.\n\n\n1\n\n$ tsc --init\n\n\n명령어를 실행시켜주면 다음과 같이 프로젝트의 최상위 루트에 tsconfig.json 파일이 생성되는 것을 볼 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n\n{\n  \"compilerOptions\": {\n    /* Basic Options */\n    \"target\": \"es5\",                          /* Specify ECMAScript target version: 'ES3' (default), 'ES5', 'ES2015', 'ES2016', 'ES2017','ES2018' or 'ESNEXT'. */\n    \"module\": \"commonjs\",                     /* Specify module code generation: 'none', 'commonjs', 'amd', 'system', 'umd', 'es2015', or 'ESNext'. */\n    // \"lib\": [],                             /* Specify library files to be included in the compilation. */\n    // \"allowJs\": true,                       /* Allow javascript files to be compiled. */\n    // \"checkJs\": true,                       /* Report errors in .js files. */\n    // \"jsx\": \"preserve\",                     /* Specify JSX code generation: 'preserve', 'react-native', or 'react'. */\n    // \"declaration\": true,                   /* Generates corresponding '.d.ts' file. */\n    // \"declarationMap\": true,                /* Generates a sourcemap for each corresponding '.d.ts' file. */\n    // \"sourceMap\": true,                     /* Generates corresponding '.map' file. */\n    // \"outFile\": \"./\",                       /* Concatenate and emit output to single file. */\n    // \"outDir\": \"./\",                        /* Redirect output structure to the directory. */\n    // \"rootDir\": \"./\",                       /* Specify the root directory of input files. Use to control the output directory structure with --outDir. */\n    // \"composite\": true,                     /* Enable project compilation */\n    // \"removeComments\": true,                /* Do not emit comments to output. */\n    // \"noEmit\": true,                        /* Do not emit outputs. */\n    // \"importHelpers\": true,                 /* Import emit helpers from 'tslib'. */\n    // \"downlevelIteration\": true,            /* Provide full support for iterables in 'for-of', spread, and destructuring when targeting 'ES5' or 'ES3'. */\n    // \"isolatedModules\": true,               /* Transpile each file as a separate module (similar to 'ts.transpileModule'). */\n\n    /* Strict Type-Checking Options */\n    \"strict\": true,                           /* Enable all strict type-checking options. */\n    // \"noImplicitAny\": true,                 /* Raise error on expressions and declarations with an implied 'any' type. */\n    // \"strictNullChecks\": true,              /* Enable strict null checks. */\n    // \"strictFunctionTypes\": true,           /* Enable strict checking of function types. */\n    // \"strictPropertyInitialization\": true,  /* Enable strict checking of property initialization in classes. */\n    // \"noImplicitThis\": true,                /* Raise error on 'this' expressions with an implied 'any' type. */\n    // \"alwaysStrict\": true,                  /* Parse in strict mode and emit \"use strict\" for each source file. */\n\n    /* Additional Checks */\n    // \"noUnusedLocals\": true,                /* Report errors on unused locals. */\n    // \"noUnusedParameters\": true,            /* Report errors on unused parameters. */\n    // \"noImplicitReturns\": true,             /* Report error when not all code paths in function return a value. */\n    // \"noFallthroughCasesInSwitch\": true,    /* Report errors for fallthrough cases in switch statement. */\n\n    /* Module Resolution Options */\n    // \"moduleResolution\": \"node\",            /* Specify module resolution strategy: 'node' (Node.js) or 'classic' (TypeScript pre-1.6). */\n    // \"baseUrl\": \"./\",                       /* Base directory to resolve non-absolute module names. */\n    // \"paths\": {},                           /* A series of entries which re-map imports to lookup locations relative to the 'baseUrl'. */\n    // \"rootDirs\": [],                        /* List of root folders whose combined content represents the structure of the project at runtime. */\n    // \"typeRoots\": [],                       /* List of folders to include type definitions from. */\n    // \"types\": [],                           /* Type declaration files to be included in compilation. */\n    // \"allowSyntheticDefaultImports\": true,  /* Allow default imports from modules with no default export. This does not affect code emit, just typechecking. */\n    \"esModuleInterop\": true                   /* Enables emit interoperability between CommonJS and ES Modules via creation of namespace objects for all imports. Implies 'allowSyntheticDefaultImports'. */\n    // \"preserveSymlinks\": true,              /* Do not resolve the real path of symlinks. */\n\n    /* Source Map Options */\n    // \"sourceRoot\": \"./\",                    /* Specify the location where debugger should locate TypeScript files instead of source locations. */\n    // \"mapRoot\": \"./\",                       /* Specify the location where debugger should locate map files instead of generated locations. */\n    // \"inlineSourceMap\": true,               /* Emit a single file with source maps instead of having a separate file. */\n    // \"inlineSources\": true,                 /* Emit the source alongside the sourcemaps within a single file; requires '--inlineSourceMap' or '--sourceMap' to be set. */\n\n    /* Experimental Options */\n    // \"experimentalDecorators\": true,        /* Enables experimental support for ES7 decorators. */\n    // \"emitDecoratorMetadata\": true,         /* Enables experimental support for emitting type metadata for decorators. */\n  }\n}\n\n\n이와 같이 타입스크립트에 대한 config 파일이 생성되었다면 한번 Express 앱을 실행시켜보도록 하자. 만약 이 포스팅과 동일하게 실행했다면 다음과 같이 에러가 발생할 것이다.  \n\n이러한 이유는 기존의 Javascript로 작성해준 코드를 Typescript 파일로 변경하면 적어주지 않은 인자에 대한 타입이 지정되어 있지 않아 컴파일시 발생하는 에러이다. bin/www.ts 파일의 normalizePort 함수와 onError 함수의 인자(Parameter)에 다음과 같이 타입을 지정해주면 해당 문제를 해결할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n// ErrnoException 타입을 추가한다\nimport ErrnoException = NodeJS.ErrnoException;\n\n// normalizePort 함수\nfunction normalizePort(val: string) {\n  // val 파라미터에 string 타입을 지정한다. \n  // ...\n}\n// onError 함수\nfunction onError(error: ErrnoException) {\n  // error 파라미터에 ErrnoException 타입을 지정한다.\n  // do something\n}\n\n\n이와 같은 방법으로 기존에 생성되어져있는 js 확장자를 ts 확장자로 변경하며, 애러를 하나씩 잡아보도록 하자.\n샘플 API Entry 포인트 생성하기\n현재는 서버가 실행되고 있는 localhost:3000으로 접속하면 Express의 화면이 노출될 것이다. 하지만 해당 포스팅에서 Express는 API 서버로만 이용할 예정이기 때문에 화면(View)나 혹은 스태틱 리소스(Static resource)에 대한 코드는 필요가 없다. 그렇기 때문에 API와 무관한 코드는 모두 삭제해주도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n// ...\n// View 템플릿이 위치하고 있는 views 디렉토리와 view 템플릿 엔진을 설정하는 코드를 삭제한다.\n// app.set('views', path.join(__dirname, 'views'));\n// app.set('view engine', 'pug');\n\n// 정적 리소스 파일을 서빙(Serving) 해주는 코드 역시 삭제한다.\n// app.use(express.static(path.join(__dirname, 'public')));\n\n// ...\n\n\n불필요한 파일을 삭제한 후, routes 디렉토리 안의 index.ts 파일을 아래와 같이 수정한다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\nimport { NextFunction, Request, Response } from \"express\";\n\nconst express = require('express');\nconst router = express.Router();\n\n/* GET home page. */\nrouter.get('/', function(req: Request, res: Response, next: NextFunction) {\n  res.json({ title: 'Express' });\n});\n\nmodule.exports = router;\n\n\n파일을 수정한 후 localhost:3000 디렉토리에 접속해보면 설정한 응답값을 반환받을 수 있다.\nExpress 앱을 배포할 때는…\nNode 서버의 경우 에러 발생 시, 서버가 쉽게 죽어 만약 재시작 되도록 처리되어 있지 않은 상황에서 서비스에 올렸다면 초유의 사태가 생길지도 모른다. 그렇기 때문에 앱을 재시작해주는 프로세스 관리할 수 있는 모듈이 필요하다. 물론 편리한 개발 환경 구축을 위해 Nodemon을 설치하기는 했지만, Nodemon의 경우 development에서만 사용해야 한다. 그래서 실제 NodeJS 기반의 서버를 배포할 때 여러가지 Process Manager 중 하나를 선택하여 프로세스들을 관리한다. 대표적으로 PM2, Forever, StrongLoop Process Manager 등이 있다. (관련 비교 링크) 이 중 꾸준한 업데이트와 모니터링의 편의성, 로깅, 클러스터링 지원 등으로 인해 개인적으로는 PM2를 주로 이용을 한다. PM2는 Process Manager의 약자로서 NodeJS의 프로세스를 관리해줄 프로세스 관리자 중 하나이다. 먼저 간단하게 명령어를 살펴보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n# 1. 프로세스를 실행할 때\n# 기본 명령어\n$ pm2 start index.js\n# 애플리케이션의 이름을 지정할 경우\n$ pm2 start index.js --name \"my-app\"\n\n# 2. PM2를 이용하여 시작된 애플리케이션 리스트를 볼 경우\n$ pm2 list\n\n# 3. PM2 애플리케이션 프로세스를 종료할 경우\n$ pm2 stop {application_id}\n\n# 4. PM2에 등록된 애플리케이션을 삭제할 경우\n$ pm2 delete {application_id}\n\n# 5. PM2 애플리케이션의 상세 정보를 조회할 경우\n$ pm2 show {application_id}\n\n\n간단한 명령어이지만 이러한 명령어들을 추가해주기 위해서는 package.json 의 scripts 안에 한줄로 작성해줘야 한다. 간단한 앱이라면 명령어 한줄로 관리해도 무방하지만 실제 애플리케이션을 PM2를 이용하여 올리기 시작하다보면 적용해야할 옵션들이 많아지고 그 때마다 추가하게된다면 점점더 관리가 힘들어질 가능성이 다분하다. 다행이도 PM2는 이러한 옵션들에 대하여 별도의 설정 파일을 Javascript, JSON 그리고 YAML 형식에 따라 각각 지원한다. \n\n\n1\n\n$ pm2 ecosystem\n\n\n위와 명령어를 치면 아래와 같이 ecosystem.config.js 파일이 생성된다. 자바스크립트의 경우 파일명은 큰 상관은 없으나 꼭 끝이 .config.js 로 끝나야 한다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\nmodule.exports = {\n  apps : [{\n    name: 'API', // 애플리케이션 이름\n    script: 'app.js', // PM2를 실행시켜줄 스크립트 파일의 경로\n\n    // Options reference: https://pm2.io/doc/en/runtime/reference/ecosystem-file/\n    args: 'one two', //\n    instances: 1, // 설치될 앱 인스턴스가 갯수\n    autorestart: true, // 앱 크러쉬 등으로 인해 앱이 종료될 때 재시작할지 여부 (default: true)\n    watch: false, // 폴더 내의 파일에 변경이 있을때, 앱이 리로딩 여부\n    max_memory_restart: '1G',\n    env: { // 앱의 env를 설정\n      NODE_ENV: 'development'\n    },\n    env_production: {\n      NODE_ENV: 'production'\n    },\n    log_date_format: \"YYYY-MM-DD HH:mm Z\",\n    out_file: \"logs/out.log\"\n  }],\n\n  deploy : {\n    production : {\n      user : 'node',\n      host : '212.83.163.1',\n      ref  : 'origin/master',\n      repo : 'git@github.com:repo.git',\n      path : '/var/www/production',\n      'post-deploy' : 'npm install && pm2 reload ecosystem.config.js --env production'\n    }\n  }\n};\n\n\n여기에서 ㅇ 것은 env에 대한 설정이다.\n다만 여기에서 중요한 것은 이 포스팅과 같이 Typescript로 개발된 앱을 서버로 올릴 때는 결국 Webpack을 통해 빌드를 거친 후 서버를 띄우게 된다. Webpack 4의 경우 mode를 통해 ‘development’, ‘production’, ‘none’ 등을 NODE_ENV로 지원을 해주게 된다. 그리고 무엇보다 wepback의 빌드를 거친 후에는 해당하는 값이 변수로 치환되어 런타임에서 결정되는 것이 아니라 String 형태로 변환 되어지기 때문에 PM2의 env 값중 NODE_ENV가 정상적으로 동작하지 않는다. 그렇기 때문에 만약 이러한 값을 설정해주고 싶다면 NODE_ENV가 아닌 다른 값을 통해 환경 변수를 가지고 들어갈 수 있도록 설정을 해줘야 한다. \n설정 파일이 생성되었다면 다음과 같이 프로세스를 쉽게 관리 할 수 있다. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n# Start all applications\n$ pm2 start ecosystem.config.js\n\n# Start only the app named worker-app\n$ pm2 start ecosystem.config.js --only worker-app\n\n# Stop all\n$ pm2 stop ecosystem.config.js\n\n# Restart all\n$ pm2 start   ecosystem.config.js\n## Or\n$ pm2 restart ecosystem.config.js\n\n# Reload all\n$ pm2 reload ecosystem.config.js\n\n# Delete all\n$ pm2 delete ecosystem.config.js\n\n# 특정 프로세스만 실행시킬 경우\n$ pm2 start   ecosystem.config.js --only api-app\n\n\n출처\n\n[PM2 공식 홈페이지] (http://pm2.keymetrics.io/docs/usage/application-declaration/#environment-definition)","link":"http://blog.martinwork.co.kr/nodejs/2019/09/20/nodejs-with-typescript02.html","title":"Typescript와 NodeJS를 이용한 간단한 목킹 서버 띄우기 02","pubDate":"2019-09-19T15:50:51.000Z"}},{"node":{"contentSnippet":"이 포스팅에서 사용한 모든 소스는 깃헙에서 확인할 수 있습니다. \n\n최근 이직을 하며 새로운 조직에 합류하게 되었다. 이 새로운 조직에서는 새로운 팀원이 합류하였을 때, 새로운 팀원에게 비즈니스와 무관한 플랫폼에 대한 미션을 주는데 이번에 나에게 주어진 미션은 프론트 엔드 개발시 필요한 목 서버(MOCK Server)를 구축 하는 미션이었다. 아마 프론트 엔드 개발을 해본 사람이라면 한 번쯤은 백엔드 API가 개발되어지지 않은 상태에서 먼저 개발을 시작해야하는 곤란한 상황에 처해본적이 있을 것이다.     \n\n(이거 너무 한거 아니냐고)\n그래도 하늘이 무너져도 솟을 구멍은 있다고 이러한 상황에 닥치면 보통 백엔드 개발자와 API에 대한 인터페이스 미리 협의 후 진행하기 때문에 어찌되었던 개발은 진행할 수 있을 것이다. 이후에는 아마 2가지 선택권으로 나뉠 것이다. 먼저 별도의 API 레이어를 분리해놓고 쓴다면 그냥 해당 API 레이어의 함수에서 정해져있는 결과값을 반환해주면 될 것이다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\nfunction fetchTodo () {\n  return {\n    isSuccess: true,\n    message: \"\",\n    data: [\n        { id: \"1\", text: \"투두 API 샘플1\", isDone: true },\n        { id: \"2\", text: \"투두 API 샘플2\", isDone: false },\n    ]    \n  }\n}\n\n\n개발 서버의 API가 나오기 전까진 일단은 Vuex던 Redux던 API 레이어 함수를 호출하여 개발을 진행하다 API 서버가 나오면 API 레이어 코드를 AJAX로 호출하는 형태로 변경하면 된다. 하지만 개발을 하다보면 모든 상황이 낙관적이지 않듯, 개발 서버의 API 배포되어도 여러 비즈니스의 개발 소스가 모이기 때문에 개발 서버가 제대로 돌아가리란 보장이 되지 않는다. 백엔드 API의 코드를 분석할 수 있다고 하더라도 여러 비즈니스가 얽키고 설켜있기 때문에 이것이 내가 사용하는 API가 문제인지, 아니면 내가 사용하는 API가 의존하고 있는 다른 API에서 문제가 생겼는지 알 수가 없다.  \n\n그렇다고 강 건너 불구경 하듯 나올 때까지 기다릴 수도 없을 따름이니 다시 API 레이어 코드를 AJAX를 통하지 않도록 수정해야 한다. 이러한 상황이 반복되다보면 비즈니스 코드를 수정하는 게 아니라 별도의 목킹 서버의 필요성을 느끼게 된다. \n그래서 이참에 한번 NodeJS와 타입스크립트를 이용한 간단한 API 서버를 구축해보며 그 과정을 정리해보고자 한다. 만약 이러한 과정이 귀찮다면 JSON Server를 이용하는 것도 하나의 방법이다.\nNodeJS 서버 환경 구축\n일단 NodeJS의 서버 프레임워크 중 유명한 프레임워크인 Express를 이용하여 API 서버를 구축해볼 예정이다. 간단하게 Express Generator를 이용하여 환경을 구축한다. Express generator가 설치되어 있지 않다면 express 명령어를 사용할 수 있도록 express-generator를 전역적으로 설치한다.\n\n\n1\n\n$ npm install express-generator -g\n\n\n설치가 되었다면 아래와 같이 환경을 구축한다. \n\n\n1\n\n$ express --view=pug nodejs-express-typescript-sample\n\n\nAPI 서버이기 때문에 view 템플릿을 무엇이든 상관없다. 위의 명령어를 이용하면 Express 서버를 띄우기 위한 모든 코드가 자동으로 생성된다. 생성되었다면 package.json에 있는 모듈을 npm install 명령어를 이용하여 설치해준다. 설치되었다면 다음으로 Nodemon에 대해서 살펴보도록 하자.\nNodemon 이용하기\nNodemon은 마치 webpack-dev-server와 같이 NodeJS 서버의 코드가 수정되면 자동으로 서버를 재시작시켜주는 모듈이다. Nodemon을 이용하면 별도의 설정 없이 간단하게 이용할 수 있다. \nnodemon을 이용하여 앱을 실행할 경우, 아래의 명령어를 이용하여 간편하게 앱을 실행시킬 수 있다.\n\n\n1\n\nnodemon [your node app]\n\n\n단, nodemon의 경우 개발할 경우 사용하고 실제 서비스에 올릴 때는 PM2 모듈을 사용하는 것을 권장한다. PM2에 대해서는 뒤에서 따로 다루도록 한다. nodemon을 이용하기 위해서는 nodemon을 전역으로 설치 해야 하지만, NPX를 이용한다면 전역적으로 설치하지 않고도 해당 명령어를 사용할 수 있다. NPX는 Create-React-App을 이용해봤다면 한번쯤은 다들 접해봤을 것이다. NPX는 npm@5.2+ 이상의 버전에는 자동으로 추가되는 도구로서 주로 npm run scripts 없이 패키지 바이너리를 쓸 경우 사용하게 된다. \n예를 들어, npx를 사용하지 않았을 경우 create-react-app을 사용해야한다고 하면 다음과 같이 create-react-app을 글로벌로 설치한 후 사용해야 한다.\n\n\n1\n\n$ npm install create-react-app -g\n\n\n물론 create-react-app의 경우 큰 문제가 되지 않을 수 있지만, mocha나 grunt, bower, webpack 등과 같은 툴의 경우 프로젝트 별로 버전을 관리하기가 힘들 수 있다. 또 이런 경우 다른 버전의 패키지를 사용하려고 한다면 해당 패키지를 새로 설치해야 한다.\n물론 이러한 문제점을 scripts에 포함하여 사용하면 문제없이 사용할 수 있지만, 이보다 더 편한 방법으로 npx를 이용할 수도 있다.\n일단 먼저 NPX를 사용하기 전 NPX가 패키지 바이너리를 사용하는 순서를 살펴보면 다음과 같다.\n\n\n1\n2\n\n1. 해당하는 프로젝트의 node_modules/.bin 안에 존재하는 패키지를 기준으로 호출한다.\n2. 로컬에 해당 패키지가 없다면 npx 레지스트리에서 해당 이름의 패키지를 자동으로 다운로드 받은 후 호출한다.\n\n\nNPX는 패키지 명령어를 위와 같은 방법으로 실행시키기 때문에 Global로 패키지를 설치하지 않아도 해당하는 패키지의 명령어를 사용할 수 있다. 다만 2번과 같이 자동으로 다운로드 받은 후 호출된다고 하더라도 전역적으로 설치를 시키는 게 아니라, 패키키를 호출한 후 삭제해버리니 글로벌로 설치되지 않을까 하는 걱정은 하지 않아도 된다. 다만 매번 명령어를 실행할 때마다 설치를 진행하므로 명령어를 한번 실행할 때 시간이 오래 걸리기 때문에 로컬 프로젝트에 설치하여 사용하는 것을 권장한다.\n일단 nodemon을 사용할 수 있도록 로컬에 설치해주도록 하자.\n\n\n1\n\n$ npm install nodemon --save-dev\n\n\n설치되었다면 npm 명령어 한번으로 실행시킬 수 있도록 scripts에 명령어를 수정한다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n{\n  \"name\": \"nodejs-express-typescript-sample\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"scripts\": {\n    // \"start\": \"node ./bin/www\",\n    \"start\": \"nodemon ./bin/www\"\n  },\n  \"dependencies\": {\n    \"body-parser\": \"~1.18.2\",\n    \"cookie-parser\": \"~1.4.3\",\n    \"debug\": \"~2.6.9\",\n    \"express\": \"~4.15.5\",\n    \"morgan\": \"~1.9.0\",\n    \"pug\": \"2.0.0-beta11\",\n    \"serve-favicon\": \"~2.4.5\"\n  },\n  \"devDependencies\": {\n    \"nodemon\": \"^1.19.2\"\n  }\n}\n\n\n그 후에 start 명령어를 이용해 정상적으로 서버가 재실행이 되는지 확인한다. 여기까지 확인이 되었다면 로컬 내에서 개발하기 위한 NodeJS를 이용한 Mock API 서버에 대한 기본적인 환경은 갖춰졌다. 이후의 포스팅에서는  타입스크립트와 레디스(Redis)를 이용하여 Mock 데이터를 키-벨류(Key-Value) 형태로 저장할 수 있는 API 서버를 구축할 예정이다. 만약 간단하게 SUCCESS(성공)/FAILURE(실패)에 대한 결과만 클라이언트 앱에 던져주고자 한다면 여기까지 환경을 구축하여 컨트롤러(Controller)에 각각의 응답값을 정의해서 써보도록 하자. 이후의 단계에 대해서 궁금하신 분은 다음 포스트을 살펴보도록 하자. \n\n출처\n\nPM2 공식 홈페이지","link":"http://blog.martinwork.co.kr/nodejs/2019/09/08/nodejs-with-typescript01.html","title":"Typescript와 NodeJS를 이용한 간단한 목킹 서버 띄우기 01","pubDate":"2019-09-07T15:50:51.000Z"}},{"node":{"contentSnippet":"들어가기 전\n개인적으로 나는 타입스크립트를 아주 좋아한다. 좋아하는 데는 여러가지 이유가 있겠지만 가장 큰 이유는 아무래도 타입을 통해 버그를 쉽게 잡을 수 있다 라는 점 때문이었다. 타입스크립트를 이용하면 얻는 이점에 대해서는 해당 포스팅에서 따로 다루진 않는다. 이 글을 읽는 분이라면 대부분 이미 타입스크립트를 사용하며 얻는 이점에 대해 공감을 하여 타입스크립트를 도입하여 사용하고 있다고 생각한다. \n하지만 그런 나조차도 가끔은 회의감이 들때가 있으니 그것은 바로 라이브러리에 대한 인터페이스이다. \n \n물론 과거에 비해 요즘은 많은 라이브러리에 d.ts 파일이 존재하기도 하지만, 아직은 제로 None d.ts 시대는 아닌 것 같다. \n\nd.ts 파일은 말 그대로 구현되어져 있는 모듈에 대한 타입 선언 파일이다. 그러므로 모듈 구현과 타입 선언은 명백하게 분리해서 생각해야 한다. 모듈 구현 코드는 런타임에서 실제 돌아가는 동작부 코드이며, 타입 선언의 경우에는 컴파일되는 과정에서의 타입 검증을 위해 사용된다.\n\n이러한 경우에 직면할 때는 자바스크립트 라이브러리에 대해서 직접 인터페이스를 선언해줘야 한다. 물론 가끔은 다음과 같이 any 키워드를 이용하여 사용하는 경우도 있을 것이다. 귀찮음을 대변하는 타이핑이 아니라는 변명은 하지 않겠다\n\n\n1\n2\n3\n\ndeclare var $: any;\n\nexport default $;\n\n\n물론 jQuery 역시 @types가 존재하기는 하지만, 과거의 최신 버전의 jQuery가 아닌 경우는 미묘하게 타입이 일치하지 않는다. 한 예로 T사에 재직 중일 당시 리뉴얼 프로젝트에서 새로 개발되는 페이지의 경우 ReactJS + Typescript 조합을 이용하여 개발하였지만, 기존의 코드와의 의존성을 끊을 수가 없었다. 하지만 기존의 코드는 1.x.x대의 jQuery를 사용하고 있었는데, 이러한 코드에 대한 타입이 없기도 했으며 부분적인 코어 모듈에서만 사용하는 라이브러리이기 때문에 any 키워드로 뭉트그려도 타입 검증이 이뤄지지 않아도 상관이 없었다. \n\n사실 써드 파티 라이브러리의 경우, 대부분 자체적으로 단위 테스트가 붙어 있을 정도로 어느 정도 검증된 코드들인 경우가 많다. 물론 버그가 아예 없는 경우는 없지만, 어느 정도의 궤도에 올라온 라이브러리인 경우 이미 이슈로 등록되어있던가 혹은 스택 오버플로우에 해결 방법이 거의 대부분 나와있다. 그래서 시간이 부족하거나 정말 타이핑하기가 귀찮을 경우는 any 키워드로 때려넣은 적도 있다.\n\n하지만 문제는 어플리케이션 내에 강력한 영향을 미치는 라이브러리이다. 일단 본격적으로 타이핑을 해보기 전에 선행 작업이 필요하다. tsconfig.json 파일에 다음과 같이 typeRoots에 타입 선언해줄 디렉토리를 추가한다. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n{\n  \"compilerOptions\": {\n    // ...\n    \"typeRoots\": [\n      \"./node_modules/@types/\",\n      \"./@types\" // 타입 선언해줄 디렉토리를 추가해준다.\n    ],\n    // ...\n  },\n  // ...\n}\n\n\n본격적으로 타이핑 해보기\n앞서와 같이 @types 디렉토리를 생성했다면 다음으로 타입을 선언해줄 d.ts 파일을 생성한다. 그 다음으로 사용하려는 라이브러리에 대하여 declare module 키워드를 이용하여 선언한다. 본 포스팅에서의 예제는 현재 사이드로 개발 중인 에디터 플랫폼에서 사용하는 써드 파티 라이브러리 중 하나인 editorjs의 의존성을 가진 라이브러리를 이용하여 작성하였다.  \n\n\n1\n\ndeclare module '@editorjs/header';\n\n\n일단 이렇게만 작성해도 거의 대부분이 끝났다. 위와 같이 선언만 해준다면 애플리케이션 내에서 문제 없이 사용할 수 있다. 하지만 이렇게 선언된 써드 파티 라이브러리의 타입은 any 타입을 가지게 된다. 만약 써드 파티 라이브러리의 타입 선언에 대해 더이상의 시간을 투자하고 싶지 않다거나 시간 관계상 현재 타이핑 하는 것이 현실적으로 불가능하다면 위와 같이 선언해도 문제는 없을 것이다.\n하지만 개인적으로는 이러한 방식은 타이핑에 대한 문제를 해결하기 보단 그냥 회피하는 방법이라고 생각이 들었다.   \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\ndeclare module '@editorjs/header' {\n  import { BlockToolConstructable } from \"@editorjs/editorjs\";\n  interface HeaderConstructor extends BlockToolConstructable {\n    // 이후 추가되는 인터페이스에 대해서는 해당 영역에 작성한다. \n  }\n\n  const k: HeaderConstructor;\n  export = k;\n}\n\n\n다행히도 내가 사용하는 editorjs 라이브러리 안에는 각각의 tools에 해당하는 constrructor에 대한 인터페이스가 존재하였기 때문에 그 안에서 상속받는 형태로 작성하였다. 이후부터 editorjs에 추가되는 플러그인에 대해서는 각각의 타입 선언 파일 내에서 선언하면 되지 않을까 한다. \n마치며\n대략적으로 자바스크립트 라이브러리에 타이핑을 하는 방법을 살펴보았다. 과거 불과 1년여 전만 해도 이러한 써드 파티 라이브러리에 대해 타이핑하는 것 때문에 타입스크립트 도입을 꺼려하는 개발자들이 많았다. 하지만 더 이상 타입스크립트의 도입에 반대하는 의견 중 가장 큰 원인이 써드 파티 라이브러리에 대한 타입이 원인이 되기는 힘들다는 생각이 든다. 물론 이것조차도 누군가에겐 부정적인 의견을 가질 수 있겠지만…\n\n출처\n\n타입스크립트 공식 홈페이지\n네이버 FE 플랫폼\nd.ts 만들기","link":"http://blog.martinwork.co.kr/typescript/2019/08/25/declare-javascript-library-in-typescript.html","title":"Javascript 라이브러리에 타이핑하기","pubDate":"2019-08-24T15:50:51.000Z"}},{"node":{"contentSnippet":"들어가기 전\n이번에 처음 책을 집필하며 책 관련한 스터디를 여러 차례 진행하였습니다. 당시 스터디원을 모집하면서 간단하게나마 설문을 받았는데, 설문에 작성해준 내용에는 생각보다 많은 분들의 버킷리스트 혹은 목표가 책 집필인 경우가 많다는 것을 느꼈습니다. 그래서 이 글은 미래의 저자 분들을 위해 작성하였습니다. 이 글이 책을 집필하며 어려운 점이나 궁금한 점 혹은 조언이 필요한 분들에게 조금이나마 도움이 되었으면 좋겠고, 혹시라도 이외에도 도움이 필요하신 분은 언제든 이 글에 댓글 혹은 메일로 연락을 주시면 최대한 빨리 회신 드리도록 하겠습니다. 😁\n\n일단 먼저 이 책을 쓰기까지 도움을 주신 김용기님, 백재연님, 이지만님, 김지영님, 김상열님, 박은정님, 윤경선님, 배수향님, 박지윤님, 이정재님, 박철현님, 허승님 모두에게 감사하며, 이 외에도 언급하진 않았지만 도와주신 모든 분들에게 이 기회를 빌어 감사하다는 말씀을 드립니다 :) (물론 이분들이 이 글을 볼지는 모르겠지만…😜) 무엇보다 공동 집필에 대한 제안을 흔쾌히 수락하고 10개월이 넘는 시간을 함께 고생해준 문동욱에게 고맙다는 인사 전합니다.\n뜻밖의 기회\n과거 2018년의 회고에도 썼지만, 나에게 생각치도 못한 기회가 찾아왔다. 2018년 8월 30일, 뜬금없이 스팸 메일이 의심되는 메일을 받았다. 🤔\n\n그럴 수 밖에 없는게 일단은 이미 유명한 수많은 개발자들도 많은데 굳이 하찮은 나에게 연락을 했을까라는 생각이 앞섰기 때문이다. 무엇보다 나를 어디서 보고 연락을 했을까라는 의심이 들어 회신을 보내니, 과거의 올린 블로그 글을 우연치 않게 보게 되어 연락을 했다고 회신이 왔다. 아마 지인 개발자들에게 블로그의 중요성을 전파하기 시작한 때가 이때 이후부터였던 것 같다. 😎 \n\n(이미 많은 개발자들을 위한 자기계발서 책에서도 블로그에 대한 긍정적인 효과에 대해서 설명을 하고 있기 때문에 굳이 여기에서 다시 언급하진 않지만, 사실 나는 직접적으로 블로그의 긍정적인 효과를 경험했기 때문에 더더욱 블로깅에 대해 적극적으로 홍보하고 있다.)\n일단은 주변의 지인 개발자 분들도 책 집필 문의에 대해서 메일로 받았다라는 이야기를 몇 번 듣고는 의심의 눈초리가 한풀 식은 것도 있었다. 여튼 메일에 대한 확신이 든 순간에 큰 고민 없이 제안을 수락하였다. \n\n일단 받은 책 진행 프로세스에 대해서 받은 제안은 다음과 같았다.  \n\n👉 최소 6개월에서 한두달 정도 조율하여 집필을 진행했으면 좋겠다.\n👉 시작 전, 출판사쪽에서 보내주는 양식에 맞게 집필 계획서를 제출했으면 좋겠다.\n👉 인세는 원고 탈고 후 50만원, 출간이 된 후 50만원 총 100만원의 선인세를 지급하겠다.\n👉 초판은 1,500부에서 2,000부 정도 찍을 예정이며, 인세는 권당 책값의 10%를 지급하겠다.\n\n주변에서 제일 많이 궁금해하는 것이 돈이었다. 🤑 다들 책을 쓰면 돈을 많이 번다고 생각을 하겠지만, 작업 기간 대비 금액은 거의 무보수라고 할 수준이다.  \n\n하지만 책을 쓰는데 가장 큰 의의는 쉽게 경험해보지 못할 짜릿한 경험을 해볼수 있었다라는 점에서 돈보다 더한 가치를 느꼈다. 일단은 수락하고 나서 바로 함께 사이드 프로젝트를 하던 친구에게 공동 집필 제안을 했다. 지금도 아마 이 친구는 모르겠지만, 당시 제안을 하기 전에 의견을 물어보고 나서 제안을 수락하겠다라고 말을 했다. 하지만 사실은 미리 공동 집필에 대해 출판사쪽에 이야기를 한 상태였다. 이 친구 또한 항상 새로운 경험을 추구하며 도전하는 친구이기 때문에 거절할 것이라는 생각은 하지 않았다는 점이 가장 컸다. 😉 \n아마도 공동 집필을 하는 분들이 이 점이 가장 궁금할텐데, 공동 집필을 한다고 인세나 선인세가 2배가 되거나 하지 않는다. 혹시나 공동 집필을 준비하려는 분은 꼭 이점 염두해두길 바란다.  \n집필 계획서 작성하기\n집필을 시작하기 마음 먹고 출판사 쪽에도 뜻을 전달한 후, 바로 집필 계획서를 제출하라는 회신을 받았다. 그리고 나서 바로 집필 계획서를 쓰기 시작하였다. 일단 집필 계획서에는 출판사마다 조금씩 다를 수 있지만 나 같은 경우는 다음과 같은 내용을 기재해서 제출해달라는 요청을 받았다.\n\n👉 도서명(또는 주제)\n👉 작성자와 작성일 \n👉 연락처 및 이메일\n👉 제목(가제)\n👉 책의 핵심 컨셉\n👉 내용 요약 및 특징\n👉 이 책과 관련된 기술의 동향\n👉 타깃 독자층\n👉 경쟁 도서 및 경쟁 도서와의 차별성\n👉 원고의 탈고 날짜(3회에 걸쳐 탈고하며 1/3 분량씩 나눠서 제출하기 때문에 총 3번에 걸쳐 날짜를 기재해야한다.)\n\n집필 계획서를 쓰기도 전에 마음 먹었던 컨셉은 VueJS를 이용하여 실제 애플리케이션을 만들어보기 였다. 나는 기술 서적 컬렉터여서 한달에 평균 10만원 어치의 기술 서적을 사모으는데, 사모은 책들을 보며 느꼈던 점은 많은 책들이 기술에 대한 API 등만 설명할 뿐 실제 응용해서 어떻게 써먹어야되는지에 대해서 다루는지 않는다는 점이 아쉬웠다. \n\n(10만원씩 꾸준히 언젠가는 도서관을 만들 예정이다🤤)\n물론 그렇다고 그 책들을 보며 배울게 없다거나 혹은 그 안에 있는 내용을 모두 이해했다는 것은 아니다. 또한 모든 책들이 API에 대해서만 다루는 것이 아니라 응용하여 어떻게 내가 이 기술을 써먹어야지 좋은지에 대해서 설명해주는 좋은 책들 역시 많다. 하지만 이 책에서는 조금더 실무에 가까운 혹은 응용한다면 실무에서 바로 써먹을 수 있는 그러한 내용을 다루고 싶었다. 그래서 그때부터 열심히 집필 계획서를 작성하기 시작했다.\n\n지금 이 글을 쓰며 오랫만에 집필 계획서를 살펴보았지만, 작명된 파일명을 보며 디자이너들의 고통을 조금이나마 이해할 수 있었다. 지금은 어떤 것이 최종본 집필 계획서인지 나조차도 알수가 없다… 거의 이정도면 디자이너의 흔한 최종 파일과 같은 느낌이랄까..\n\n여튼 여차저차 대략적으로 한달 정도의 집필 계획서 끝에 출판사에서도 승인을 한 이후 본격적인 집필에 앞서 출판사의 담당자와의 미팅을 시작했다. 미팅에서는 주로 집필 계획서에 대한 내용과 계약서에 대한 대략적인 이야기 그리고 출판 계획 등에 대해서 이야기를 하였다. 그리고 몇일 뒤 출판사에서 최종 집필에 대한 승인이 떨어지고나서 계약서를 작성하였다. 10월 2일, 제안을 받고 나서 대략 2달 후 본격적으로 책 집필을 하기 시작하였다. 이때부터가 현기증 나는 책 집필이 시작될 것이라는 사실을 나는 몰랐다. \n\n당시의 일정은 2018년 11월 30일 1차 탈고, 2019년 2월 28일 2차 탈고, 2019년 4월 30일 최종 탈고를 계획으로 시작되었다. 하지만 모두가 알듯 직장인에게 계획이란 큰 의미가 없었다.\n\n(한 취업 포털 회사에서 정말 계획에 대해서 잘 대변해주고 있는 것 같다.)\n본격적인 집필 시작\n본격적으로 집필을 시작하기에 앞서 먼저 시작했던 것은 포멧팅에 대한 규칙과 문체였다. 혼자 집필하면 아무 상관이 없었지만 공통 집필을 하게 되면 가장 큰 문제는 각자 쓰는 문체가 다르다는 것이었다. 그래서 대략적인 문체와 문서에 대한 포멧팅을 정하기 시작했다. 처음에 정한 규칙은 다음과 같았다.  \n\n👉 “기본 형식은 이와 같다”를 기준으로 모든 문체는 평서형을 기준으로 작성한다. \n👉 이미지 내에 화살표나 박스를 추가한 경우 원본 이미지와 함께 “(line)” 폴더명을 함께 기재한다.\n👉 코드나 이미지 다음 문장에는 한 열씩 띄어쓴다.\n👉 제목과 단락에 대한 폰트 사이즈 규칙은 통일한다. \n\n규칙을 정한 이후로는 구글 독스를 이용하며 하나씩 작성하기 시작했으며, 서로 작성한 글을 읽어보며 어색한 문제에 대해서는 구글 독스의 댓글 형태로 남기기로 하였다. 하지만 이러한 방식은 한 챕터 3개 정도 쓰다가 교체되었다. 구글 독스에다가 쓰는 것이 불편하다. README로 작성하게 해달라라는 공동 집필하는 친구의 의견 때문이었다. 찾아본 결과 pdf 혹은 docx 파일로 제너레이팅 해주는 모듈이 있고, 오히려 README 문법으로 글을 쓰면 더 나은 파포먼스를 낼수 있을거라고 판단해 나 역시 동의했다.  책을 집필하며 나중에 작성해야할 내용이나 놓치고 잇던 부분들은 이슈로 남겨두었고, 각자 쓴 글에 대해서는 서로에게 PR을 날려 서로의 글에 대해서 리뷰를 진행하였다. 하지만 이게 나중에는 엄청난 혹으로 돌아왔다. 😅 혹시나 책을 집필하려는 분들 중 README로 작성한 후 모듈을 통해 변환하려고 하신다면 나는 적극적으로 말리고 싶다. 우리가 하고자 하는 포멧을 세부적으로 조절할 수 없을 뿐만 아니라, 코드 역시 포멧팅이 다 깨져 결국에는 수작업으로 옮겨와야만 했다..\n\n책을 진행하며 총 2차례에 걸쳐 일정이 밀렸지만 가장 큰 이유는 포멧팅 문제도 문제였지만 직장은 다니면서 책을 쓴다는 것이 생각보다 쉽지 않았다. 책을 집필하기 이전에 한 회사의 개발자로서 회사 내에서의 역할 역시 무시못할 중요한 일이었다. 책을 집필하는 시기와 이직하는 시기가 겹쳐 이직한 후 새로운 조직에 적응하는 것도 일이었지만, 새로운 조직에서 담당하게 된 프로젝트의 일정 역시 책을 집필하는 시기와 맞물렸다. 회사의 프로젝트에 소홀할 순 없으니 결국에는 책을 집필하는 것에 소홀해질 수 밖에 없었다. 😒\n베타 리딩 스터디 모집\n처음 책을 쓰다보니 우리 책이 실제 대상 독자군들에게 어떻게 다가갈 수 있을지에 대해 명확하게 확신이 들지 않았기 때문이다. 아마 책을 처음 쓰는 분들이 공통적으로 고민하는 부분이 아닐까 싶다. 그래서 실제 우리 책에서 독자를 하는 분들을 모시고 스터디를 진행하기로 했다. 직접 옆에서 집필된 책을 보며 공부하며 어려움을 겪는 부분에 대해서 조언을 드리고, 우리는 그 부분에 대해서 조금더 쉬운 표현으로 바꾸거나 지나친 부분들에 대해서 내용을 추가하기 위해서 였다. \n\n그렇게 2월 VueJS 커뮤니티와 하코사에 글을 올려 스터디원을 모집하기 시작했다. 대략 1주일 정도 모집을 하였지만, 이렇게 많은 분들이 지원해주실지는 상상도 못했다. 이 글을 읽고 계신 분들중 당시 스터디 신청한 분들도 계실지 모르겠지만 130명이 넘는 인원이 지원을 하였다.\n\n 공간상의 이유로 모든 분들을 다 모시지 못하고, 그 중에서도 책에서 독자 타켓팅이 되는 분들에게 연락을 드렸다.(당시 이메일을 기재해주신 분들에게는 한분한분 죄송하다는 메일을 드렸지만, 혹시 그때 연락을 받지 못한 분들에게는 다시 한번 스터디에 관심을 가져주셔서 감사하고, 함께 하지 못해 죄송하다는 말씀을 드리고 싶다.) 그렇게 2달동안 스터디를 진행하였다.  당시 스터디를 진행하면서 책 뒷부분들은 계속해서 집필을 하고 잇었는데 집필 속도를 따라 잡는데는 얼마 걸리지 않아 오히려 책을 씀과 동시에 리뷰를 진행하기도 했다.😅 그렇게 2달동안 수많은 오타와 수정을 반복하여 6월 드디어 최종 탈고를 하였다. 처음 최종 탈고를 하기로 한 4월 30일에 비해 거진 두달이나 밀린 일정이었지만, 담당자분에게 들어보니 이정도면 양호하다는 이야기를 들었다. 실제 집필하시는 분들이 반년 이상 밀리는 경우도 많다고 한다\n끝을 향한 마지막 여정\n책을 다쓰고나서 이제 본격적으로 책 제목에 대해서 고민하게 된다. 책을 쓰기 전에는 책 제목을 확정 짓고 책을 써야하는지 알았지만, 막상 진행해보니 내용을 다쓰고 나서 정한다는 점이 의외였다. 책 제목을 정하면서 정말 재미있는 의견도 많이 나왔지만, 작명 센스가 없는 나로서는 책 제목을 정하는게 고역이 아닐수 없었다. 그래서 주변 지인들의 의견과 무엇보다 출판사 담당자 분의 의견이 책 제목을 짓는데 가장 큰 힘이 되어주었다. 그렇게 책 제목은 커피 한잔 마시며 끝내는 VueJS로 확정 지었다😊\n책을 최종 탈고 및 책 제목도 확정짓게 되면 이제 출판사 쪽에서 책의 컨셉에 맞게 책 내부의 디자인과 책 표지를 디자인 하여 몇가지 시안을 보내준다. 내가 받은 시안은 아래와 같이 총 3가지 였다. 아무래도 책 제목을 반영한 디자인이지 않나 싶다.\n\n결국 이중에 주변 지인분들의 조언을 통해 2번 시안으로 최종 결정하였다.\n이때까지만해도 책을 다썼다라는 실감이 되지는 않는다. 정말 책을 썼구나 라고 실감하던 때는 서문과 작가소개를 쓸때였던 것 같다. 그 동안 책을 사서 보면 서문을 그렇게 주의 깊게 읽지 않앗는데 서문에 책에 대한 저자의 생각이 가장 많이 들어간다라는 점을 서문을 작성하면서 느꼈다. 책을 쓰며 어떠한 생각을 가지고 썼는지, 어떠한 분들에게 어떠한 지식을 전달해드리고 싶었는지에 대해서 정말 많은 고민을 하게 되는 시간이었다. 그리고 무엇보다 서문을 작성하며 느낀 것이지만, 이 책을 쓰기까지 너무 많은 분들의 도움이 있었구나를 느꼈다. \n그렇게 뒷표지 문구, 찾아보기, 저자 소개, 서문 그리고 전체 책에 대한 검토를 마치고나서야 드디어 7월 19일 책 집필에 대해 마무리를 하게 되었다.\n책을 집필을 마치며\n주변에서도 나와 같은 집필에 대한 제안을 받는 분들을 심심치않게 볼수 있다. 하지만 대부분의 분들이 내 지식에 대해 누가 궁금해할까 하는 부담감으로 인해 재안을 거절한다고 한다. 나 역시도 그러한 부담감을 갖지 않은 것은 아니였다. 혹여라도 누군가에게 잘못된 지식을 전달하면 어떻게 하지 혹은 한문장 한문장이 다른 누군가에게 불편하면 어떻게 하지 라는 걱정이 많이 들엇다. 하지만 반대로 그러면서 글을 쓰는 방법과 내 의도를 효과적으로 전달하려면 어떻게 해야하는지에 대해서도 고민을 하게 되엇고, 무엇보다 모르는 지식들을 나역시도 누군가에게 전달하기 위해 공부해나가기 시작했다. 오히려 책상에 앉아 혼자 공부하는 시간보다 책을 쓰며 보낸 시간이 더 많은 성장을 이루지 않았나라는 생각을 한다. 혹여하도 집필을 준비한다던가, 혹은 제안을 받은 분들이 이 글을 읽고나서 집필에 대한 결정을 하는데 도움이 되었으면 한다.","link":"http://blog.martinwork.co.kr/review/2019/07/18/publish-book-for-beginner.html","title":"흔한 주니어 개발자의 VueJS 책 집필기","pubDate":"2019-07-17T15:50:51.000Z"}},{"node":{"contentSnippet":"2019년에 들어 새로운 활동을 시작하였다. 그리고 2019년 상반기에 대한 회고를 작성하는 것이 그 활동이 첫 시작이었다. 때마침 하반기가 시작됨에 따라 올해의 하반기 목표를 되새기고자 회고를 작성하게 되었다.\n2월, 프로젝트의 달\n2월 26일, 새로운 조직에 입사하고 난 후 담당하게 된 프로젝트를 성공적으로 오픈하였다. 이커머스 특성상 회사에서는 물건을 구매하기 전의 프로세스와 물건을 구매하는 프로세스 그리고 물건을 구매하고 난 이후의 프로세스 세 가지 프로세스로 나누어지는 데, 이번에 진행한 프로젝트의 경우 상품 구매 이후의 프로세스에 대해서 이해할 수 있는 프로젝트로서 쉽게 이야기하여 “구매내역”과 “구매한 상품에 대한 일련의 클레임”이 포함돼 있다.\n\n이 프로젝트는 나에게는 감회가 새로운 프로젝트이기도 했다. 새로운 조직에 합류한 후 처음 오픈하는 프로젝트라는 점도 있지만 제일 큰 점은 ReactJS 생태계에 처음 발을 들였다는 점이다. 이전에는 ReactJS를 사용해본 경험이 없어 프로젝트를 시작과 동시에 문법부터 공부하기 시작했다. 회사에서 세미나 발표에서도 이야기 했지만, ReactJS를 이용하여 프로젝트를 진행하며 제일 기억에 남는 것은 온갖 종류의 써드 파티 라이브러리를 접할 수 있다라는 점이다. 이번 프로젝트는 상태 관리 시스템을 Redux를 선택하였는데, Redux를 선택하기 전에도 Redux와 Mobx 사이에서 많은 고민을 하기도 했다. 그런데 막상 Redux를 선택하고 나니 다시 redux-thunk냐 redux-saga냐의 갈림길에 빠지게 되는 한마디로 매 순간이 선택의 기로에 빠지게 만들었다. \n\n개인적으로는 상태 관리나 라우트와 같이 주요한 라이브러리는 정해져있고, 사용할지 말지만 결정하면 되는 VueJS와는 대비되는 점에서 약간의 거부감이 들기도 했던 시점이기도 했다. 물론 회사의 기술 스택 선택은 자유롭기 때문에 처음 프로젝트를 들어가기 전에 꼭 ReactJS가 아닌 VueJS로 개발을 해도 상관은 없었지만, 회사 내에 개발되는 대부분의 프로젝트들이 ReactJS로 개발이 되고 있기 때문에 ReactJS로 개발을 시작하였다. 개인적으로는 프로젝트 안에서의 타입 시스템은 휴먼 에러를 위한 최소한의 방어선이라고 생각하기 때문에 타입스크립트를 도입하여 사용하였다. 조직 안에서 처음 프로덕션 레벨에 타입스크립트를 도입한 이후로 점점더 타입스크립트로 개발되는 프로젝트들이 많아 개인적으로는 또 하나의 보람을 느끼기도 했다.\n3월, 본격적인 TDD 프로그래밍\n3월은 테스트 코드와 친해지기 위해 고군분투하였다. 2월에 사내에서 프로젝트를 오픈하고 나서는 여유가 생겨 기존에 작성돼 코드에 테스트 코드를 붙이기 시작하였으며, 오픈 이후에 들어오는 스펙에 대해서는 TDD 프로그래밍 방식으로 개발을 하기 시작하였다. 테스트 코드를 작성하게 된 계기는 아무래도 리팩토링 시 생길 수 있는 사이드 이펙트에 대한 안정성을 보장받기 위해서였던 것 같다. 아무래도 일정과 익숙하지 않은 환경에서의 개발로 인해 프로젝트를 오픈한 이후에는 개선해야 할 코드들이 많았기 때문에 주기적으로 리팩토링을 하여 배포를 했다. 리팩토링 과정에서 매번 QA의 도움을 기대할 수 없었기 때문에 예측 범위 내에서의 코드 수정을 해야만 했다. 그러한 사이드 이펙트에 대한 예측 가능성을 테스트 코드를 통해 높여나갔다. \n4월과 5월, 현기증 나는 책 집필\n   4월에는 집필하고 있던 VueJS 서적의 대상군을 모집하여, 집필된 책에 대한 코멘트를 받고자 VueJS 스터디를 진행했다. 페이스북의 VueJS Korea와 하코사에 글을 올려 사람들을 모집했는데, 감사하게도 대략 130명이 넘는 많은 분들이 지원을 해주셨다. 죄송하게도 공간상의 이유로 모든 인원과 함께 할 수 없어 집필하는 책의 대상군을 선별하여 함께 스터디를 진행하였다. 대략 3달 정도 진행을 했는데 신중하게 선별해서 그런지 모든 분들의 참여율이 거의 100%에 가까웠으며, 책에 대한 코멘트 역시 적극적으로 해주셨다. \n   그렇게 5월, 함께 스터디를 진행한 분들의 코멘트까지 모두 반영하여 책의 집필이 모두 끝났다. 아니, 끝난지 알았다… \n   \n스터디가 끝난 후 본격적인 기술 감수 및 베타리딩을 위해 베타 리더분들께 책을 발송하기 시작했다. 그리고 2주 만에 100개가 넘는 수많은 수정 사항들을 남겨주셨다.. 이러한 이유에는 베타 리더분들의 기대 이상의 관심을 보여주셨고, 모든 분들이 책을 꼼꼼하게 봐주셨기 때문이 아닐까 싶다. 물론 이러한 이유로 결국 최종 탈고가 한 달 미뤄지는 상황이 생겼지만, 다행히도 출판사의 담당자분이 양해를 해주셨다. 그렇게 5월 내내 책을 탈고하기 위해 고군분투했다.\n   (이 글을 보실지는 모르겠지만, 이 기회를 빌어 스터디 참여에 지원해주신 분, 스터디에 참여해주신 분 그리고 베타리딩 해주신 분들 모두에게 감사하다는 인사드립니다.) \n6월, 상반기의 마지막 달\n   그리고 6월, 최종 수정본을 받아 드디어 책을 출판사에 넘겼다. 2018년 8월부터 시작하여 10개월 동안 준비를 했지만 아쉬움이 많이 남았다. 한 문장 한 문장에 대해 고민을 많이 하기도 했지만 혹시나 “틀린 정보를 전달하면 어떻게 하지?”라는 두려움을 가진 채 글을 쓰다 보니, 생각보다 10개월이라는 시간이 길게 느껴지지 않았다. 물론 최종 탈고를 미룰 수도 있었지만, 기간이 늘어난다고 더 좋은 책을 쓸 수 있다는 확신이 들지 않았다. 그리고 무엇보다, 한평생에 쓸 글을 10개월 사이에 쏟아내듯 작성했기 때문에 책상에 앉아 인내심 있게 글을 쓸 수 있을 것 같지도 않았다. 책에 대한 자세한 이야기는 책이 출판된 이후에 별도의 회고로 작성하지 않을까 싶어 할 말은 많지만 더 이상 말하진 않는다…\n   그리고 VueJS 집필 스터디의 좋은 인연으로 생각지도 못한 좋은 경험을 하게 될 기회가 생겼다. 함께 스터디 하던 분들 중 한분이 “멋쟁이 사자처럼”의 운영진으로, 해커톤을 주최하게 되었는데 그 해커톤에 연사로 참여할 기회가 생겼다. 해커톤의 자세한 참여 후기는 여기에서 별도로 다루진 않지만 혹시나 궁금하신 분들을 위해 링크는 남겨둔다. 내 스스로 아직 이뤄놓은 것도 없는데, 누군가 앞에서 나의 경험을 이야기한다는 것은 하나의 부담이기도 했지만, 보잘것없는 나의 이야기로 인해 다른 누군가에게 긍정적인 영향을 미칠 수도 있다는 점이 또 하나의 매력이기도 했다. 앞으로 이런 기회가 또 올지는 모르겠지만, 조그마한 나의 경험이 다른 누군가에게 도움 줄 수 있는 기회가 생긴다면 또 하지 않을까는 생각을 해보기도 했다.\n상반기를 돌아보며, 하반기는…\n돌이켜보면 생각보다 상반기에 적은 것 같으면서도 많은 일을 한 것 같다. 올해 초, 상반기와 하반기로 나누어 두루뭉실하게나마 올해의 목표를 세운 적이 있다. 상반기에는 책 집필 완료였고, 하반기에는 본격적으로 기술 부채를 해결하는 것과 영어 공부였다. 7월의 시작과 동시에 영어 공부를 시작하였고, 기술 부채를 기록하기 위한 새로운 커뮤니티를 시작하였다. \n새로 시작한 모임은 글또 라는 변성윤님이 주최하는 커뮤니티로서 여러 분야의 개발자들이 모여 주기적으로 블로깅을 하는 모임이다.\n\n사실 어색한 분위기를 못 견뎌하기 때문에 새로운 사람들과의 만남에 있어 엄청난 부담감을 가지고 있지만 다행히도 모임은 처음과 마지막만 오프라인으로 진행하고, 나머지는 온라인으로 진행하기 때문에 이러한 부담감을 덜 할것 같아 참여를 하게 되었다. 앞으로 구체적으로 어떠한 글을 쓸지는 미지수이지만, 대략적인 방향은 회사에서의 트러블 슈팅 혹은 내가 배워나가는 개발 지식들에 대해서 글을 쓰지 않을까 싶다 :)\n이제 하반기가 시작되었다. 과거 2018년의 회고와 같이 아마도 별일이 없다면 2020년이 되기 전 2019년의 회고를 작성하지 않을까 싶다. 그리고 그 글을 작성할 때는 “상반기보다 조금 더 보람찬 하반기를 보낸 한 해였다”라는 글을 쓰고 싶다.","link":"http://blog.martinwork.co.kr/review/2019/07/04/the-first-half-year-remembrance.html","title":"2019년 상반기의 회고","pubDate":"2019-07-03T15:50:51.000Z"}},{"node":{"contentSnippet":"2019년 6월 1일, 경희대학교에서 멋쟁이 사자처럼의 일부 운영진들이 진행하는 대학생 해커톤이 열렸다. 해커톤에는 대략적으로 50여명 정도의 대학생이 참여를 했으며, 나는 그 해커톤의 연사로 참여를 하게 되었다. 사실 연사라고 표현하기에는 너무 거창하고 스스로가 부족하다고 생각하여 그냥 해커톤에서 대학생들의 시간을 뺏는 사람 정도였다라고 생각한다 😅\n일단 글을 시작하기 전에 이러한 좋은 기회를 준 멋쟁이 사자처럼의 운영진 분들과 함께 연사로 참여해준 숨고의 동욱님, 그리고 티몬에서 함께 일을 하는 철현님, 허승님께 감사하다는 말을 드리고 싶다. 🙏 (물론 그 분들이 이 글을 볼지는 미지수이지만.. 🙄) \n어떠한 질문을 받았니?\n연사분들과 함께 해커톤을 진행 중인 장소 안으로 들어가기 전까지만 해도 다들 걱정이 많았다. 무엇보다 우리가 어떠한 질문을 받을지, 어떠한 주제로 라이트닝 토크가 진행되는지, 우리에게 무엇을 기대하는지 어떠한 가이드도 받지 못했기 때문이다. 하지만 막상 들어가서 우리의 걱정이 기우였다는 것을 깨닫기까지는 얼마나 걸리지 않았다. \n소개가 끝나고 나서 바로 첫 질문을 받기 시작했을 때는 2분 정도 밖에 손을 들지 않아 속으로 “생각보다 우리에게 질문이 별로 없구나. 괜히 시간을 뺏는게 아닐까?” 라는 걱정을 했다. 하지만 첫번째 질문에 대한 답이 끝나고 나서 두번째 질문을 받는다고 했을 때, 7분이 넘는 분들이 질문을 위해 적극적으로 손을 들었다. \n\n그 모습을 보고 한편으로 “과연 나는 사람들이 많이 모인 공적인 자리에서 남들 앞에서 용기내어 누군가에게 질문을 한적이 있었던가?” 라는 생각이 들 정도로 깜짝 놀랐다. 대학생분들이 우리에게 궁금했던 것은 다음과 같았다. \n\n👉 개발자라는 직업을 선택할 때, 비전공자로서의 두려움은 없었나요?\n👉 개발자/기획자가 되기 위해 어떻게 공부 혹은 노력을 했나요?\n👉 언제 “개발자/기획자로서의 길이 내 길이다” 라는 확신이 들었나요? \n👉 이후 취업을 하기 위해 어떤 언어/프레임워크를 공부하면 될까요?\n👉 성장을 할 때 가장 중요한 것은 무엇이라고 생각하나요? \n👉 해외 취업을 하기 위해서는 어떠한 준비를 해야할까요?\n👉 중소기업에 들어가면 대기업으로 이직하기 힘들다고 하는데 그럼 첫회사는 대기업으로 가야할까요?\n👉 지금 회사에서 나를 뽑아준 이유는 뭐라고 생각하시나요?\n\n개발자로서의 내 모습은…\n사실 돌이켜보면 누구나 한번쯤은 고민했을 법한 주제이다. 하지만 나는 부끄럽게도 적어도 대학생 때는 이러한 고민과 불안감을 가지지 않았다. 어떻게 보면 커리어에 대해서 그들만틈 진지하게 생각해보지 않았을 수도 있다. 내가 개발자로서의 길을 선택한 것은 나 스스로 보기에도 개발을 할 때의 내 모습이 다른 일을 할 때보다 즐거워보인다라는 단순한 이유였다. \n물론 이렇게 이야기하면 일부 사람들은 나에게 “그건 괜찮은 환경에 있어서 즐겁다고 느낄 수 있었던 거 아니에요?”라는 역질문을 던질 때가 있다. 하지만 처음 개발 공부를 흥미를 갖고 시작했던 회사는 10명도 채 안되는 조그마한 에이전시였다. 다른 에이전시에 비해 특출날 것 없이 고객사에 뜻대로 작업을 해야하며, 고객사가 원하면 의미없는 야근도 감수해야 했으며, 깊은 지식을 요하는 작업을 하는 것도 아니였다. 그럼에도 불구하고 새로운 것을 공부하는 것이 즐거워 고시 공부하듯 남들보다 빠르게 출근해서 공부를 하고, 퇴근하고 나서도 미친듯이 공부를 했다. (이때가 나의 멘토를 만나게 된 시점과도 비슷한 시점이다.) 아마 이때가 엘런 머스크의 80시간의 법칙을 지켰던 유일한 기간이 아닐까 싶다. (요즘에는 조금 나태해져 80시간까지 채우기는 쉽지 않다.😰)\n\n물론 세상을 바꾸기 위해 공부를 했던 것은 아니다. 단지 내가 좋아하는 분야에 대해 무지한 내 스스로에게 실망하기 싫었기 때문이다. 그렇기 때문에 나에게 있어 비전공자로서의 두려움 혹은 선택에 대한 확신을 생각했던 적도 없었던 것 같다. 말 그대로 좋아서 했던 하나의 취미가 곧 내 일이되었기 때문이다.\n\n무엇보다 앞서의 질문과 같이 성장이 환경에서 나온다고 생각하진 않는다. 물론 이에 대해서는 사람마다 생각하는게 다를 수 있지만, 개인적으로는 스스로 느끼기에 환경이 불만족스럽다라고 하면 환경을 개선하기 위해 최소한의 노력은 해봐야 한다고 생각한다. 물론 개인의 힘으로 환경을 바꾸기란 쉽지 않다. 만약 동료들이 역시 환경을 개선해야한다는 필요성을 느끼면서도 함께 행동하지 않거나 혹은 최악의 경우 동료가 비관주의자라면 그땐 더 좋은 환경을 위해 떠나는게 맞다. 하지만 노력에 따라 개선의 여지가 있다면 환경을 탓하기보단 한번쯤은 나의 주변 환경을 바꾸기 위한 노력을 해봐도 되지 않을까.\n무엇을 공부해야 할까?\n주기적으로 한 커뮤니티에 제일 많이 올라오는 글 중 하나는 “무엇을 공부해야하나요?” 라는 질문이다. 주기적으로 이런 글이 올라온다는 것이 이제는 놀랍지도 않다. 과거의 회사에서는 VueJS와 타입스크립트를, 현재의 회사에서는 ReactJS와 타입스크립트 조합을 이용하여 개발을 하고 있지만, 단 한번도 프레임워크에 대해서 공부한 적이 없다. 아니, 엄연히 말하면 프레임워크를 학습하고 난후 회사를 입사하거나 혹은 프로젝트를 진행한 적이 없다.  \n\n지금 돌이켜보면 놀라운 사실은 전회사에서 VueJS를 이용한 프로젝트를 진행할 때는 VueJS라는 것을 처음 들어봤으며, 지금 회사의 대부분의 팀이 ReactJS를 이용하여 개발을 하지만 회사를 입사할 때는 ReactJS의 존재유무만 알았을 뿐 문법조차 몰랐다. 회사에서 쓰기 전까지는 사실 관심도 가지지 않았으며 공부를 해야할 필요성도 느끼지 못했다. 평소에 내가 공부하는 것들을 대체적으로 회사 서비스에 사용하는 기술이거나 현재 진행 중인 프로젝트에 도입을 해야하는 것 혹은 진행했던 프로젝트를 개선하기 위한 기술들이다. \n언젠가 지인과 이야기 하다 2가지 유형의 개발자라는 주제를 가지고 이야기한적이 있다. 첫번째 유형은 플랫폼 성향이 강한 개발자이고, 두번째 유형은 비즈니스 성향이 강한 개발자였다. 첫번째 유형의 개발자는 새로운 기술에 대해 관심이 많으며, 끊임없이 신기술들에 대해 학습을 한다. 반대로 두번째 유형의 개발자는 새로운 기술보다는 비즈니스에 관심이 많다. 이 중 개인적으로 나는 두번째 유형에 가까운 개발자인 것 같다. 그렇기 때문에 개인적으로 학습하는 모든 기술은 회사에서 진행하는 혹은 사이드 프로젝트를 위한 기술이다. \n이러한 예 중 기억나는 것은 바로 타입스크립트 도입이었다. 자바스크립트는 동적 언어기 때문에 특성상 자료형이 실행시 결정된다. 그렇기 때문에 자바스크립트를 이용하여 개발할 때 제일 많이 겪는 에러 중 하나는 타입 에러이다. 이러한 에러로 인해 기능이 제대로 작동하지 않을 때도 있으며, 이러한 에러를 찾기 위해서는 실제 실행시켜봐야 알 수 있다보니 에러를 찾고 수정하는 데 까지 걸리는 시간이 길 수 밖에 없다. 이러한 단점은 개발 생산성을 떨어트릴 뿐만 아니라 리팩토링할 때에도 흔히 이야기하는 몸 사리는 코딩을 하게 만들었다. 이러한 문제의 돌파구로 찾은 것이 타입스크립트였다. 타입스크립트는 개발자로 하여금 컴파일 단계에서 타이핑 에러를 감지했으며, 인터페이스로 하여금 전보다 더 예측 가능한 애플리케이션을 만들 수 있도록 했다.\n물론 프레임워크나 라이브러리를 깊이 알고 잘 쓴다고 한다면 더할 나위 없이 좋다. 하지만 근본적으로는 대부분의 기술은 어떠한 문제를 해결하기 위해 나온다. 마치 자바스크립트의 동적 타입에 불편함을 느끼고 정적 타입 시스템을 도입하기 위해 나온 타입스크립트라던지, 혹은 브라우저의 DOM을 효율적으로 업데이트 하기 위한 Virtual DOM 개념, 클라이언트의 싱태를 효율적으로 관리하기 위한 flux 패턴과 거기서 파생된 Vuex, Redux 등과 같다. 그렇기 때문에 막연하게 불나방처럼 기술에 달려드는 것보단 기술의 본질에 대해서 공부를 하고, 필요한 시점이 되었을 때 적재적소에 사용하는 것이 어떨까라는 생각을 한다. \n멘토 혹은 함께하는 동료 만들기\n나는 개인적으로 학원에 대해 마냥 긍정적인 입장은 아니다. 일단은 학원은 상업 기관임에도 마치 교육기관처럼 탈바꿈하려고 하는 것도 그렇지만, 제일 큰 이유는 “X주 완성하기”와 같은 허위 혹은 과대 광고 때문이다. (물론 모든 학원이 그렇다는 것은 아니다.) 하지만 학원에는 분명히 순기능이 존재한다. 개인적으로 학원의 가장 큰 순기능 중 하나는 함께 공부하는 사람들과의 네트워킹으로 인한 동기부여라고 생각한다.  \n\n혼자 공부를 하면 내가 옳은 방향으로 가고 있는가에 대한 방향성, 성장 속도에 대한 지표 혹은 혼자 얻을 수 있는 정보에 대한 한계에 부딪히기 쉽다. 하지만 긍적적인 네트워킹을 통한 동료들은 러닝 메이트와 같이 함께 성장하며 함께 나아갈 수 있으며, 나의 성장 속도, 방향에 대한 측정 가능한 지표가 되어줄 수 있다. 또는 내가 지치거나 번아웃이 왔을 때 나를 이끌어 주고나 붙잡아줄 수 있으며, 반대로 내가 동료에게 힘이 되어 줄수도 있다. \n물론 제일 좋은 것은 멘토의 유무이지만, 멘토를 찾기도 쉽지 않다. 설령 찾았다고 해도 개인적인 시간까지 할애해가며 도와주는 멘토를 찾기가 더더욱 쉽지 않다. (글을 써놓고 보니 2년째 나를 지켜봐주시는 멘토분께 새삼 감사함을 느낀다😝) 만약 좋은 멘토를 구하기 어렵다면 컨퍼런스나 커뮤니티를 이용하여 동료를 구해보는 것도 좋을 것 같다.\n나에게 있어 오늘의 경험은\n무엇보다 해커톤에서 대학생 분들에게 받은 질문을 듣고 느꼈던 그들의 고민의 키워드는 성장과 미래에 대한 불안감이였다. 그에 대한 답변을 하면서도 나 자신에게도 되묻고 싶었다. 불과 3년 전만 해도 나도 그들과 비슷한 고민을 했었던 것 같다. 아니, 오히려 오늘 만난 대학생분들보다 더 진지하게 고민해보지 못했던 것 같다. \n내 이력서에 에이전시인 회사는 딱 1군데이지만 비공식적으로 내 이력서에 적기엔 내 스스로를 너무 초라하게 만드는 에이전시가 더 있었다. 그 중 한군데서는 그래도 에이전시 내에서는 저력이 있었던 작지 않았던 에이전시 였으며, 다음 회사는 임금 체불이라는 나에겐 또 다른 시련을 안겨준 회사도 있었다. 이러한 회사를 다 합쳐서 채 3개월이 되지도 않은 시점에 나는 2번째 회사를 나오며 “스스로를 보며 실패했다”라는 생각을 많이 했다. 그 때까지만 해도 내가 누군가의 앞에서 나의 경험에 대해서 혹은 성장에 대해서 이야기할 기회가 올거라고 생각하지 못했다. 물론 이번에 우연치 않은 기회로 해커톤의 연사로 참여했지만, 앞에서 말했듯 스스로가 연사를 참여할 만큼 대단하다고 생각하진 않는다. 하지만 오늘의 경험으로 인해 나는 과거의 열심히 살지 못했던 내 스스로를 돌아보게 되었으며 오히려 해커톤에 참여함으로써 앞으로도 계속 노력해나가야 하는 부족한 개발자라는 것을 깨닫는 계기가 되었다. \n아쉬웠던 점\n해커톤에 참여하기 하루 전, “과연 나는 어떠한 질문을 받을까? 그리고 그 질문을 받고 나면 나는 어떻게 대답해줄까”라는 생각을 하다 잠이 들었다. 물론 다행이도 어느 정도 예상했던 예상 범위 안에서 질문을 받았다 하지만 어떠한 주제로 라이트닝 토크가 진행되는지 컨셉이라도 알았더라면 막상 이러한 질문을 받았을 때, 조금더 도움되는 말을 해줄텐데 라는 생각에 많은 아쉬움이 남았다. 물론 이러한 아쉬움 내 스스로에 대한 아쉬움일 수도 있다. 한때는 나역시도 커리어에 대한 비슷한 고민과 걱정을 했었고, 만약 그때 고민과 걱정에 대해 조금더 깊게 생각해보았더라면 아마 오늘의 답변이 다른 이들에게 조금더 도움이 되지 않았을까란 생각을 해봤다.","link":"http://blog.martinwork.co.kr/review/2019/06/01/likelion-hackerton.html","title":"멋사 해커톤 참여기","pubDate":"2019-05-31T15:50:51.000Z"}},{"node":{"contentSnippet":"타입스크립트를 이용해서 개발을 한지 어느덧 1년이 조금 넘은 것 같다. 현재의 직장으로 옮기기 전에는 VueJS와 타입스크립트 조합으로 사용을 하였고, 지금은 ReactJS와 타입스크립틀 조합으로 사용하고 있다. \n\n타입스크립트로 프로젝틀를 진행을 하며 처음에는 인터페이스와 모듈에 대한 인풋/아웃풋의 타입에만 신경을 쓰고, 비효율적인 인터페이스 선언에 대한 고려는 크게 하지 않았다. 사실은 알면서도 모른 척 했을 수도 있다. 하지만  비지니스가 점점 더 복잡해짐에 따라 인터페이스를 효율적으로 관리해야하는 니즈가 생겼다. 간단한 프로젝트에서는 모델에 대한 인터페이스 선언을 해도 그 수가 많지 않았지만, 지금은 선언된 모듈 인터페이스만 해도 그 수가 기하급수적으로 많아지기 시작했다. 그러던 중 타입스크립트 내에서 제공해주는 유틸 타입을 보게 되었고, 추가적인 라이브러리 내에서 지원하는 유틸 타입을 보게 되었다. 물론 라이브러리에서 지원하는 모든 유틸 타입이 다 필요한 것은 아니기에, 살펴보고 필요하다라고 판단되는 것은 일부 정의해서 쓰기로 했다. 그 중 유용하다고 생각하는 유틸 타입이나 타입스크립트에서 제공해주는 유틸 타입에 대해서 살펴보았다.\n타입스크립트에서 기본적으로 제공해주는 유틸 타입\nPartial 유틸 타입\nPartial 타입은 제네릭 타입 T에 대해서 모든 프로퍼티들을 Optional하게 변경한다. 정의된 Patial 타입을 살펴보면 아래와 같이 선언되어져 있다.\n\n\n1\n2\n3\n\ntype Partial<T> = {    \n[P in keyof T]?: T[P];\n};\n\n\nPatial 타입은 제네릭 타입의 T 타입의 Public Property 들에 대하여 기존의 타입은 유지하되, 각각의 Property 들을 Optional 타입으로 변경해준다. \nkeyof T 와 T[K]\nIndexed type query인 keyof T는 제네릭 타입 T의 Public Property 들에 대한 Union 타입이다. 우리에게 익숙한 TODO에 대한 인터페이스를 통해 살펴보면 다음과 같다.\n\n\n1\n2\n3\n4\n5\n6\n7\n\ninterface Todo {\nid: string;\ntext: string;\nisDone: boolean;\n}\n\ntype Key = keyof Todo; // \"id\" | \"text\" | \"isDone\"\n\n\n이러한 keyof 는 문자열(String) 타입의 서브 타입이다. 이러한 keyof 키워드와 함께 자주 쓰이는 연산자가 있다. 위에서 Partial 타입에서 살펴봤던 T[K]이다. Indexed access operator 인 T[K] 연산자 덕분에 Partial 타입이 기존의 타입을 유지한 상태에서 각각의 Property들을 Optional 한 타입으로 변경해줄 수 있었다. Indexed type query와 Index access operator를 응용한다면 아래와 같이 type-safe 한 값들을 가져올 수 있다.\n\n\n1\n2\n3\n\nfunction getValue<T, K extends keyof T>(todo: T, key: K): T[K] {\n  return todo[key];\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\ninterface Todo {\nid: string;\ntext: string;\nisDone: boolean;\n}\n\ntype OptionalTodo1 = Partial<Todo>;\n\n// OptionalTodo1 와 OptionalTodo2 는 동일한 타입이다.\ninterface OptionalTodo2 {\nid?: string;\ntext?: string;\nisDone?: boolean;\n}\n\n\n이러한 Partial 타입을 이용하여 인터페이스 안에 혼재 되어 있는 타입들에 대하여 Required 타입과 Optional 타입을 분리할 수 있다는 장점이 있다. 사용자 정보에 대한 모델 타입이 있다고 가정해보자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\ninterface UserInformation {\n  id: string;\n  uid: string;\n  name: string;\n  age?: number;\n  profile?: string;\n  phone?: string;\n}\n\n\n물론 이러한 형태로 작성해도 문제는 없을 것이다. 하지만 상황에 따라 Required 타입와 Optional 타입을 분리해야할 상황이 있다. 예를 들어 별도의 API 거쳐 Service Layer 를 통해 Optional한 값에 대하여 기본값을 추가해주는 일종의 Generate 함수의 Parameter 등이 있을 것이다. (이에 대한 자세한 설명은 비즈니스 코드를 작성하는 개인의 취향 혹은 팀 내의 컨벤션이므로 깊이있게 설명하진 않는다.) 이러한 경우 아래와 필요성에 따라 아래와 같이 분리할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\ntype UserInformation = RequiredUserInformation & Partial<OptionalUserInformation>;\n\ninterface RequiredUserInformation {\n  id: string;\n  uid: string;\n  name: string;\n}\n\ninterface OptionalUserInformation {\n  age: number;\n  profile: string;\n  phone: string;\n}\n\n\nRequired 타입\nRequired 타입은 앞서 살핀 Partial 유틸 타입과는 반대의 개념이다. Partial 유틸 타입은 모든 프로퍼티를 Optional로 만들어줬다면 Required 타입은 제네릭 타입 T의 모든 프로퍼티에 대해 Required 속성으로 만들어준다.\n\n\n1\n2\n3\n\ntype Required<T> = {\n    [P in keyof T]-?: T[P];\n};\n\n\nPatial 타입과 동일하게 기존의 값은 유지된 상태에서 Requied 타입으로 변경된다는 것을 꼭 인지하도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\ninterface OptionalTodo {\n    id: string;\n    text?: string;\n    isDone?: boolean;\n}\n\n// 타입 Todo1과 타입 Todo2 는 동일한 타입이다.\ntype Todo1 = Required<OptionalTodo>;\n\ninterface Todo2 {\n    id: string;\n    text: string;\n    isDone: boolean;\n}\n\n\nReadonly\nReadonly 타입을 이용하면 주어진 제네릭 타입 T의 모든 프로퍼티가 readonly 속성을 갖도록 변경한다.\n\n\n1\n2\n3\n\ntype Readonly<T> = {\n    readonly [P in keyof T]: T[P];\n};\n\n\nreadonly를 이용하는 대표적인 경우가 React 컴포넌트의 Props와 State 값이 이에 해당한다. 우리는 모두 알듯, React 내에서의 모든 Props 와 State 는 Immutable이다. 그렇기 때문에 직접적으로 변경해서는 안된다. 이러한 상황에서 Readonly 타입을 이용할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nclass Component<P, S> {\n    // 중간 생략\n  constructor(props: Readonly<P>);\n  setState<K extends keyof S>(\n      state: ((prevState: Readonly<S>, props: Readonly<P>) => (Pick<S, K> | S | null)) | (Pick<S, K> | S | null),\n      callback?: () => void\n  ): void;\n  state: Readonly<S>;\n}\n\n\n혹은 immutable-js과 같은 데이터의 불변성을 보장해주는 라이브러리나 혹은 Javascript의 freeze 함수에 사용할 때 유용하게 사용할 수 있다. 예를 들어 Object를 얕은 동결을 지원하는 freeze 함수를 사용하는 경우 모든 함수의 경우 readonly 속성을 가져야 함으로 이럴 때 유용하게 사용할 수 있다.\n\n\n1\n2\n3\n\nfunction freeze<T>(obj: T): Readonly<T> {\n  // do something\n}\n\n\nPick<T, K>\nPick 타입은 주어진 첫번째 제네릭 타입 T 내에서 Union 타입 K에 대한 프로퍼티에 대한 타입들을 뽑아낸다.\n\n\n1\n2\n3\n\ntype Pick<T, K extends keyof T> = {\n    [P in K]: T[P];\n};\n\n\n익숙한 TODO를 통해 Pick  타입을 사용하는 예제를 살펴보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\ninterface Todo {\n  id: string;\n  title: string;\n  isDone: boolean;\n};\n\n// TodoWithIdAndTitle 타입과 PickedTodo 타입은 동일한 타입이다.\ntype TodoWithIdAndTitle = Pick<Todo, 'id' | 'title'>;\n\ninterface PickedTodo {\n  id: string;\n  title: string;\n}\n\n\n이러한 Pick 타입을 이용한다면 필요한 타입만 추출하여 원하는 새로운 타입을 만들 수 있다.\nRecord\nRecord 타입은 총 두개의 제네릭 타입을 받을 수 있다. 첫번째 제네릭 타입 K은 프로퍼티 타입으로, 두번째 제네릭 타입 T은 값의 타입으로 사용된다.\n\n\n1\n2\n3\n\ntype Record<K extends keyof any, T> = {\n    [P in K]: T;\n};\n\n\nRecord 타입의 구조체를 살펴보면 일반적으로 사용하는 Object와 닮은 꼴을 하고 있다는 것을 알 수 있다. 아래와 같이 사용할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\ntype IFooBar = {\n  foo: string;\n  bar: string;\n};\n\ntype IHelloWorld = 'hello' | 'world';\n\nconst x: Record<IHelloWorld, IFooBar> = {\n  hello: {\n    foo: 'foo',\n    bar: 'bar'\n  },\n  world: {\n    foo: 'foo',\n    bar: 'bar'\n  }\n}\n\n\nExclude\nExclude 타입은 2개의 제네릭 타입을 받을 수 있으며, 조건부 타입(Conditional type)을 이용하여 타입을 정의 한다. \n\n\n1\n\ntype Exclude<T, U> = T extends U ? never : T;\n\n\n두번째 제네릭 타입에 대하여 첫번째 제네릭 타입이 할당 가능한 타입(Assignable)인지를 여부를 판단하여 할당 가능한 타입을 제외한 나머지 타입들을 이용하여 타입을 정의한다. 이해하기 쉽도록 아래의 예제를 살펴보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\ninterface Todo {\n  id: string;\n  title: string;\n  isDone: boolean;\n}\n\ninterface Memo {\n  id: string;\n  title: string;\n  content: string;\n}\n\ntype Contents = Exclude<Todo | Memo, Memo>;\n\n\n위에서 살펴본 할당 가능한(Assignable) 타입을 제외한다면 Cotents 타입은 어떠한 타입을 가지게 될까 ? 두번째 제네릭 타입 U에 해당하는 Memo에 대하여 첫번째 제네릭 타입에 해당하는 Todo와 Memo 중 할당 불가능한 타입은 Todo 타입이 될 것이다.\nExtract\nExtract 유틸 타입은 단어 그대로 추출의 의미를 가지며, Exclude 타입과 반대의 타입이다.  첫번째 제네릭 타입 U에 대하여 제네릭 타입 T 중 할당 가능한 타입을 할당한다.\n\n\n1\n\ntype Extract<T, U> = T extends U ? T : never;\n\n\nExclude 타입에서 살펴보았던 예제를 다시 한번 살펴보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\ntype Todo = {\n  id: string;\n  title: string;\n  isDone: boolean;\n};\n\ntype Memo = {\n  id: string;\n  title: string;\n  content: string;\n};\n\ntype Contents = Extract<Todo | Memo, Memo>;\n\n\nExclude 타입과는 다르게 유니온 타입 Todo과 Memo에 대하여 Memo에 할당 타입을 반환하기 때문에 Contents 타입은 Memo 타입이 되는 것을 알 수 있다.\nOmit\nOmit 타입은 두개의 제네릭 타입을 받으며 첫번째 제네릭 타입 T에 대하여 두번째 제네릭 타입 K의 값을 제외한 나머지 값을 반환합니다. \n\n\n1\n\ntype Omit<T, K extends keyof any> = Pick<T, Exclude<keyof T, K>>;\n\n\n정의된 Omit의 타입을 살펴보면 앞서 우리가 익혔던 타입들을 이용하여 정의하고 있다. 예제를 통해 정의된 타입들을 Todo 인터페이스의 예제를 가지고 이해해보도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n\ninterface Todo {\n  id: string;\n  title: string;\n  isDone: boolean;\n}\ntype TodoWithOutId = Omit<Todo, \"id\">;\n\n\n일단 Exclude 타입을 통해 살펴보면 keyof T 에 해당하는 Todo 타입의 프로퍼티 중 두번째 제네릭 타입 K에 해당하는 id를 제외한 나머지 값들을 뽑아낸다.\n\n\n1\n\nPick<Todo, \"title\" | \"isDone\">;\n\n\n그렇게 Exclude를 통해 뽑아낸 유니온 타입들에 대하여 Pick 타입을 이용하여 Todo 타입 중 그에 해당하는 프로퍼티의 타입을 뽑아낸다. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\ninterface Todo {\n  id: string;\n  title: string;\n  isDone: boolean;\n}\ntype TodoWithOutId = Omit<Todo, \"id\">;\n// TodoWithOutId의 타입은 아래의 인터페이스와 일치한다.\ninterface TodoWithOutId {\n    title: string;\n  isDone: boolean;\n}\n\n\nNonNullable\nNonNullable 유틸타입은 주어진 제네릭 타입 안에서 null이나 undefined을 제거한다.\n\n\n1\n\ntype NonNullable<T> = T extends null | undefined ? never : T;\n\n\n이미 앞에서 살펴봤던 타입들을 모두 이해했다면 이와 같은 Conditional type(조건부 타입)은 이해하기 어렵지 않을 것이다. 간단한 예제를 통해 살펴보면 후, 넘어가도록 하자.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\ntype Todo = {\n  id: string;\n  title: string;\n  isDone: boolean;\n}\n\ntype NullableTodos = null | undefined | Todo[];\n\ntype Todos = NonNullable<NullableTodos>;\n/**\n * type Todos = Todo[];\n */\n\n\nReturnType\nReturnType 타입은 주어진 제네릭 타입 T의 return type을 할당한다.\n\n\n1\n\ntype ReturnType<T extends (...args: any) => any> = T extends (...args: any) => infer R ? R : any;\n\n\n여기에서 익숙하지 않은 infer 키워드가 등장하는데, 간단히 이야기하자면 해당 타입을 추론 하고자 할 때 사용하는 키워드라고 생각하면 된다. 관련해서 조금더 자세히 알아보고자 한다면 https://dev.to/aexol/typescript-tutorial-infer-keyword-2cn 를 참고하도록 하자. 그래서 위에서의 반환 타입을 분석해보자면, T extends (...args: any) => infer R ? R : any 는 R 타입에 대해서 타입 추론이 가능하다면 R 타입을 그렇지 않다면 any 타입을 반환한다. ReturnType 의 경우 해당 함수에 대한 반환 타입을 타이핑할 때 사용할 수 있다. 아마 Redux 를 사용해본 사람이라면 이러한 ReturnType을 ActionCreator 함수에서 활용할 수 있음을 깨달을 수 있을 것이다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\ninterface IPayload {\n  foo: string;\n  bar: string;\n}\n\nconst fooBarCreator = () => ({\n    foo: \"foo\", bar: \"bar\"\n});\n\ntype IFooBarCreator = ReturnType<typeof fooBarCreator>;\n\n\n기타 유용한 유틸 타입\n타입스크립트에서 제공해주는 유틸 타입을 제외하더라도 개인적으로 커스텀해서 사용하기 좋아하는 타입들 역시 있다. 이러한 유용한 유틸 타입 중 일부에 대해서는 아래의 링크에 conditional-type-checks를 참고하였으며, 기타는 회사에서 업무를 진행하며 필요하다는 생각이 들어 추가한 것들도 있다.\nNullable\nJavascript로 개발을 할때는 Object를 초기화할때 null을 이용하여 초기화 시켜줬다. 이러한 객체는 선언시 null 타입을 가질 수도 혹은 이후 할당된 객체의 값을 가질 수 있어야 한다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\ntype INullable<T> = T | null;\ninterface ITodos {\n  id: string;\n  text: string;\n  isDone: boolean;\n}\n\nfunction fetchTodo (): Promise<INullable<ITodos>> {\n  // request api\n} \n\nfetchTodo().then(todo => {\n  // 서버의 응답값이 null일 수 있기 때문에 방어코드를 추가해준다.\n  if(!todo) {\n    // do something\n  }\n});\n\n\n개인적으로는 보통 서버와의 통신을 제어하는 래퍼 함수(Wrapper function)의 리턴 타입의 인터페이스에 사용한다. 이렇게 작성된 인터페이스로 인해 일차적으로는 서버의 응답값과의 정합률이 높아질 뿐만 아니라 자칫 놓치고 넘어갈 수 있는 방어 코드에 대해서도 컴파일 단계에서 미리 알아차릴 수 있다는 장점이 있다.\nMaybe\n프론트 개발을 하다보면 아무리 정해져 있는 API 규약에 맞춰 백엔드 서버와 커뮤니케이션 한다고 하더라도 실제 정해져있는 규약으로 넘어오지 않는 경우가 많다. 그러한 경우 값이 없을 때의 타입이 null인지, undefined인지 알 수가 없다. 물론 아예 해당하는 키의 데이터가 안내려올때도 있지만 그러한 경우는 optional 옵션을 이용하여 인터페이스를 선언하면 되지만, 타입이 명확하지 않은 경우는 런타임 환경에서 타입 에러에 직면하는 경우가 많다. 그런 경우를 대비하며 Nullable 타입과 분리하여 Maybe 유틸 타입을 선언하여 사용한다. \n\n\n1\n\ntype IMaybe<T> = T | undefined | null;\n\n\n이 외에도 AJAX의 응답값에 대한 래퍼 함수의 리턴 타입 등을 커스텀해서 사용하고 있지만, 이외에는 생각나는 유틸 타입은 없는 것 같다. 이 외에도 이러한 유용한 유틸 타입을 제공해주는 라이브러리가 있다. 혹시나 또다른 유틸 타입이 필요하다면 아래의 참고 링크를 참고하는 것도 좋을 것 같다. \n혹시나 이 글을 보시고, 커스텀으로 사용하고 있는 또다른 좋은 유틸 타입이 있다면 여러분의 타입도 한번 공유 부탁드립니다.😄\n\n참고\n\nTypescript handbook\nconditional-type-checks\nutility-types","link":"http://blog.martinwork.co.kr/typescript/2019/05/28/typescript-util-types.html","title":"Typescript의 기본 유틸 타입","pubDate":"2019-05-27T15:50:51.000Z"}},{"node":{"contentSnippet":"들어가기전\n프론트엔드 개발자가 아니더라도 웹 개발에 몸을 담고 있다면 한번쯤은 SPA 애플리케이션에 대해서 한번쯤 들어봤을 것이다. 요즘은 일부 회사들의 공고에도 보란듯이 SPA 애플리케이션 경험자라는 키워드가 나와있을 정도로 프론트엔드 개발자에게는 알아야하는 기본적인 소양이 되었다. 이참에 대체 SPA 애플리케이션이 무엇인지, 왜 SPA 애플리케이션이 나오게 되었는지 하나씩 알아볼 것이다. 그 전에 먼저 우리가 살펴보고자 하는 클라이언트의 구조에 대해서 한번 살펴보도록 하자.\n2 티어 아키텍쳐 구조(2 Tier Architecture Structure)\n과거 많은 애플리케이션은 2 Tier 형태의 아키텍쳐를 가지고 있다. 2 티어 아키텍쳐란 클라이언트와 DB가 물리적으로 분리되어있는 구조 안에서, 클라이언트에는 UI와 비지니스 로직(Business logic)이 함께 있는 구조이다. \n\n👍 2 티어 아키텍쳐의 구조를 가져가게 될시 다음과 같은 장점이 가진다.\n\n쉽게 빠르게 개발할 수 있으며 시스템 구축이 어렵지 않다.\n애플리케이션의 구조가 단순하다.\n개발 비용이 비교적 저렴하다.\n\n보통 초기의 스타트업와 같이 적은 인원으로 시스템을 빠르게 구축할 때 이와 같은 구조를 많이 가져간다. \n👎 하지만 이러한 구조에는 장점에 비해 부각되는 단점들이 있다.\n\n클라이언트에서 DB에 직접 붙기 때문에 안정성에도 문제가 있다.\nUI와 비지니스 로직을 클라이언트에서 모두 관리하기 때문에, 클라이언트에 부하가 걸리기 쉽다.\n재사용이 어렵다.\n비지니스가 점점 더 복잡해지거나 비대해지면 관리하기가 어려워진다.\n\n이러한 단점은 스타트업의 성장통이라 하여 급격하게 성장하는 회사들이 결국에는 애플리케이션을 다시 만들게 되는 이유가 되기도 한다. 이러한 단점을 개선하고자 3 Tier 혹은 그 이상의 N Tier 아키텍쳐로 설계를 한다.\n3 티어 아키텍쳐 구조(3 Tier Architecture Structure)\n2 티어 아키텍쳐의 구조와 큰 차이점은 클라이언트 안에 존재하는 UI와 비지니스 로직(Business logic)을 분리된다는 점이다.\n\n각각의 용어는 다른 단어로도 표현되는데 클라이언트를 프레젠테이션 티어, 비지니스 계층은 애플리케이션 티어라고 불르기도 한다. 이러한 구조를 가지게 된 경우 클라이언트 로직은 Frontend 개발자가, 비지니스 로직은 Backend 개발자가 담당하게 된다. 이렇게 분리하면 장점은 무엇보다 재사용이 쉽다라는 점이 있다. 요즘 많은 엔터프라이즈급 회사에서 지향하고 있는 MSA 구조 역시 이와 같이 프레젠테이션 티어와 비지니스 티어가 분리되어야 비로서 가져갈 수 있는 구조 중 하나이다. \n👍 3 티어 아키텍쳐는 이러한 장점을 가진다.\n\n클라이언트가 직접 DB 붙는 것이 아닌 Business 계층을 통해 붙기 때문에 보안에 용이하다.\n재사용이 쉬우며, 확장에 용이하다.\n비지니스 로직과 UI를 분리할 수 있다.\n\n이러한 구조의 제일 큰 장점으로는 UI와 비지니스 로직이 분리되어 있어 애플리케이션을 배포할 때에도 UI나 비지니스 로직을 각각 배포할 수 있다. (당연히 비지니스 로직이나 DB 관련 코드가 포함되어 있으면 안된다.)\n🎉 굉장히 멀리 돌아왔지만 드디어 우리가 살펴보고자 하는 클라이언트 혹은 프레젠테이션 티어라고 부르는 UI가 등장하였다. \nMPA 애플리케이션\nMPA 애플리케이션은 Multiple Page Application의 약자로서, 전통적인 웹 애플리케이션 방식이다. MPA 애플리케이션의 가장 큰 단점으로는 프론트엔드와 백엔드가 강하게 결합되어 있다는 점이 있다. 이러한 점은 우리가 앞에서 살펴본 2 Tier 아키텍쳐의 단점과 같다는 것을 쉽게 유추할 수 있다. 이러한 구조는 우리가 편하게 웹서버라고 불리우는 형태의 애플리케이션이다. 아마 모두들 한번쯤은 jsp 혹은 php 등과 같은 템플릿 언어의 코드로 작성된 UI 로직을 수정해본 경험이 있을 것이다. 이렇게 작성된 애플리케이션에서 다른 페이지를 이동할 때 이동하려는 웹페이지에 대한 서버로 요청을 날리는 후 페이지를 새롭게 렌더링한다는 것을 알 수 있다. 조금더 자세하게 설명한다면 사용자가 “A”라는 페이지를 요청한다면 다음과 같은 과정을 거칠 것이다.\n\n“A”에 대한 페이지 정보를 서버에 요청한다.\n“A”에 대한 요청을 받은 서버는 해당하는 UI와 필요한 데이터(예를 들어 게시판이라고 한다면 게시글 데이터를 의미한다)를 이용하여 HTML 데이터로 파싱한다.\n브라우저는 전달받은 HTML 데이터를 지정된 인코딩으로 변환하여 사용자에게 보여준다.\n\n브라우저는 이러한 과정을 거치면서 페이지를 갱신하게 되며 사용자는 페이지가 이동될때마다 새로고침되는 것을 볼 수 있다. 이러한 새로고침은 로딩 시간을 늘어질 뿐만 아니라 사용자의 경험 역시 좋지 않을 수 밖에 없다. 이러한 단점을 AJAX 라는 기술이 어느정도 해결해줄 수 있었다. 과거에는 폼을 작성한 후 웹 서버에 제출(Submit)하게 되면 이러한 요청을 처리하며 브라우저가 새로고침되는 것을 볼 수 있었지만 요즘에는 거진 대부분 페이지 이동없이 화면을 전환하고 있다. 이러한 요청을 통해 웹 서버의 처리량 역시 과거보다는 많이 줄어들었다. 하지만 이러한 근본적으로 페이지 이동시 새로고침된다는 문제를 해결하기엔 역부족이었다. \nSPA 애플리케이션\nMPA 애플리케이션의 사용자 경험에 대한 단점을 개선해줄 수 있는 것이 바로 SPA 애플리케이션이다. SPA 애플리케이션은 Single Page Application의 약자로서 다른 말로는 CSR(Client Side Rendering)이라고 표현하기도 한다. 이러한 SPA 애플리케이션은 하나의 HTML 파일만 있으면 나머지는 Javascript를 이용해서 동적으로 화면을 구성할 수 있다. 서버는 어떠한 요청을 받던 무조건 동일한 HTML 파일은 내려준다. 그 후 사용자는 하나의 HTML 파일에서 Javascript를 이용해 동적으로 구성되는 UI 화면 속에서 마치 페이지가 이동되는 것처럼 보여지는 느낌을 받는다. 이렇게 URL을 조작해 마지 이동되는 것처럼 보여지게 하는 것은 해쉬 방식 혹은 HTML5의 히스토리 API를 통해 가능하다. \nSPA의 가장 큰 장점으로는 변경된 부분만 효율적으로 변경한다는 점이다. \n\n이러한 애플리케이션 내에서 사용자에게 제공하는 모든 페이지는 Javascript 안에 포함되어져 있으며, Javascript를 이용하여 동적으로 화면이 갱신되어지는 것처럼 보여지게 하는 것이다. 이러한 점은 좋은 사용자 경험을 제공해줄 수 있다는 장점을 가진다. 하지만 그렇다고 장점만 있는 것은 아니다. 앞에서 설명했든 애플리케이션의 모든 페이지들이 Javascript 안에 포함되어져 있기 때문에 애플리케이션에서 사용하는 Javascript 파일은 점점 더 비대해지며 이러한 이유로 인해 애플리케이션의 초기 랜더링 속도가 느리다. 뿐만 아니라, Javascript를 이용하여 화면을 동적으로 그리기 때문에 SEO에 굉장히 취약하다는 단점을 가지고 있다. 물론 애플리케이션의 초기 랜더링 속도를 개선하기 위해 써드파티 라이브러리들을 Vendor 단위로 나눈 후 CommonChunking으로 별도로 청킹 처리를 해주거나 Dynamic Loading을 이용하여 Javascript 파일의 리소스를 최대한 줄이는 등의 전략을 세우고 있다. 하지만 이렇게 아무리 노력한다고 해도 SSR보다는 초기 랜더링 속도가 늦을 뿐만 아니라 근본적인 문제인 SEO를 해결할 수 없다는 단점을 가지고 있다. 이 다음에는 SPA의 이러한 단점을 해결하기 위한 방법에 대해서 살펴보도록 하겠다.","link":"http://blog.martinwork.co.kr/devops/2019/05/24/server-side-rendering01.html","title":"왜 다시 SSR인가 01","pubDate":"2019-05-23T15:50:51.000Z"}},{"node":{"contentSnippet":"2019년 4월 11일, 네이버의 사옥 그린팩토리에서 네이버 테크 콘서트가 열렸다. 주제는 프론트 엔드 개발 전반적인 기술과 경험, 그리고 개발 문화에 대한 공유였다. 일단은 컨퍼런스의 대상은 대학생 대상으로 주최된 행사였으나, 막상 가서 보니 실무자들 참여율이 높은 편이었다. 아무래도 대학생을 대상으로 하는 컨퍼런스이기 때문에 실무자들에게는 이미 익숙해져 있는 이야기도 있었지만, 개인적으로 컨퍼런스의 전체적인 내용에 아주 만족스러웠다. 그 중 인상 깊었던 내용들에 대한 리뷰를 남기고자 한다. \n플랫폼 UI 개발 전략\n첫번째는 이주용님의 플랫폼 UI 개발 전략의 모든 것 이라는 주제의 세션이었다. \n\n내용은 스마트에디터의 UI를 개발하면서 생겼던 이슈 및 해결 과정에 대한 것들을 풀어나가는 시간이었다. 모든 개발의 시작은 좋은 설계에서 시작한다고 생각한다. 개인적으로 생각했을 때, 중요한 것은 새로운 비지니스가 추가될 때, 그러한 요구 사항을 수용할 수 있느냐와 두번째는 유지 운영의 용이성이라고 생각한다. \n스마트 에디터의 설계를 하면서 가장 중요하게 생각했던 것의 첫번째는 UI 컴포넌트의 공통화는 디자인 중심이 아닌 기능 중심으로 설계를 했다는 점이다. 디자인은 템플릿 요소의 관점에서 접근하며, 실제 UI 컴포넌트를 공통화를 고민할 때는 기능 중심의 분석을 거쳐 설계를 했다고 한다. \n두번째는 조건 및 상태에 따라 다른 스타일 적용했다는 점이다. 우리가 생각하는 CSS는 Javascript와 같이 동적인 언어가 아닌 정적인 언어이다. 하지만 비지니스의 니즈에 따라 조건이나 상태에 따라 UI의 화면이 수도없이 바뀐다. 마치 아래와 같은 스타일을 작성하는 이유이다.\n.button {  // ...  .has-text {    // ...  }  .has-icon {    // ...  }}\n이러한 상황을 고려하여 CSS는 정적인 언어이지만 설계는 동적으로 이뤄져야 한다는 것이다. \n\n이와 동시에 중요한 것은 모듈화와 공통 요서 분리 작업이다. 위와 같이 상태나 조건에 따라 컴포넌트의 스타일이 동적으로 변경될 수 있으려면 제일 먼저 비지니스의 니즈를 분석한 후, 공통 분모를 도출 해야한다. 무엇보다 이러한 공통점을 찾을 때는 단순하게 디자인만 고려하는 것이 아니라 꼭 기능도 함께 고려를 해야 한다. 디자인이 비슷하다고 해도 기능이 다른 경우가 존재하기 때문이다. \n\n이러한 고민들을 해결해나가는 과정에서 스마트 에디터의 팀은 개발 방법론과 CSS Preprocessor도입했다고 한다. 개발 방법론은 BEM, SMASS, OOCSS 등과 너무도 유명한 3가지 방법이 있다. 이 중에서는 대표적으로 BEM 방식이 가장 인기 있는 방법이지만, OOCSS를 선택했다고 한다. 혹시나 스마트 에디터 팀에서 OOCSS를 선택했다고 꼭 OOCSS 방법만 선택할 필요는 없다. 위의 3가지 방법은 각각의 장단점이 있기 때문에 꼭 하나의 방법을 고수하는 것보다는 상황에 따라 선택하면 될 것 같다. \n첫번째 세션에서 제일 인상 깊었던 말은 플랫폼은 만능이 아니다라는 말이었다. 내가 만든 코드가 모든 요구사항을 다 소화할 수 없으며, 플랫폼은 지속적으로 발전하기 때문에 코드 역시 변화해야 한다. 소프트웨어 장인 이라는 책에서는 개발자는 건축가와 같이 설계하고 끝나버리는 직업이 아니라, 정원사와 같이 꾸준히 관리를 해줘야하는 직업이다 라고 한다. 그렇기 때문에 개발자로서 살아가는 우리들이 꾸준히 리펙토링과 클린코드에 관심을 갖는 게 아닐까?\n \n회사에서 성장하기\n두번째로 인상 깊었던 내용은 한재엽님의 주니어 개발자의 성장에 대한 뻔하지만 뻔하지않은 이야기 였다. \n\n개발자라면 한번쯤 스스로의 위치와 앞으로 내가 얼마나 나아갈 수 있을지에 대한 고민을 한번쯤을 해보았을 것이다. 이러한 고민을 재엽님의 경험기에 빗대자면, 많은 시간을 투자하여 다음의 것들을 공부하면 된다.\n\n출근 전후 그리고 주말 내내 시간을 투자해서 공부를 한다.\n사이드 프로젝트를 진행한다.\n개발 관련 뉴스레터를 통해 꾸준하게 개발에 관심을 놓지 않는다.\n개발 관련 서적을 독파하여 기본기에 충실한 공부를 한다.\n블로그를 운영한다.\n알고리즘을 하루에 한 문제씩 풀어 논리적인 사고 방식에 대한 감을 잃지 않는다. \n\n만약 이런 것들을 하루에 한다고 하면 우리는 누구나 노력형 천재 개발자가 될 수 있다. 하지만 안타깝게도 우리의 인생은 개발자로서의 성장만 꿈꾸기에는 너무나 많은 이벤트들이 있다. 가족 혹은 애인과 시간도 보내야 하고, 가끔 번아웃된 나 자신을 위해 스스로르 보듬어줘야하기도 하며, 회사를 다닌다면 야근과 같이 불가피한 상황에 놓일 수도 있을 것이다. 그렇다면 빠른 성장을 포기해야 할까?\n재엽님의 주제가 제일 마음에 와닿는 것은 이러한 현실적, 물리적 제약을 해결해나간 방법에 있다. 바로 회사에서 성장하기이다. 언젠가 누군가에게 했던 말 중 제일 와닿는 말이 하나가 있다. 개발자의 실력이 궁금하다면 그 사람의 비지니스의 코드를 보면 알 수 있다. 회사는 나와 함께 동반 성장을 해나가야할 파트너이다. 개인은 정체되어 있지만 회사만 성장을 하거나 혹은 개인은 성장을 하지만 회사가 정체되어 있는 불균형은 일하고 있는 환경이 좋지 않은 상황일 가능성이 크다. 그러한 의미에서 우리는 회사라는 환경을 잘 이용하여 동반 성장을 꿈꿔야 한다. \n물론 사이드 프로젝트를 통해 회사에서 채우지 못한 개발적 욕구를 채울 수도 있다. 하지만 사이드 프로젝트를 진행하다보면 모든 욕구를 다 채울 수 없다는 것을 느낄 수 있다. \n\n재엽님이 말씀하신 간헐적 버그에 대한 무시, 버그 제보에 대한 무시, 디바이스 이슈를 제외하고도 가장 큰 것은 유저풀이다. 하루에도 수십개의 사이드 프로젝트가 탄생하며 사라진다. 정말 성공한 사이드 프로젝트를 제외하고는 유저풀이 1000명 넘는 플랫폼이 있을까? 물론 적은 수더라도 내가 사이드 프로젝트로 만든 플랫폼을 이용해준다는 것은 정말 고마운 일이지만 개발적 욕구를 모두 채우기엔 유저풀이 적은 것은 사실이다. 많은 유저풀이 있다는 것은 내가 만든 플랫폼에 대한 자부심을 갖기에도 충분하지만, 더 많은 사용자의 데이터를 통해 사용자들의 플랫폼에 대한 니즈가 점점 더 커져 플랫폼을 발전시키는 계기가 되기도 한다. 그러한 점에서 개인적으로는 유저풀 역시 꾸준한 플랫폼의 발전에 없어서는 안되는 요소라고 생각한다. 사이드 프로젝트를 통한 플랫폼에서는 이러한 것을 채울 수 없지만, 회사에서 만드는 플랫폼의 경우에는 상황이 다르다. \n일단은 회사의 플랫폼의 경우, 작은 스타트업의 경우 회사의 사활이 걸려 있을 수도 있고, 큰 회사더라도 플랫폼 사업 하나에 걸려있는 비용이 천문학적인 비용인 경우가 많다. 그런 만큼 회사에서는 더 많은 관심을 가질 수 밖에 없으며, 간헐적 버그, 디바이스 이슈 등의 사소한 것 하나하나도 전문적인 QA 검증 과정을 거쳐 완벽하게 만들고자 노력한다. 이러한 환경은 개발자가 성장을 하기에 좋은 환경이라고 생각한다. 우리에게 주어진 환경 안에서 이러한 환경을 잘 이용하여 설장하는 계기가 되었으면 한다.\n똑똑한 질문하기\n일을 하다보면 스스로 혹은 동료에게 수없이 많은 질문을 하게 된다. 하다못해, 스택오버플로우만 가도 수없이 많은 개발자들의 질문과 답변이 존재한다. 커뮤니티의 수많은 질문과 답변을 통해 우리는 스스로 성장하기도 하지만 때때로 성의없는 질문들도 있다. 질문의 내용만 살펴봐도 질문을 하기에 충분한 고민을 하지 않았다는 것을 유추할 수 있다. 스스로 해결하려 하는 것이 아니라 질문을 남의 시간을 통해 해결하려고 하는 아주 괘씸한 질문들이다. \n반대로 회사에서 동료에게 이러한 질문을 던지게 된다면, 나의 시간 뿐만 아니라 동료의 시간까지도 빼앗는 상황이 되기도 한다. 그리고 이렇게 몇번 성의 없는  질문을 하다보면 동료에게서 미움을 살수도 있다. 그렇다면 성의 없는 질문은 무엇일까?\n일단 먼저 질문을 하기 전에 충분하게 검색을 해보자. 우리에게는 구글이라는 훌륭한 플랫폼과 스택오버플로우라는 훌륭한 선배 개발자들이 많다. 뿐만 아니라 체계가 잡혀있는 회사 내에는 트러블 슈팅을 다룬 문서가 존재하기도 하다. 대부분의 질문들은 다 그안에서 찾을 수 있다. 하지만 가끔 어떠한 키워드로 찾아야 할지 조차 감을 못 잡을 때가 있다. 그러한 경우에는 동료에게 어떠한 질문을 해야 하는지, 사전 준비를 거쳐야 한다. \n일단 첫번째로 발생 상황을 정리를 한다. 예를 들어 ‘하위 IE 브라우저에서 특정 컴포넌트에서 문제가 발생한다.’ 와 같을 것이다. 물론 조금 더 상세하면 좋다. ‘하위 IE 브라우저 중 IE8에서만 유저의 데이터를 노출시켜주는 UserInformation 컴포넌트에서 x라는 문구와 함께 랜더되지 않는다’ 와 같을 것이다. 다음으로 그러한 문제를 해결하기 위해 어떠한 시도를 했는지도 함께 전달을 해주면 좋다. 상대방 역시 내가 했던 삽질을 그대로 반복해서 시간을 낭비할 수 있기 때문이다. 물론 동료 역시 나의 질문에 답변을 못할 수도 있다. 그런 경우는 차라리 덕 디버깅과 같이 내가 생각하고 있는 바를 논리적으로 토론해보는 것도 좋다. 이러한 방식 역시 상대방과 이야기를 하다보면 스스로 논리에 구멍이 있는 것을 찾기도 혹은 동료가 찾아주기도 한다. 고로 현명한 질문을 통해 서로 발전하는 계기가 되어야 할 것이다.\n성능 최적화\n최근에 회사에서 오픈한 마이티몬의 프로젝트가 안정화 단계에 들어선 후, 플랫폼의 성능에 대한 고민을 하고 있었다. TTI(Time to interaction) 수치 라던지 초기 화면이 그려지는 로딩의 속도 혹은 리소스 절감 등이 포함이 될 것이다. 그러던 중 손찬욱님의 강연이 그러한 고민을 해결할 수 있는 하나의 빛과 같았다.\n\n일단 클라이언트에서 성능을 개선한다고 하면 우리가 흔히 생각하는 것들이 있다. 일단 제일 간단한 것은 css는 head 안에, js는 body태그가 끝나는 바로 상단에 넣어 준다던지, 혹은 레이지 로딩 적용, 요청수 줄이기 등이 있을 것이다. 하지만 제일 중요한 것은 현재의 상태를 분석하는 것이다. 현재의 상태를 알아야 분석을 통해 어떠한 부분을 개선할 수 있는지, 어떻게 개선하면 좋을지 등에 대한 해답을 찾을 수 있기 때문이다. 그러한 성능 분석은 아래와 같이 Chrome devtools의 Network 안에 있는 Waterfall 그래프를 통해 확인할 수 있다.\n\n이렇게 나온 그래프를 높이는 낮게, 오른쪽에 가깝게, 그래프 하나당 길이는 짧게 줄이는 것이 성능을 최적화할 수 있는 방법이다. 조금 더 풀어서 설명을 하면, 일단 그래프의 높이가 낮으려면 요청하는 리소스의 갯수가 적어야 한다. 그 말인즉, 모든 리소스를 청킹(Chunking)한다고 성능이 좋아진다는 것은 아니라는 말이다. 두번째로 오른쪽에 가깝다는 것은 리소스 간의 다운로드 수를 줄인다는 것이다. 이러한 두번째의 경우에는 브라우저의 랜더링에 대한 배경이 있어야 한다. 이 부분에 대해서는 추후 다른 포스팅에서 깊이 다루도록 하겠다. 마지막으로 세번째는 사실 클라이언트의 이슈보다는 서버쪽의 이슈에 가까운 경우가 많다. 그래프가 길다는 것은 서버에서 비지니스 로직을 처리하는데 시간이 오래 걸리기 때문이다.","link":"http://blog.martinwork.co.kr/review/2019/04/11/2019-04-11-naver-tech-review.html","title":"Naver Tech Concert 리뷰","pubDate":"2019-04-10T15:50:51.000Z"}},{"node":{"contentSnippet":"들어가기전\n\n책에 대한 리뷰를 시작하기 전에 미리 이야기를 하자면 이 책에 대한 리뷰 작성은 누구한테 부탁받은 것도 아니며 개인의 돈으로 책을 구매하여 읽은 후 작성하였습니다. 그러므로 맹목적인 책에 대한 긍정적인 리뷰가 아닌 개인적인 의견임을 미리 밝힙니다. \n\n들어가기전에 바쁘신 분들을 위해 책에 대한 한줄평을 남기자면 책의 모든 내용이 나에게 도움이 될순 없겠지만 내가 부족한 부분과 앞으로의 성장을 위해 필요한 것에 대한 가이드가 되기에는 충분했다. 라고 할 수 있다. 필자의 경우 이 책을 지인에게 추천을 의향도 있으며, 실제로 이 글을 작성하는 날에도 지인에게 이 책에 대해서 추천을 했다. 그럼 어떠한 부분들이 좋았는지 어떠한 부분이 아쉬웠는지에 대해서 상세하게 설명을 시작해보겠다.\n과연 이 책은 어떠한 점의 나의 구미를 땡겼는가?\n역시 보기 좋은 떡이 먹기도 좋다고 빼놓을 수 없는 것은 책의 표지가 아닐 수 없다. 한달에 평균적으로 15만원 정도의 책을 사는 필자로서는 서점에서 모든 책의 내용을 파악하고 사기보단 일단 표지가 읽고 싶게 생긴 책을 무작정 선정 후 내용을 살펴본 후 구입 여부를 판단한다. 고로 필자에게 책의 표지는 책 구매 의사 여부에 대한 영향도가 크다고 할 수 있다. 그런 의미에서 소프트 스킬이라는 책의 표지는 개인적으로 합격이었다:)\n\n엠포싱이 들어간 표지 뿐만 아니라 무채색을 좋아하는 필자에게는 너무나 마음에 드는 색상이었다. 그리고 너무 개발자스럽지 않은 표지가 대중교통을 이용하면서도 읽기에 부담되지 않았다:) 무엇보다 평범한 개발자 라는 책의 부제에서 이것은 나같은 평범한 개발자를 위한 책이다!라는 생각이 들었다. 그러고나서 책의 첫장을 펼지면 다음과 같이 이 책에 대한 저자의 철학을 바로 살필 수 있다.\n\n언젠가 지인이 “요즘은 집필되는 책은 내용에 대한 깊이나 몰입도가 예전에 출판된 읽기 좋은 책에 비해 아쉬운 경우가 많다.” 라는 이야기를 한적이 있다. 사실 필자의 경우에도 막상 사오고나서 마음 잡고 책을 읽으려고 앉았다가 실망한 적이 한두번이 아니다. 그런 의미에서 이 책에 대한 필자의 철학은 또다른 감동을 주기에 충분했다:)  \n이러한 감동을 가지고 책의 내용을 한글자 한글자 마음에 새기며 살펴보았다. \n책의 내용이 나에게는 어떠한 의미였지?\n첫번째로 와닿는 말은 소프트 스킬은 생각보다 중요하다 였다. \n\n어느 한 포스팅에서 기술 업계에 대한 독성 말투에 대해서 써놓은 글을 보았다. (궁금하신 분은 기술 업계의 독성 말투 문제, 고칩시다!에서 확인 할 수 있습니다.) 지금까지 짧게 다닌 회사를 제외하고 최소 1년 이상 다녔던 회사는 3개, 짧은 회사까지 포함한다면 5군데의 회사에 있었지만 어느 단 한곳 인간 관계에서 상처받지 않은 사람이 없었던 곳이 없었다. 위의 독성 말투에 대한 포스팅에서도 강조를 하고 있지만 우리가 하는 모든 일이 다양한 사람들의 참여로 인해 일어나기 때문에 이 책에서도 역시 소프트 스킬이 아주 중요하다고 강조한다. \n\n여기까지만 본다면 아마 ‘뭐야, 그건 어디서나 볼 수 있는 내용이잖아.’ 라는 생각을 할수 있을 것이다. 하지만 이 책에서 문제아 다루기 라는 내용으로 정말 마음 속 깊은 곳을 찌르는 말이 등장한다. 아마 포스팅에서도 이야기 하는 습관적 반대론자도 이 안에 포함되지 않을까 싶다. 새로운 시도하려고 했을 때, 맹목적인 비난을 통해 의욕을 꺾어놓으려고 하거나 혹은 새로운 시도를 하려는 사람을 깎아내리려는 사람은 어느 집단에나 꼭 한명 이상씩은 있지 않을까 싶다. 이 사람들을 이 책에서는 문제아 라고 표현하며 그 사람들과 만났을 때는 바꾸려고 하거나 잘 지내보려고 하지 말아라 라는 이야기를 한다. 말 그대로 교류를 최소화하며 되도록이면 피해야하는 사람을 굳이 내 사람 혹은 눈 안에 들려고 노력할 필요는 없다는 의미이다. 개인적으로도 이러한 사람들은 어느 집단에 가나 약간의 말만 주고받아도 충분히 파악이 되기 떄문에 되도록이면 피하는 것을 추천한다:) \n두번째로 와닿는 말은 이루고 싶은 게 있다면 이미 이룬 것처럼 연기하라이다. 예전 어디선가 구글러들은 구글에 입사해서 구글러가 아니라, 이미 구글러였기 때문에 구글에 입사한 것이다라는 식의 글을 읽은 적이 있다. 정확하게 이러한 맥락을 아니였는데, 비슷한 의미였던 것으로 기억한다. (아무리 찾고자해도 너무 오래전이라 저는 검색에 실패했지만 혹시나 이 글을 읽는 분 중에 정확하게 아시는 분은 메일 주시면 조그마한 성의 표시라도 하고 싶습니다:) 아시는 분은 연락 바랍니다!) 이 글을 보고 구글러가 일하는 방식에 대해서 연구하고 실제 그들처럼 되고자 연기했던 적이 있었다 껄껄:) \n\n여기에서 이야기하고자 하는 핵심은 거짓 연기를 하라는 것이 아니라 그러한 태도로 살려고 노력한다면 원하는 것을 이룰 수 있다라는 것이다. 잠깐이나마의 연기를 통해 이러한 행동 및 마음가짐은 스스로가 프로패셔널이라는 생각을 불어넣어줄 뿐만 아니라 개개인에게 책임감과 의지를 불어넣기에 충분하다는 생각을 하게 되었다. 다만 중요한 것은 오만함과 자신감은 꼭 구분하여 그로 인해 본인 스스로를 과대평가하는 실수는 저지르지 않길 바란다.\n세번째는 셀프 마케팅이라는 말이었다. 개발자 스스로 본인의 가치를 증명하며 재능을 돋보이게 해야한다는 의미이다. 요즘 우리는 자기 PR의 시대 속에 살고 있다고 말을 하면서도 실제로는 쑥쓰러워서 남들 앞에 나서는 걸 꺼려한다. 하지만 꼭 셀프 마케팅이라는 것을 거창하게 생각할 필요는 없다. 꼭 남들 앞에 나서지 않더라도, 이미 우리는 셀프 마케팅을 하고 있다. \n\n이력서 또한 지원한 회사에 ‘나는 귀사에 입사해서 이러한 것들을 해낼 수 있어요’ 라는 의미의 셀프 마케팅이 된다. 혹은 필자와 같이 소소하게나마 블로그를 운영하는 것 역시 셀프 마케팅일 수 있다. 필자의 경우 블로그를 운영하며 회사들의 러브콜을 받기도, 책 집필에 대한 제안을 받기도 했던 경험이 있어 개인적으로는 블로그를 운영하는 것에 대해서 적극 추천한다:) (형편없는 제 글을 보고 연락주신 담당자분들에게 굉장히 감사하다는 말씀을 드립니다 하하하:) ) 물론 우리가 알면서도 당하는 허위 광고식의 마케팅이나, 과대 포장이 이뤄져서는 안된다.\n\n아마 한번쯤 당해본 사람이라면 막상 그 포장을 뜯어본 후, 엄청난 분노를 느꼈을 것이다 :(((( 이러한 분노를 또다른 누군가에게 전파하는 일이 없도록 조심하도록 하자 \n네번째는 학습 방법 익히기이다. 책에서는 학습 방법에는 10단계의 과정을 통해 학습 방법에 대해서 표현하지만 본 포스팅에서 이야기하고자 하는 것은 그런 것이 아니다. 얼마 전 한 커뮤니티에서 고민 상담글로 이러한 글을 본적이 있다. ‘6개월동안 국비 지원 학원에서 열심히 공부한 후 신입으로 막상 취직해서 보니 자기가 너무 부족함을 깨달아서 다시 국비 지원을 통해 학원에서 학습을 하고자 한다.’ \n\n스스로가 부족하다는 고민은 개발자라면 공통적으로 하는 고민일 것이다. 여기에서 중요한 것은 학원에서 6개월 배우고도 부족함을 느꼈는데 다시 학원으로 돌아가려고 하는 글쓴이가 안타까웠다. 사실 필자의 경우 한번도 개발 관련해서 한번도 학원에서 배워본적이 없어 주관적인 의견일 수는 있겠지만 학원은 앞으로 내가 공부해야할 것들에 대해 방향성만 제시해준다고 생각한다. 그 이후부터는 지식에 대한 갈증은 스스로 해결해야 한다고 생각한다. \n만약 위의 고민과 같이 시간이 지남에 따라 과거를 돌아보았는데 과거의 나와 현재의 내가 별 차이가 없다면 학습의 방법이 잘못된 것이 아닌가를 고민해보길 바란다.\n\n다섯번째는 멘토에 대한 내용이다. 멘토에 대한 사전적인 의미를 살펴보면 정신적으로나 내면적으로 신뢰할 수 있는 상담 상대, 지도자, 스승님, 선생이라는 뜻을 가진다. 많은 사람들이 멘토에 대한 의미를 헷갈려서 스승이라고 생각한다. \n\n멘토는 현명하고 신뢰할 수 있는 상담 상대, 지도자, 스승, 선생의 의미이다. 하지만 그렇다고 멘토 = 스승인 것은 아니다. 보통 스승이라고 하면 자신보다 나이가 많은 사람을 떠올리지만, 멘토의 경우는 동갑내기 친구가 될 수도 있다. 심지어는 자신보다 어린 사람이 멘토가 될 수 있다. 다시 말해 스승이 무엇인가를 직접 가르쳐주는 사람이라고 한다면 멘토는 이끌어 주는 사람 이라는 뜻이 강하다.\n\n[출처: 나무위키-멘토]\n물론 멘토가 기술적으로 뛰어나서 멘토에게 직접적으로 배울 수 있다면 굉장히 좋을 수 있다. 하지만 필자가 생각하는 멘토란 꼭 기술적으로 월등히 뛰어나서 직접적으로 기술을 가르쳐주는 사람보단 방향을 제시해줄 수 있는 사람이라고 생각한다. 물론 기술적으로도 가르침을 받으면서 방향을 제시해줄 수 있는 멘토를 만난다면 더할 나위 없이 좋을 것이다. 필자의 경우 지금까지 6명 정도의 멘토를 만났다. 그분들 중 필자와 고등학교 동창인 사람도 있고, 전직장 동료인 사람도 있으며 혹은 학교에서 우연치 않게 만난 분 그리고 같은 이커머스 종사자인 분도 있다. 지금까지 2년 가까이 거의 매주 뵙고 있는 멘토님 같이 기술적으로 뿐만 아니라 충분한 방향성을 제시해주셨던 분도 계시지만 앞에서 언급한 모든 분들이 기술적으로 월등히 뛰어나서 멘토인 것은 아니다. 물론 지금은 웹 개발로 전향하시긴 했지만 이분들 중에는 아예 웹 개발을 하지 않았던 분도 계신다. 이 분들을 만나면서 필자의 경우는 기술적인 성장도 성장이지만, 기본적인 마음 가짐에 대한 성장에 많은 도움을 받았다고 생각한다. 기술적으로 성장하는 것은 멘토가 없이도 성장이 가능하지만, 마음 가짐에 대한 성장은 혼자서 바꾸기가 쉽지 않다. 누군가의 본보기가 되여 ‘나도 언젠가는 저 사람의 모습처럼 누군가에게 비춰지고 싶다’라는 귀감을 사기란 정말 제대로 된 멘토가 없다면 쉽지 않다. 아마 이 글을 읽는 여러분이라면 한번쯤은 그런 사람을 만나보지 않았을까란 생각을 한다. 반대로 말하자면 꼭 기술적으로 월등하지 않아도 되지 우리 모두 누군가의 멘토가 될 수 있다는 의미이기도 하다. 누군가의 귀감이 되어 닮고 싶은 사람이 된다는 것이 멋진 일이 아닐 수 없다:) \n\n개인적으로 추천하는 멘토 관련된 동영상이다. 혹시나 궁금 하신 분은 한번 보시는 것도 추천한다.\n여섯번째는 탈진 극복에 대한 내용이다. 필자의 경우 회사에서 일하는 시간 40시간을 제외하더라도 평균적으로 주 30시간 정도는 관련 지식을 쌓기 위해 공부를 하고 있다. 물론 프로젝트로 인해 야근을 하는 경우는 주에 15시간을 못 채우는 경우도 있지만, 정시 퇴근해서 한두시간 저녁 먹고 늦장부린다는 가정하에 대략적으로 30시간이다. 그리고 주변을 살펴보면 필자 뿐만 아니라 대부분의 개발자들이 그렇게 하고 있다. (개인적으로 이러한 개발자들과 함께 한다는 것이 얼마나 행복한 것인지를 깨달았다) 하지만 이러다보면 제일 중요한 것이 바로 컨디션 조절이다. 그리고 이러한 컨디션 조절에 실패한 경우 흔희 번아웃(Burn out)에 빠졌다고 표현한다.\n\n제일 위험한 것이 바로 이 번아웃 증후군에 걸린 경우이다. 이러한 증후근을 정신적 탈진 이라는 표현을 한다. 이러한 정신적 탈진을 극복하기란 쉽지 않다. 책에서는 정신적 탈진을 벽이라고 표현하며 벽 너머에는 좋은 보상이 존재하며 탈진이라는 고통을 견뎌야지만 더 강한 에너지와 성취 동기가 생긴다고 이야기 한다. 누구나 한번쯤은 이러한 번아웃 상태에 빠져본 경험이 있을 것이며, 그 벽의 높이는 개개인에게 다르게 와닿을 수 있다. 누군가에게는 별것 아닌 것처럼 비춰져보일 수 있지만 나에게는 그 벽이 마치 오르지 못할 산보다 더 높을 수도 있다.\n \n책에서도 그냥 넘으면 된다고 표현하지만 목표를 성취하고 나서의 성취감은 그 산을 넘을 수 있는 충분한 이유가 되지 않을까란 생각을 한다. 마치 과거 우리가 엄청 큰 고민이라고 여겼던 것이 시간 지나고 보면 별것 아닌 것처럼 웃어넘기듯, 사실 벽을 넘고나면 별거 아니었을 수도 있다. 그러한 사실은 벽을 넘어본 사람들만 알 수 있는 또다른 특권이지 않을까? :-)\n \n이 책의 모든 것이 다 만족스웠을까?\n앞에서 설명했듯 책의 모든 내용이 만족스러울 수 없고 도움이 되지는 않았다. 하지만 그런 부분들은 부동산이나 주식 등과 같이 저자와 같이 미국에 사는 사람들에게는 도움이 될 수 있을 내용이다. 이런 부분을 제외한다면 대체적으로 필자에게는 또다른 영감과 열의를 일으켜준 책이었다. 앞에서도 말했듯이 이러한 아쉬운 부분들은 개인적인 견해일 뿐 읽는 사람에 따라서는 또다른 영감과 느낌을 줄 수 있을 것 같다라는 생각을 한다. 만약 나에게 앞에서 설명했던 멘토가 없다고 생각된다면 이 책을 또다른 정신적 멘토로 삼고 천천히 읽어보는 것도 굉장히 좋을 것 같다.\n \n끝으로 책의 한 부분에서 표현하듯 이 책은 스스로의 발전을 원하는 사람에게 또다른 성장을 가져다 주기에 충분한 책이라는 생각을 한다.","link":"http://blog.martinwork.co.kr/review/2019/03/17/soft-skill-review.html","title":"소프트 스킬에 대한 리뷰","pubDate":"2019-03-16T15:50:51.000Z"}},{"node":{"contentSnippet":"들어가기전\n\n글을 시작하기에 앞서 미리 밝히자면, 이 글은 글쓴이의 아주 주관적인 의견이며 팀에 따라 혹은 시간에 따라 달라질 수 있음을 미리 밝힌다. 글쓴이의 경우 과거 2018년의 회고에서도 밝혔듯 회사에 대한 만족도가 높은 상태에서 작성했을테니 너무 회사의 분위기나 글쓴이의 리뷰를 모두 신뢰하지 않길 바란다.\n\n필자는 2019년 8월 9일 부로 티몬이라는 회사를 떠났다. 현재 작성된 글을 작성할 당시와 퇴사한 시점에는 회사의 분위기도 많이 변화하였으며, 퇴사한 이후에도 점점더 바뀔 예정이다. 그러므로 그냥 과거의 티몬의 문화는 이랬구나 라는 정도로 가볍게 읽기를 권장한다. \n\n지금 현재 글쓴이의 경우 티켓몬스터의 표준화 랩에서 배송 파트의 Typescript와 React를 이용하여 프론트 개발을 담당하고 있습니다. 아직은 부족한 점이 많은 개발자이지만, 좋은 프로덕트를 만들기 위해 온갖 잔머리를 굴리며 얼마 있지 않은 지식을 쥐어짜고 있습니다. 구매 내역 개편 프로젝트 이후 최근에는 서비스의 안정을 위해 최대한 테스트 코드를 빡세게 작성하여 커버리지를 올리기 위해 고군분투하고 있습니다. \n티켓몬스터에 합류하기\n티몬의 채용 절차는 홈워크 통과 후 1차 기술 면접, 2차 CTO 면접 그리고 티몬만의 특별한 면접인 써드아이 면접 이렇게 3가지 면접으로 이루어진다. 글쓴이의 경우 1차 기술 면접에 총 3분이 들어왔고, 면접은 그동안 해왔던 업무에 대한 이야기와 그 업무 속에서 어떠한 점을 개선해갔는지에 대한 이야기를 주로 했다. 개인적으로는 편안한 분위기에서 진행되었다. 1차를 합격하고 나면 바로 그 유명한 2차 면접인 CTO 면접을 보게 된다. 이미 면접을 들어가기 전부터 2차 면접에 대한 분위기에 대해 익히 들어 마음의 준비를 하고 갔다고 생각했었지만 막상 닥치고 보니.. 정말 쉽지 않았다. (분위기에 대해 궁금하신 분은 잡X래닛에 면접 관련 후기를 보면 많이 나와있다.) \n\n질문에 대한 대답을 거의 하지 못했다. 그렇게 대략 30-40분 정도의 2차 면접이 끝나면 바로 써드아이 면접을 보게 된다. 써드 아이는 같은 직군의 사람이 들어와서 보는 면접이 아니라 다른 직군 사람과의 면접으로 쉽게 생각해서 회사의 문화에 어울리는 사람인지 등을 보는 면접이라고 생각하면 될 것 같다. 나중에 들은 이야기이지만 2차 면접에서 멘탈이 깨진 많은 사람들이 써드 아이 면접에 임할 때 이미 떨어졌다고 가정하고 면접에 임하는 경우가 많다고 한다. 하지만 걱정할 필요 없다:) 입사하여 만난 대부분의 티모니언들이 2차에서 여러분과 똑같은 생각을 했다고 하니, 써드아이 면접에 임할 때도 최선을 다하길 바란다. 글쓴이 역시 마찬가지였다. 면접을 여러번 본 경험이 있다면 면접에서의 분위기만 봐도 내가 떨어졌는지 알 수 있는데, 이때가 바로 내가 떨어졌다고 생각된 때였다. 하지만 면접 끝나고 1시간도 채 되지 않아 최종 합격 전화를 받게 되었다. 여러분은 모두 뛰어난 사람이니 2차 면접에서 너무 기죽지 않았으면 하는 바램이다 :-) \n최종 처우 협의 후 2018년 10월 15일, 티모니언으로 티켓몬스터에 합류하게 되었다. \n\n참고로 덧붙이자면 티켓몬스터의 임직원을 티모니언이라고 부른다. 입사 첫 날 업무 지원 센터에서 기다리게 되는데 그때 마주칠 수 있는 광경이다. \n\n그렇게 기다리다보면 인사팀과의 계약서 작성 후, 앞으로 일하게 될 자리를 배치 받게 되며 그 자리에는 웰컴 키트가 놓여 있다.\n\n사실 티몬이라는 회사를 좋아했던 가장 큰 이유는 바로 이러한 브랜딩이었던 것 같다. 문방사우 세트와 티몬 케릭터 나노 블럭과 티셔츠, 임시 사원증 등등이 들어있지만 개인적으로 제일 마음에 드는 것은 티셔츠였다. 물론 입사할 때가 아직은 반팔을 입을 때가 아니라 티셔츠를 입어보진 못했지만 여름이 되면 교복과 같이 입고 다니지 않을까 싶다 :-)/images/review/be-a-tmonian02.png\n그리고 그 자리에는 윈도우 데스크탑이 놓여있다. 이 때 굉장히 괴리감이 들 수 있다. 맥북도 아니고 윈도우에 데스크탑이라니…. \n\n하지만 걱정 안해도 된다. 13인치의 맥북을 하나 더 준다. 윈도우는 처음 끄적끄적하다보면 킬일이 거의 없다. 물론 윈도우에서 개발하는 경우는 예외로 한다. \n티모니언이 누릴 수 있는 복지\n아마 많은 분들이 궁금해 하는 것이 티모니언이 되면 누릴 수 있는 복지가 무엇인지에 대한 궁금점이지 않을까 싶다. \n첫번째는 지하에 티모니언들만 이용할 수 있는 저렴한 카페가 있다는 점이다. 아메리카노가 1,000원 정도로 굉장히 저렴한 편이다.\n\n이전 회사에서도 사내 카페가 있었지만, 전 회사의 사내 카페보단 티몽 카페가 훨씬 넓고, 그 옆에 조그만하게나마 간단한 간식 거리를 살 수 있는 공간도 있다. \n두번째는 헬스장에 대한 지원이다. 물론 하루에 한번으로 제한되어 있지만 듣기로는 회원권만 해도 가격이 어마어마하다고 한다. \n\n사실 글쓴이의 경우 운동을 좋아하지 않아 헬스장이 무슨 의미가 있거나 싶었지만, 그 헬스장과 함께 운영되는 사우나가 가고 나서 생각이 바뀌었다. 사우나의 시설이 엄청 좋아 점심시간에 식사를 일찍 하고 가서 피로를 푸는 사람들도 있으니 ‘아, 난 운동도 안하는데 이게 무슨 복지람’ 이라고 생각하시는 분들에게는 사우나를 추천한다:)\n세번째로는 슈퍼패스 라는 특이한 형태의 휴가이다. 이러한 형태의 휴가를 운영하는 회사들도 종종 있는 것 같다. 이 휴가는 연차와는 별개로 지급되는 또다른 복지로서 반반차와 같이 2시간 휴가권이라고 보면 된다. 처음 입사했을 때는 상/하반기 각각 4개 정도였으나 지금은 현재 상/하반기 각각 6개로 변경되었다. 당일 바로 사용해도 무방하며 늦게 출근하든 조기 퇴근 하던 자유롭게 이용할 수 있다. 팀에 따라 다르겠지만 휴가 사용에 있어 큰 제약이 없기 때문에 언제든 편하게 사용할 수 있다. \n네번째는 재택 근무이다. 개인적으로는 이 복지가 제일 마음에 든다. 아마 부서별로 다르겠지만 이 복지의 경우 CTO 조직에만 적용된다. 휴가와는 다르게 워킹데이로 2일전에는 미리 신청을 해야 하며, 재택 근무 후에는 별도의 보고서를 제출해야 한다. 종종 월요병이나 연휴로의 피로감으로 인해 회사까지 출근하기가 귀찮다라는 게 예상되는 날이면 미리 재택 근무를 신청하여 어디서든 편하게 근무를 할 수 있다:) 다만 한달에 2회라는 제약점이 있다는 것은 아쉽다. \n이 외에도 화요일에 한해 모닝빵 제공이라던지, 명절 귀성버스 제공 등의 복지가 있지만 객관적으로 생각했을 때, 복지가 많다고 생각되진 않는다. 하지만 이러한 복지보다는 함께 일하는 동료들이 회사에서 제공해주는 최고의 복지가 아닐까란 생각을 한다. 물론 이또한 주관적이기 때문에 강조하지는 않는다:-)\n티몬의 개발 문화\n개인적으로 티몬의 개발 문화는 굉장히 자유로우며 기술 선택에 있어서도 제약이 많이 따른다고 생각하진 않는다. 물론 이러한 점은 팀에 따라서 다를 수 있다. 또한 팀에 따라서 사용하는 기술이 다를 수 있지만 스스로 책임을 질 수 있다면 새로운 기술을 도입함에 있어 큰 제약이 따르지 않는다. 자신의 도메인과 프로덕트에 대한 책임감은 당연하다는 입장에서 이정도의 제약은 새로운 기술의 도입에 큰 제약이라고 생각하진 않는다. 참고로 필자의 경우에는 프론트 개발팀 내에서 처음으로 타입스크립트를 도입하여 개발을 했다. 물론 새로운 기술을 도입함에 있어 같은 코드를 보는 팀원들간의 도입의 타당성에 대해서는 이야기를 해본 후 도입을 해야겠지만, 성장을 추구하는 티몬의 개발 문화에서 논리 없는 반대 의견에 부딪히는 경우는 거의 없었던 것 같다. 글쓴이가 속한 팀의 전체 연령이 타팀에 비해 전체적으로 연령대가 낮다보니 아마 더 열정에 넘쳐서 일수도 있을 것이다.\n끝으로..\n이 글은 전 직장 동료의 위드이노베이션에 합류해서 젊은이로 살아가기 라는 글을 본 후, 동의를 구하여 또다른 버전의 젊은이로 살아가기를 작성하게 되었습니다. 아마 회사에 입사하기 전 저와 같이 회사에 대해 궁금한 점이 많은 분들을 위해 조금이나마 도움이 되었으면 좋겠습니다. 혹은 그 전까지는 별 생각이 없다가 이 글을 본 후, 나도 한번 티모니언이 되어볼까? 라는 생각을 하게 된다면 언제든 지원을 해보시는 것도 좋을 것 같습니다:) 이 외에도 이야기하지 못한 회사의 이야기들이 많을 테지만.. 지금 이 글을 작성하는 시간이 새벽이라 병든 닭처럼 졸며 쓰기 때문에 기억이 나질 않습니다… 혹시나 이 글에 나와있는 내용 혹은 이외에 궁금한 점이 있다면 메일 혹은 댓글 혹은 SNS을 통해 연락주시면 친절하게 답변드리도록 하겠습니다. 채용 공고에 대한 자세한 내용은 티켓몬스터 채용 공고 홈페이지에서 확인 가능합니다. 긴 글 읽어주셔서 감사합니다:)","link":"http://blog.martinwork.co.kr/review/2019/03/07/be-a-tmonian.html","title":"티모니언으로 살아가기","pubDate":"2019-03-06T15:50:51.000Z"}},{"node":{"contentSnippet":"들어가기 전\n본 포스팅은 구글 클라우드 플랫폼 사용자 그룹의 운영진인 Jaeyeon Baek 님과의 스터디에 대한 기록입니다. 또한 이 글은 구글 클라우드 플랫폼 입문 이라는 도서에 대한 리뷰 및 개인적인 스터디 기록임을 미리 밝힙니다. \nGoogle Compute Engine(GCE)\nGCE는 GCP에서 제공 하는 가상 머신 서비스이다. 일반적인 클라우드 환경에 비해 가상머신의 기동이 빠르고 초 단위로 청구가 과금이 되기 때문에 사용한 컴퓨팅 시간에 대해서만 요금을 지불하면 된다. 가상 머신의 두가지 타입의 메모리 인스턴스를 제공하는데 첫번째는 사전 정의된 머신 유형이다. 해당 유형을 사용하는 경우 마이크로 인스턴스에서부터 3.75TB까지 용도에 따라 사전 정의된 가상 머신 인스턴스를 사용할 수 있다. 두번째 타입은 커스텀 머신 유형이다. 이러한 경우는 사전 정의 머신 유형과는 다르게 vCPU나 메모리 등 구체적인 요구사항을 조정하여 이용할 수 있다. 만약 운용하려고 하는 서비스의 스펙에 대해서 명확하다면 커스텀 머신 유형을 이용하는 것이 비용 절감에도 도움이 된다. \nGCE에서는 가상 머신에 접속이 가능한 두가지 종류의 저장소를 제공해준다. 첫번째는 최대 10TB의 네트워크 저장소인 영구 디스크이다. 영구 디스크는 SSD나 HDD 형식 중 하나를 선택해서 만들 수 있으며 해당 VM 인스턴스가 종료되어도 영구 디스크에 데이터가 유지가 되며 다른 가상머신 인스턴스에서 접근을 해도 이용할 수 있다. 두번째는 로컬 SSD이다. 로컬 SSD는 가상 머신 인스턴스를 호스팅하는 서버에 실제로 연결되므로 영구 디스크와 비교하여 IOPS(입출력 초당 작업수)가 매우 높고 레이턴시(지연시간)가 매우 낮다. vCPU가 1개 이상인 가상 머신에 최대 3TB의 로컬 SSD를 사용할 수 있다. \nDebian, CentOS, CoreOS, Ununtu, Red Hat, Windows 등 원하는 OS를 실행시킬 수 있으며, 뿐만 아니라 GCP 커뮤니티에서 얻은 이미지나 혹은 스스로 만든 이미지를 사용할 수 있다. 이러한 GCE 가상 머신의 각각의 인스턴스는 독립된 네트워크로 접속된다. 물리적으로 다른 지역에 속한 인스턴스더라도 같은 네트워크에 접속되어있다면 내부 IP 주소를 통해 통신할 수 있다. \n또한 GCE에서는 라이브 마이그레이션 지원한다. 라이브 마이그레이션은 말 그대로 어떤 물리 서버에서 운용 중인 가상머신을 다른 물리서버로 서비스 중단 없이 이동하는 기술이다. 만약 물리 서버의 유지 보수와 같이 데이터 센터 안에서 물리 서버를 정지해야하는 경우에는 해당 머신은 라이브 마이그레이션을 사용하여 다른 호스트 머신에 자동으로 이동된다.\nGoogle Cloud Storage\nCloud Storage는 높은 가용성과 내구성을 가진 오브젝트 저장소이다. 전 세계에 엣지 캐시를 가지기 때문에 사용자가 어디에 있든 빠르게 접근할 수 있다. Google App Engine이나 GCE로 동작하는 어플리케이션이나 BigQuery, Cloud Dataflow, Cloud Dataproc 등과 같은 데이터 처리 서비스의 백엔드 저장소로도 이용할 수 있다. 이러한 서비스는 아래의 4가지 저장소 등급의 가격의 서비스를 제공하니 목적에 따라 원하는 서비스를 선택하면 좋다.\n\n해당 서비스에서는 2가지 중요한 단어가 나온다. \n버킷이란 무엇인가\n버킷이란 데이터를 담는 기본 컨테이너이다. Cloud Storage에 저장되는 모든 데이터는 버킷 안에 포함된다는 의미이기도 하다.\n오브젝트란 무엇인가\nObject는 Cloud Storage에 저장되는 개별 데이터 조각이다.\nGoogle Cloud SQL\nCloud SQL은 앞서 설명했던 RDBMS의 한 종류인 MySQL을 제공하는 완전 관리형 서비스이다. 해당 서비스에서는 MySQL의 실행 환경이 자동으로 만들어지고 백업이나 유지보수도 자동으로 이뤄진다.\nMySQL의 완전 관리형 서비스\n유지보수\n어플리케이션에서 이용\n\n출처\n\nGoogle Cloud 공식홈페이지\nGoogle Cloud 공식홈페이지-Google Cloud Storage","link":"http://blog.martinwork.co.kr/devops/2019/01/21/gcp-services-for-web-system.html","title":"초보자의 GCP 사용기","pubDate":"2019-01-20T15:50:51.000Z"}},{"node":{"contentSnippet":"들어가기 전\n본 포스팅은 구글 클라우드 플랫폼 사용자 그룹의 운영진인 Jaeyeon Baek 님과의 스터디에 대한 기록입니다. 또한 이 글은 구글 클라우드 플랫폼 입문 이라는 도서에 대한 리뷰 및 개인적인 스터디 기록임을 미리 밝힙니다. \n웹 서비스의 종류\n우리가 주로 이용하는 클라우드 서비스, 예를 들어 AWS나 GCP 혹은 Azure와 같은 서비스를 이용하기 전에 기본적으로 알아야 할 것이 있다. 그것이 바로  어플리케이션의 종류와 동작하는 방식 등이다. 그 안에는 웹 어플리케이션과 앱 어플리케이션, HTTP 통신 규약 혹은 데이터베이스에 대한 것들이 포함되어 있다. 이러한 대부분의 것들이 앞으로 학습할 GCP 공부에 도움이 되길 바라며 간단히 정리를 해보았다.\n우리가 주로 사용하는 많은 서비스들은 거의 대부분 웹 어플리케이션이나 앱 어플리케이션에 포함되는 경우가 많다. 쇼핑을 위한 이커머스 서비스, 정보 검색을 위한 포털 서비스, 숙박을 위한 배달 앱 서비스 등등 편의를 위한 서비스를 제공해주는 많은 훌륭한 회사들이 많다. 우리가 PC나 모바일 웹에서 도메인을 기반으로 검색하고 들어가는 사이트들은 거의 웹 어플리케이션인 경우가 많다. 반대로 앱스토어에서 다운받은 어플리케이션의 경우에는 네이티브 어플리케이션인 경우가 많다. 하지만 앱스토어에서 받았다고 하더라도 모든 어플리케이션이 네이티브 어플리케이션은 아니다. 요즘은 네이티브 어플리케이션과 반응형 웹을 이용하여 하이브리드 형태로 제작하는 경우도 많다. 그러한 경우는 물론 여러가지 이유가 있겠지만, 개인적으로 생각했을 때 가장 큰 이유 중 하나는 플랫폼 별 리소스 를 집중적으로 관리하기 위해서가 아닐까란 생각을 한다. 네이티브 앱 개발자도 IOS 개발자가 있고 Android 개발자가 있다. 물론 각자 사용하는 프레임워크도 다르고 언어도 다를 수 있다. 이런식으로 플랫폼 별로 리소스를 별도로 관리를 해준다면 버전 관리, 동일한 사용자 경험 부여 등의 문제가 생길 수 있다. \n\n이러한 여러 가지 플랫폼들을 공통적으로 백엔드(Back-end)서버 혹은 API 서버라고 불리는 웹 서버와 요청(Request)와 응답(Response)을 주고 받는다. \n\nURL와 Method의 종류\n이러한 API 서버는 상황에 따라서는 여러 대일수도 있습니다. 클라이언트에서는 어떠한 자원이 필요한지를 알고 원하는 자원에 대한 요청을 합니다. 그러한 원하는 자원은 URL(Uniform Resource Locator)을 지정하게 됩니다.  \n\n위의 URL 중 사용자 이름(user)와 비밀번호(password), 그리고 포트번호(port) 등 아마 생소한 것들도 있을 것이다. 우리가 URL을 통해 이용하는 많은 웹 사이트들의 경우 사용자 이름이나 비밀번호 같은 경우 생략 되어져있다. 또는 포트번호와 같이 기본 포트(80)로 지정이 되어있거나 포트 포워딩을 통해 우리가 URL만 보고는 쉽게 유추할 수 없는 경우도 많다.\n단순히 이러한 URL만 있다고 해서 모든 요청을 API서버에서 알아서 할 순 없습니다. 그래서 우리는 요청에 대한 메소드(Method)와 함께 요청을 보냅니다. 보통 많이 사용하는 Method는 Get, Post, Put, Delete 등이 있으며 종종 Option 메소드도 볼 수 있다. 이러한 메소드와 URI(Uniform Resource Identifier)을 통해 RESTful API 설계를 많이 한다. RESTful API에 대한 이야기는 해당 포스팅 주제가 아니므로 별도로 다루진 않는다. 궁금한 경우는 여기에서 확인할 수 있다.\n각 메소드들에 대해서 간단히 설명을 하면 다음과 같이 설명할 수 있다.\n\nNameDescription              \n\nGet해당 자원을 조회할 때 사용\nPost해당 자원을 생할 때 사용 \nPut해당 자원을 수정할 때 사용 \nDelete해당 자원을 삭제할 때 사용 \nOptionPreflight(사전 전달)의 역할을 함으로써 해당 요청 전 해당 자원에 대한 지원여부를 확인할 때 사용\n\n이러한 요청에 대한 결과값은 상태(Status)와 함께 응답을 준다.\nStatus 규약\nHTTP Request status codes HTTP 상태 코드는 특정 HTTP 요청에 대한 상태를 나타내준다. HTTP 표쥰 규약의 경우 3자리수로 이루어져 있으며 각자 대역별로 추상적인 의미를 가지게 된다.\nSuccess status(2XX)\n해당 상태들은 클라이언트에서 요청한 동작을 수신하여 이해해서 성공적으로 처리했음을 의미 한다. \n\nStatusMessageInformation\n\n200OK요청 성공(단, HTTP method에 따라 성공 의미는 다름)\n201Created요청이 성공하여, 새로운 리소스가 생성(put, post methods에 해당)\n202Accepted요청을 성공적으로 수신하였으나 처리되지 않음. 비동기 처리에서 사용하며 response 에는 결과가 포함되지 않을 수도 있음.\n\nRedirect status(3xx)\n해당 상태들을 클라이언트에서 요청을 한 후, 그 요청을 마무리하기 추가적인 액션을 취해야 함을 의미한다.\n\nStatusMessageInformation\n\n301Moved Permanently요청한 리소스의 URI가 영구적으로 변경되었음을 의미하며, 새로운 URI를 response 에 포함.\n302Found요청한 리소스의 URI가 임시적으로 변경되었으며, 이후에는 요청한 리소스의 URI 로 돌아올 예정임. \n\nError in client status(4xx)\n해당 상태는 클라이언트의 요청에 뭔가의 오류가 있음을 의미한다.\n\nStatusMessageInformation\n\n400Bad Request클라이언트에 보낸 요청을 서버에서 이해할 수 없음\n401Unauthorized요청한 리소스에 대한 권한이 없음을 의미(인증되지 않은 경우) \n403Forbidden요청한 리소스에 대한 접근 권한이 없음을 의미하며 401과 다른점은 서버에서는 요청자를 알고 있으나, 인증여부 상관없이 비공개인 경우\n404Not Found요청한 리소스의 URI가 없음을 의미\n405Method Not Allowed요청한 리소스의 URI에 요청한 메소드가 정의되어 있지 않을 경우\n\nError in server status(5xx)\n4xx는 클라이언트의 오류일 경우라고 한다면, 해당 상태는 서버에 오류가 있음을 의미한다.\n\nStatusMessageInformation\n\n500Internal Server Error서버 내부에 오류가 있어 요청을 수행할 수 없음 \n502Bad Gateway서버가 게이트웨이나 프록시 역할을 하고 있거나 또는 업스트림 서버에서 잘못된 응답을 받음\n503Service Unavailable서버가 가부하에 걸렸거나, 유지보수를 위하여 접근이 거부\n\n데이터 베이스\n이전까지 클라이언트와 API서버와의 커뮤니케이션 과정을 살펴보았다. 이제는 어플리케이션의 보다더 뒷단을 살펴볼 필요가 있다. 클라이언트와 API서버가 서로 리소스에 대한 커뮤니케이션 하는 과정에서 일어나는 대부분의 행동은 데이터 베이스에 저장이 된다.\n\n이러한 데이터 베이스에는 크게 두가지 종류가 있다. \n관계형 데이터 베이스 관리 시스템 RDBMS\n아마 데이터베이스를 사용할 때 제일 많이 이용하는 데이터 베이스가 아닐까 싶다. 데이터베이스 하면 대표적으로 생각나는 Mysql도 이 안에 해당한다. RDBMS는 Relational Database Management System의 약자로서 스프레드시트 같은 형태의 데이터 베이스를 생각하면 이해하기 쉽다. RDBMS는 IBM 산호세 연구소의 에드거 F. 커드가 도입한 관계형 모델을 기반으로 한다. 관계형 데이터 베이스는 만들거나 이용하기가 쉽다. 처음 데이터 베이스는 만든 후 관련되는 응용 프로그램들은 변경하지 않고, 새로운 데이터 항목을 데이터 베이스에 추가 할 수 있다(파일이나 네트웍 데이터베이스 등, 그 이전의 데이터베이스들은 항목이 수정되면, 그 데이터베이스를 사용하는 모든 응용 프로그램도 함께 수정해야하는 어려움이 있었다). \n관계형 데이터베이스는 미리 정의된 내용에 따라 테이블들이 구성되는데, 각 테이블은 데이터 종류나 성격에 따라 여러 개의 컬럼(column)이 포함될 수 있다. 예를 들어, 주문거래 데이터베이스에는 성명, 주소, 전화번호 등의 컬럼 항목으로 구성된 테이블과 또한 주문내용(제품, 고객, 일자, 판매가격 등)을 나타내는 테이블이 포함될 수 있다. 이러한 데이터는 사용자의 필요에 맞는 형태로 데이터베이스의 내용을 볼 수 있다. 또한, 관계형 데이터베이스를 구축할 때 데이터 컬럼이 가질 수 있는 값의 범위(domain)나, 그 값에 적용될 수 있는 제한사항(constraint)을 정의할 수 있다. 예를 들어, 고객의 성명을 빈 칸으로 남겨 놓지 못하게 한다거나, 판매가격에는 마이너스(-) 값이 올 수 없도록 제한할 수 있다. 관계형 데이터베이스를 정의하게 되면 그 테이블이나 컬럼, 도메인 및 제한사항에 대한 내용을 가진 메타 데이터(metadata) 테이블이 함께 만들어진다. 이러한 RDBMS를 GCP에서는 Cloud SQL이라는 서비스를 통해 이용할 수 있다.\nNoSQL\n관계형 데이터 베이스와는 전혀 다른 형태의 데이터 베이스이다. NoSQL은 오랫동안 사용되어진 관계형 데이터와는 다르게 2000년대 초반부터 떠오르기 시작하였다. 과거 빅데이터의 등장에 따라 RDBMS만을 이용해서는 데이터를 처리하는 데 드는 비용의 증가를 막을 수 없었다. 데이터의 양과 트래픽을 한 대에서 실행되도록 설계된 RDBMS의 Scale-up 비용이 기하급수적으로 증가했기 때문이다. 이러한 상황에서 Scale-out을 목표로 등장한 데이터 베이스가 바로 NoSQL이다. 대표적인 데이터베이스로는 Redis나 MongoDB가 포함된다. 주된 데이터 모델의 방식은 KVS(Key-Value Store) 방식이다. \nNoSQL은 대규모의 데이터와 트래픽을 위해 설계되었기 때문에 많은 양의 데이터를 다뤄야하는 채팅 데이터나 로그 데이터를 저장하는 데 용이하다. NoSQL은 관계형 데이터 베이스에 비해 유연한 데이터 모델을 사용하며  비정형 데이터를 다룰 때 그 빛을 바랄 수 있다. 또한 key-value 방식의 데이터 모델을 사용하기에 응답 속도나 처리 효율에 있어 뛰어난 성능을 보여준다. 이러한 NoSQL을 GCP에서는 Cloud DataStore와 Cloud Bigtable을 통해 서비스하고 있다.\nSQL문\n위에서 살펴본 SQL문을 통해 데이터에 접근할 수 있습니다.\n\n데이터 정의 언어(DDL: Data Definition Language)\n데이터를 저장하는 구조를 정의하기 위한 명령어로서 CREATE문(생성), DROP문(삭제), ALERT문(변경)이 이에 해당한다.\n데이터 조작 언어(DML: Data Manipulation Language)\n데이터를 조작하기 위한 명령어로서, UPDATE문(변경), DELETE문(삭제), INSERT문(추가), SELECT문(조회)가 이에 해당한다.\n데이터 제어 언어(DCL: Data Control Language)\n데이터베이스에 접근 권한 제어나 상태 관리를 하기 위한 명령어로서, 사용자에게 권한을 부여하거나 트랜잭션을 처리할 때 주로 사용합니다.\n\n사실 SQL문 자체를 많이 쓸 일이 없어서일 수도 있겠지만 다른 명령어는 익숙해도 트랜잭션이라는 것이 굉장히 생소했다. 그렇다면 트랜잭션이란 무엇일까?\n트랜잭션(Transaction)이란?\n위키 백괴에 따르면 데이터베이스 트랜젝션은 데이터베이스 관리 시스템 또는 유사한 시스템에서 상호 작용의 단위 라고 표현하고 있다. 어려운 이 문장을 풀어서 설명하면 데이터베이스 안의 데이터를 변경할 때 하나의 단위로 묶어서 관리하는 구조를 일컫는다. 많은 블로그들과 책에서도 은행의 계좌 시스템이라는 굉장히 좋은 예시로 들어 표현하고 있어 필자도 그림으로 그려 가며 표현해보고자 한다. \n만약 다음의 그림과 같이 Man이 Woman에게 $10를 계좌이체 해준다고 가정해보자. \n\n그렇다면 Man의 계좌에 대한 데이터는 -$10에 대한 변경이 일어날 것이고, Woman의 계좌 데이터에서는 +$10에 대한 변경이 일어날 것이다. 이러한 일련의 과정이 하나의 단위로 묶여서 일어나야 한다. 이러한 구조를 트랜잭션이라고 표현한다. 이러한 일련의 과정이 모두 성공적으로 처리가 완료되어 완료된 경우를 COMMIT이라고 표현하고, 만약 하나라도 실패하여 모든 처리를 되돌리는 과정을 ROLLBACK이라고 표현한다. 이러한 특성으로 인해 RDBMS는 아래의 ACID 특성을 필요하다.\n\nAtomicity(원자성)\n트랜잭션에 포함된 처리가 모두 처리가 되거나 혹은 모두 취소가 되어야 한다.\nConsistency(일관성)\n트랜잭셕 처리 전후로, 모든 데이터의 정합성이 지켜지며, 모순이 없어야 한다.\nIsolation(독립성)\n트랜잭션 안에서 처리 중인 처리가 다른 처리에 영향을 미치지 않아야 한다.\nDurability(내구성)\n트랜잭션이 끝났다면 시스템 장애가 생겨도 데이터를 잃지 않아야 한다.\n\n가상화 서버\n아마도 MacOS를 사용하는 사용자라면 쉽게 이러한 가상화 서버를 접할 수 있을 것이다. 혹은 다양한 OS에서 어플리케이션을 테스트해야하는 경우에도 접해볼 수 있다. 이러한 가상화 서버는 한 대의 물리 서버를 여러 개의 가상 서버로 나눠서 이용하는 것을 일컫는다. 이러한 가상화 서버는 3개지 형태로 분류할 수 있다.\n1. 호스트형 가상화\n\n호스트형 가상화는 하드웨어 위의 기반이 되는 호스트 OS(Window, Mac 등)가 존재하며 그 호스트 OS에 가상화 소프트웨어를 설치한다. 이렇게 설치된 가상화 소프트웨어를 통해 서버 하드웨어를 에뮬레이션하는 것이다. 서버 하드웨어를 에뮬레이트하기 때문에 필요한 여분의 CPU 자원,디스크 용량, 메모리 사용량 등과 같은 오버헤드가 커질 수 있지만 게스트 OS에 대한 제약이 없어 대부분의 OS를 동작시킬 수 있다. 이러한 소프트 웨어는 VMware, VirtualBox, Parallels 등이 포함된다.\n2. 하이퍼바이저형 가상화\n\n하이퍼바이저형 가상화는 VMM(Virtual Machine Monitor)을 하드웨어 위에서 직접 동작시키기 떄문에 호스트OS 설치가 필요 없다. 호스트 OS에 할당할 리소스가 없기에 호스트형 가상화보다는 오버헤드가 적은게 특징이며 하이퍼바이저가 하드웨어를 직접 제어하기 때문에 리소르르 효율적으로 사용할 수 있습니다. Linux KVM이나 Hyper-v 등이 해당합니다.\n3. 컨테이너형 가상화\n\n마지막은 컨테이너형 가상화이다. 앞의 두 가상화 서버와는 다르게 각자의 가상환경에 게스트 OS가 존재하지 않기 때문에 오버헤드가 적으며 가볍고 빠르게 동작하는 것이 특징이다. 호스트 OS위에 컨테이너 기반으로 분리해두고 각가에 독립된 OS 환경을 제공해준다. 이러한 컨테이너는 HostOS를 다른 컨테이너들과 공유한다. Docker가 대표적인 컨테이너 가상화 기술을 사용합니다.","link":"http://blog.martinwork.co.kr/devops/2019/01/21/what-kind-of-web-service.html","title":"초보자의 GCP 사용기","pubDate":"2019-01-20T15:50:51.000Z"}},{"node":{"contentSnippet":"문제 설명\n본 문제는 프로그래머스라는 알고리즘 사이트에서 가져온 문제입니다. 알고리즘을 푸는 모든 과정은 자바스크립트로 이뤄졌습니다. \n먼저 이 문제는 다음과 같다.\n\n자연수 n 개로 이루어진 집합 중에 다음 두 조건을 만족하는 집합을 최고의 집합이라고 합니다. 최고의 집합은 n개의 원소의 합은 S가 되는 수의 집합입니다. 이러한 조건을 만족하며 n개의 수의 곱이 최대가 되는 집합입니다.\n\n예를 들어 각각의 원소 갯수 n을 2라고 가정하고 원소의 합 S를 9라고 가면 하면 경우의 수는 [1, 8], [2,7], [3,6], [4,5] 등으로 총 4개가 될 수 있다. 그 중에서 최고의 집합은 [4,5]인 집합이다. 예시를 하나 더 들어보자. 원소의 갯수 n은 2이라고 가정하고 원소의 합 S를 13이라고 가정하면 경우의 수는 [1,12], [2,11], [3,10], [4,9], [5,8], [6,7] 이다. 6개의 집합 중 최고의 집합은 [6,7]이 해당한다. \n문제 해결 방법 유추\n위에서의 두가지 공통점으로는 최고의 집합은 두수의 차가 최소일 경우이다 라는 가정을 얻을 수 있다. \n그러한 가정은 다음과 같은 방법으로 증명할 수 있다. 간단하게 예시를 들기 위해 n은 2개라고 가정한다. 자연수 a와 b의 합이 S라고 가정했을 때, a와 b의 관계를 구하기 위해 a-b과 a+b를 비교를 한다. \n\n두 값중 a+b의 제곱근이 a-b의 제곱근보다는 큰 값일 테니 전자에서 후자의 값을 빼준다.\n\n두 값을 서로 비교해서 빼면 결국에 다음과 같이 나온다.\n\n우리가 구하고자 하는 ab 값을 구하기 위해 오른쪽과 왼쪽의 값을 4로 나누어 준다.\n\n그렇게 나누고 나면 우리가 원하는 ab의 값이 나오고 다음의 그림과 같이 a-b 의 값이 작을 수록 ab의 값이 커지는 것을 알 수 있다.\n\n결국 이 문제에서 최다의 ab 값을 구하기 위해서는 n개의 수의 차가 적을수록 원하는 최대값을 구할 수 있다.\n문제 해결\n먼저 n개의 자연수가 n개의 자연수의 합 S 보다 클 경우를 대비하는 방어코드를 추가했다. n개의 자연수의 합이 S보다 작을 경우는 문제에서 주어진 경우와 같이 -1에 해당하는 배열을 반환해준다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\nfunction solution(n, s) {\n  var answer = [];\n  if (n > s) {\n    return [-1];\n  }\n\n  return answer;\n}\n\n\n우리는 합이 S인 서로 차이가 적은 n개의 자연수를 찾는 것이기 때문에 S를 n으로 나누어 준다. 나눈 값을 그대로 넣어주면 소수점이 나온다. 하지만 우리가 구하고자 하는 값은 자연수 이기에 반올림 혹은 내림 등 중 하나를 선택해야 한다. 여기에서 주목할 것은 우리가 구하고자 하는 값은 올림차순으로 이루어진 집합이기 때문에 여기에서는 내림을 선택해준다. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\nfunction solution(n, s) {\n  var answer = [];\n  if (n > s) {\n    return [-1];\n  }\n  for (let i = 0; i < n; i++) {\n    // 하나씩 구해서 answer이라는 배열에 넣으면 구해야할 자연수 n개의 갯수는 점점 줄어든다.\n    const number = Math.floor(s/(n-i));\n    // 구한 값을 answer 배열에 담아준다.\n    answer.push(number);\n    // 전체의 갯수에서 구한 값을 제외시켜준후, 다시 배열을 시작한다.\n    s = s - number;\n  }\n\n  return answer;\n}\n\n\n만약 내림차순으로 이루어진 집합을 구하고자 한다면 다음과 같이 구할 수 있다.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\nfunction solution(n, s) {\n  var answer = [];\n  if (n > s) {\n    return [-1];\n  }\n\n  for (let i = 0; i < n; i++) {\n    const number = Math.round(s/(n-i));\n    answer.push(number);\n    s = s - number;\n  }\n  return answer;\n}","link":"http://blog.martinwork.co.kr/theory/2019/01/19/top-group-algorithm.html","title":"최고의 집합 알고리즘","pubDate":"2019-01-18T15:50:51.000Z"}}]}}}